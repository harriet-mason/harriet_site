---
title: "PhD Research Ideas"
author: "Harriet Mason"
---

## Sample vs Mass as N increases in size
![Main idea behind sample and mass visualisation comparison](samplesizemass1.jpeg)

#### What is in the current literature (i.e. notes that could have a citation if needed)
- Many papers treat expressing a distribution as a sample vs as a mass as distinct methods of displaying a distribution
- There are also papers that show that looking at a sample is better than looking at a density and attribute that difference to the "countability" of a sample
  - This is also the logic many authors use for displaying a quantile dot plot
  - People with high numeracy read pictographs more accurately and this is considered to be because people with high numeracy read pictographs by counting, while those with low numeracy skills read pictographs by estimating area.
- The hops plot notices the benefits of their display over a static sample decreased as the animations fps increased. They compared multiple fps, but did not seem to look at multiple sample sizes for the static images. They did not elaborate on this result, but it could be this "countable sample size" effect, but it could also be difficulty in discerning individual samples.
- Mathematically, the closer the sample size gets to infinity, the closer the statistics you draw from your sample are the true mass. 
- As sample size increases, people do not edit their underlying beliefs according to Bayesian reasoning (i.e. you cannot use bayes rule to mathematically calculate a persons change in reasoning by looking at a plot), so there is reason to suspect the effects of sample size on an visualisation of a distribution will not follow the mathematical sample size effect.

#### What I want to test (could do one or a handful of these)
- While this sample vs density effect has been identified multiple times in multiple different plots, there seems to be little research into how sample size interacts with that.
- If there is a difference between peoples ability to read information from a mass or from a sample, we would expect that difference to decrease as the sample size gets larger 
- If there is a relationship between sample size and plot readability mass, it should exist in *all* aesthetics we can map a sample to. The fps shows it exists in an animation, but it would be interesting to see if the effect changes if the aesthetic the sample is plotted on changes (e.g. colour hue, position, frames of an animation, lengths, directions, ect). This would be limited by ability to show samples of some aesthetics (like volume, shading, area, etc)

![The effect of sample size on the distinction between sample and mass might change depending on the aesthetic we map our sample values to.](samplesizemass3.jpeg)

- It also might be worthwhile to try to work out *why* a sample might outperform a mass depiction.
  (1) People are counting the sample and estimating area for the density, and estimating area is 
  (2) for that particular question, whatever heuristic is used for a density is less reliable than the heuristic used for a sample
  (3) The question is easier to answer if the viewers follow a series of individual samples (e.g. that joint density animation HOPS example). 
- We could differentiate between (1) and (2) by forcing participants to  answer the question really quickly.
  - People have an ability to count a small number of objects at a glance (about 2 or 3), and often sample sizes depicted in visualisation are much larger than that. If people have to give an answer *very* fast the sample size effect might go away, because they cannot count them. 
  - Could also interact with the sample size effect, because increasing the sample size would require participants a longer time to answer the question.
  - As the sample size increases, it might get to a point where the viewer just stops counting entirely, in which case we would see a drop off in accuracy in each participant.
  - If the participants are using a heuristic for the sample size calculations instead of counting, the sample size and time to answer relationship would not be as noticeable.

#### Side comments
- This test would need to be limited to visual expressions that minimise the number of other changes to the grammar of graphics between the plots. Although this is difficult because the grammar of graphics assumes we are starting with a set sample. As n approaches infinity, some plots that are distinct in the grammar of graphics and visually distinct at smaller sample sizes collapse into the same plot as n approaches infinity. This makes sense because this is what we would expect mathematically, but it does create a problem with using the grammar of graphics. Two entirely different plots might collapse into the same plot visually, and two plots that are almost identical in the grammar of graphics will display completely different plots as n approaches infinity. 
  - e.g. as sample size increases there is a risk of overplotting. How overplotting is managed could also have an effect on this sample size thing. E.g. if you increase transparency, randomly jitter the points, organise them. This is something else that could be a variable, or it might just have to be kept constant, in which case we would need to line make sure the mass depiction is that sample size depiction (using that overplotting management method) as n approaches infinity.
  - Do we want to compare different visualisations that only have minor changes in the grammar of graphics, or do we want to compare visualisations that collapse into the same graphic?
  - I kinda feel like mapping which visualisations collapse into each other as a sample size increases could almost be a paper in of itself

![Example of plots that are different according to the grammar of graphics, and look different at finite samples, but collapse into the same visualisation as n approaches infinity](samplesizemass2.jpeg)

- Need to consider how the information would be drawn from the density vs sample. If you ask for the most likely value, that is finding the peak on a density, so it would probably be more accurate than a sample in those cases. I would probably need to check *which specific tasks* found a sample outperformed a density.
- Need to ask participants how they got their numbers. I liked the method used in that paper of Di's.
- I am not sure what measurements to use. I have made the point that uncertainty visualisation should focus on signal suppression, but masses (and samples) are displayed for reasons other than signal suppression (i.e. they are not always "uncertainty" visualisations). Therefore I am not sure if we should measure the 
- There are concepts related to displaying densities that overlap with sample side, i.e. smoothing functions for densities and bin widths for histograms. If you compare a sample to an estimated density, it is likely the smoothing function will interfere with results in some way and also might need to be considered as a variable. Alternatively, the relationship between binning functions and sample size might also be testable in some way.

## Signal supression experiment
#### Perceptual task
![Main idea behind percpeptual task signal supression experiment](signalsupress1.jpeg) 

- Evaluating the plots on how well people can extract the average when the variance is low would be replicating existing work, evaluating plots on how much people *struggle* to extract the average when variance is high is the new work.
- Uncertainty visualisations should focus on signal suppression, not extracting particular values or decision making experiments.
- I feel like this needs to be done in conjunction with another experiment with a different goal, because it is more a methodological change than a specific idea for an experiment.
- A simple method would be to redo the perceptual task experiments but instead of showing a single value, you show a distribution.
  - Instead of identifying the perceptual task that best allows viewers to express uncertainty, we would be trying to match a perceptual task to something that suppresses that aesthetics signal
  - Looking for visualisation methods that suppress signal when the variance is high and allow you to extract signal when the variance is low. Similar concept to the value suppressing uncertainty palette.
- The problem with these simple experiments is that the task is so simple, there isn't a "signal" for someone to pull out of the visualisation. The task is simply value extraction. This means we can ask "what is the average of X" reducing the problem to a value extraction task
- Additionally, being able to suppress a single value extraction is not the main goal. This should extend questions about  combinations of distributions if this is a viable method of expressing uncertainty. 

#### Group comparisons

![More complicated task example for signal supression experiment](signalsupress2.jpeg)  

- Similar to the previous perceptual task, these tasks are so small that you kind of need to ask question. When you ask if the trend is increasing or if A is greater than B, we are asking a deterministic question about a random variable. This is not advisable so the simple versions of the experiment don't seem to be doable...

#### Plot level signal supression

![Example plots of open ended questions. We can see what signal the plot supresses based on what the viewer reports as a noticiable signal](signalsupress3.jpeg)  

- What is more interesting is if visualizing noise on individual variables extends to plot level signal suppression, we might just have to test this.
- Since we cannot ask a deterministic question about a random variable since they depend on a probability, we could instead display uncertainty and ask open ended questions and how visualisation method and variance affect signal suppression (i.e. whether or not the participants notice the relationship).
- This would test if signal suppression applied to individual values extends to plot level signal suppression. This would mean that uncertainty for things like resampling (but not model choice) can be expressed in a visualisation for EDA.

## Using Visual Integrability to Map Uncertainty

- It is also unclear if uncertainty
- This is similar to the perceptual task one but for more complicated signals like finding spatial autocorrelation in a map.


## Short Ideas
- Connect sufficient statistics to grammar of graphics. If we have a statistic we want to draw inference on, it has a sufficient statistic. Many visualisations use 
- I feel like a mass distribution implies uncertainty that was calculated using assumptions, while a sample distribution implies uncertainty that was calculated using resampling methods
- Could be fun to ask people vague questions, and because they are not sure what we want, see if they always respond using the simplest perceptual task. (e.g. "best estimate" and they just read position)