{
  "hash": "37e1506f157a3b4f9fa3535026ef41fd",
  "result": {
    "markdown": "---\ntitle: \"Part 3: Uncertainty Needs a Motivation\"\nauthor: \"Harriet Mason\"\nimage: feature.jpeg\ndescription: Part 1 established that a large reason people do not visualise uncertaitny is because of a view that it is of secondary imporance to estimates or other contextual information. Here I discuss methods of visualising uncertainty as an independent area of interest directly related to a hypothesis rather than as a by product of a data type or estimate. I present an example where visualising uncertainty without this framework leads leads to poor visualisations. \n\neditor: \n  markdown: \n    wrap: 72\nknitr:\n  opts_chunk: \n    warning: FALSE\n    message: FALSE\n    echo: FALSE\n---\n\n<!--\n### Untangling Infinite Threads\n#### A story about you who works in academia\nHello you. You are a budding academic researcher, starting on your journey to do important work and send it out into the public. How exciting. Unfortunately, these are a few steps between you doing your important work that you have been thinking about and that work being seen as a change in society. Lets look at a run through of how this process works:\n\n1) You do research that is useful for the general public and they should know about it.\n2) You spend years trying to get this research published in an academic journal. Most of this time is spent not improving the research but rather making small tweaks to match each journals formatting, length and content rules. and hope the entire time that nobody else does this exact work and publishes it before you (which has happened to people I know).\n3) after years of effort and tweaking (that is usually irrelevant to the actual research) you may finally get your work published.\n4) At this point in the process, your paper has been read by the authors (probably) and the peer reviewers of your paper, lets say about 5 people in total. This is very likely the only people who will read your paper in full.\n5) Your paper is then tossed into the research abyss. I like to imagine it as a big room with a huge pile of papers falling out of a tube in the middle. Papers with no identifiable features are flung randomly on the floor and only people who also belong to some large academic institution can access this room.\n6) People who are researching the topic you have written your paper on walk into this metaphorical room with a blindfold on and grab fist fulls of papers. They are much more likely to grab your paper if it has a well known author, institution, or journal attached to it, or you somehow got stuck to a paper that has one/all of those things. These people never access your paper through the journal it was published in, only through this blindfold shoving into a bag way, so the years you spent formatting your paper to look like other papers in the journal turned out to be unnoticed by the readers and pointless for you.\n7) Of the people who walked into the room and grabbed your paper in a blind collection, some of them will read the title, abstract and maybe some other details, even less will give a skim read of the whole document. Almost none will read the entire thing. \n8) Of those that skim read your research, some will \n9) Occasionally a paper or two will esape this room and a journalist or blogger will show parts of it to the general public. They may misrepresent your work, but most people will not check and even more cannot check because they cannot get behind the pay wall. Ultimately your work is more likely than not to remain trapped in this room forever, contributing to more work that shoots out the pipe into the pile of papers, all of which will remain trapped in this room forever.\n\nLooking at this process, at this performance of information sharing, you wonder if you should just go into industry. Maybe it wont be so bad to work at a huge corporation. You are already working long hours, you might as well make some money. Qualifying for a low income health care card was simultaneously the high and low point of last week, which shines a depressing light on your life. The hours might suck, but they already suck in academia. The last time you spent a long period of time with people in the department, you timed that they all worked 10 hours per day and most of their github repositories have a commit made every day of the past year. Although moving to industry might mean spending your days performing highly unethical work while choking on the boots of monkey torturer Elon Musk. That might actually be worse. You spend a week trying to work out if you would rather degrade yourself for a billionaire CEO or a prestigious journal editor. At least the CEO would kind of pay you. Maybe you should just quit everything and become a gardener. You would continue to live below the poverty line, stuck in a share house with 20 year old, but being outside a lot might relax you. Either because of the sunk cost or because job interviews revolt you, you decide to continue with your PhD. Plus, your dad seemed really happy that you were doing it and it would be easier to finish the degree than deal with a lifetime of him expressing disappointment that you didn't finish it. Now that you have gone through your weekly emotional turmoil of considering quitting, deciding against it, and then trudging back to your computer to do your work, you can discuss the problem at hand. This murkey inefficient academic system has given birth to a collective understanding that says one thing, and research that says the opposite. By deciding to stay in academia you have decided to work with this cycling paper pile, put on your blindfold and grab fistfulls of papers, hoping someone might one day grab yours. So if we are going to work with this nightmare, lets talk about why its ruining your research.\n\n#### Acadmia stress but this time it's mine\nIf you could not tell, that long winded second person narrative was actually about me. I bet you feel surprised by this unexpected information. You may assume this relates to my research (that despite the section above I am still doing) because there are good methods to visualise uncertainty but people are just unaware of them, and that is somewhat true. I did spend the first blog post of this series discussing how the general consensus and attitude towards visualising uncertainty does not align with the literature on the topic. It reminds me of when I was in highschool and told everyone there was no evidence that long term weight loss was possible despite a large number of clinical trials (if you don't believe me, google it) and was essentially treated like a basket case. I actually spent a weekend in a manic state in 2016 and made a stop motion [Youtube video](https://youtu.be/fFuQEKQhBn0) about it, but I digress. The actual link I want to make this week, is that this terrible organisation system for academia and information in general makes it hard to see the forest for the trees, and for broad and somewhat poorly defined topics such as \"visualising uncetainty\" I feel like I have a concussion from being hit in the face by the branches. \n-->\n\n### Footage of me pulling my hair out\nIf one more person talks to me about visualising uncertainty in the context of some specific type of data or model I'm going to rip all my hair out of the top of my skull and live as a monk. The more I read and understand the process behind visualisations and uncertainty in general, the less this makes sense. The question of \"why do you want to visualise the uncertainty\" is treated as secondary to how the uncertainty was calculated (the model) or where the uncertainty came from (the data). \n\nUncertainty doesn't exist unless there is a decision. This seems counter intuitive when you think about data in general. Randomness is all around us and inherent in life, so it doesn't make sense that it can only exist with respect to a decision. Let me rephrase, quantifying (and therefore expressing through visualisation) only makes sense with respect to a decision. I slightly touched on this in my previous post, but I want to make this very clear now, the general concept of uncertainty (randomness or incomplete information) is a much broader category that contains the uncertainty we refer to when we visualise uncertainty (uncertainty related to a decision). This is because visualisations and estimates are made my a person with a purpose. They do not exist naturally in society, floating around with the wind, so it cannot take on the same purposeless existence of \"uncertainty\". The second we start quantifying, we are making decisions about what is important and what is relevant, there is no longer an unbiased natural uncertainty floating around, there is a quantified uncertainty that we have built for a purpose. Just in the way that this uncertainty was quantified for a purpose, it should be visualised with that purpose in mind. To discuss visualising uncertainty as though there is no purpose, no decision to be made, feels purposely obtuse. This is not alien in statistics, as a matter of fact confidnece intervals (the most common representations uncertainty) are derrived from hypothesis tests (a decision), however the literature seems to enjoy pretending it is. \n\nThere is one paper in particular that comes to mind that VERY MUCH highlights how much of a problem this \"uncertainty without purpose\" issue is a plague in the field. I have seen MULTIPLE papers discussing how people are unable to understand confidence intervals and the ignorance of the method is upsetting. Participants were given two estimates with error bars and asked to move the estimates until they would no longer be considered significantly different. Basically every participant moved the error bars until they were just touching. Unfortunately the error bars were for the hypothesis test of \"what is the 95% confidence interval of value A\" and not \"is value A different from value B\" which would have a different interval. Is this a sign that people cannot take uncertainty from one decision and translate it to another? Yes, as a matter of fact it is strong evidence for the \"express uncertainty related to the decision, not related to the model or data\" point I'm trying to make. Is this a sign people cannot understand \"uncertainty\"? No. As a matter of fact, I would argue the participants understand uncertainty better than most people writing about uncertainty. The participants understood they were asking them to make a decision so the uncertainty depicted must be related to the decision because why would you be asked to make a decision with completely irrelevant uncertainty information. That is nonsensical and yet it is what they were asked to do. Another paper gives people HOPS plots and goes \"well people can tell which value is bigger with a hops plot when they can't tell with error bars so it depicts uncertainty better\" completely ignoring the fact that, again, that error bars ask \"are these values significantly different to each other\" not \"is value A bigger than value B\". The uncertainty the HOPS plot predicts includes \"is this value bigger than the other\", it doesn't include \"are these values statistically different\" which I assume it would fail on, just like the error bars. This general concept of \"uncertainty\" that exists without a decision is generating a lot of questionable literature and methods and causing perfectly good visualisations to be thrown out because we were using them for the wrong questions.\n\nReading all these conclusions I start to wonder if I live in a crazy town. If I'm in an alternative universe where decision theory doesn't exist and I have to build it up myself depsite failling measure theory during my undergrad (don't tell my supervisors). Why on earth do we talk about visualising uncertainty with respect to methods and data? I actually have an answer to this.\n\nI think two conflicting methods are butting heads. When we visualise data we do it based on the type of data. Two continuous variables should be depicted with scatter plots, spatial data with a map, time series with a line plot. When we deal with uncertainty, we calculate it and quantify it based on some decision we need to make, not related to the data. In my previous post I made a distinction between statistical and scenario uncertainty. When people try to convey the disaster of climate change they present multiple forecasts, each giving a different scenario for carbon emissions. The data of temperature and carbon emissions are presented as line plots because that is the data, but the uncertainty is presented as multiple possible outcomes. The multiple outcomes are not a product of a line plot, but rather a decision. What happens if we do something about climate change, and what happens if we don't? To look at the plot below and think of it as a method for \"time series\" rather than a method for \"uncertaint event that information about can change the course of\" would be rediculous.\n\n<center>![Image by Katharine Hayhoe, from the 2017 Climate Science Special Report by the U.S. Global Change Research Program.](scenarioforecast.png)</center>\n\nIt is obvious from the plot above two things are happening. First, the data method typically trumps visualising uncertainty, which makes sense as I have spoken before about the way uncertainty is seen as a second class citizen. But what do we do when the uncertainty does not fit nicely with the plot of the data? This brings me to the second thing I have noticed with uncertainty visualisation, we love context but if its hard, we ignore the context. This is not just a problem for data with mutiple dimensions (e.g. spatial temporal data), it is also a problem when we have complicated models. Let me draw what I mean to give a better understanding.\n\n<center>![](contextplots.jpeg)</center>\n\nThis distinction between too much and a little bit of context is implicit and important when we talk about uncertainty. The second the context and the decision are at odds with each other, we drop one. This is an issue when we have too much information, regardless of the model data. That being said, the typical method of visualising uncertainty with context may create more confusion than clarity. Take the depiction of a linear regression with a confidence band above. What is that the uncertainty for? That is, what decision is it related to? Typically when you make a linear regression, that bound will be an interval on your prediction, i.e. we are 95% certain the TRUE blue line lies inside that interval. That may seem intuitive, but we apply a lot of other hypothesis to this blue line that are not actually the same. For example, if I have a single predictor, does a straight line fitting inside this interval mean our coefficient is not significant? What about if I have multiple predictors? Another person might use the confidence interval as a confidence band for the data and assume new data will liekly fall into it. These decisions are all slightly different to the decision that constructed the band, and to plot the context with the uncertainty returns us to our previous issue, that the context creates the uncertainty, not the decision. Once we are operating under the assumption that the context created the uncertainty, people freely use the uncertainty to check decisions that are irrelevant to the uncertainty information, generating more information, not less.\n\nWith this in mind, you might have arrived at the question I have been sitting on, leaving me unable to work on the main idea of they thesis. Why is plotting uncertainty, specifically with spatial data, treated as a unique problem that needs to be solved. To me it is neither unique, nor is there an inherrent reason for it to be solved. Rather I would suggest that there is a greater issue which is that we don't have methods to effectively include context in uncertainty estimates, nor understanding on whether or not its even a good idea. Lets consider a simple map of Australia with an estimated average temperature for that state. If you express uncertainty information with that estimate, you need to have some goal in mind. Do you want people to be able to identify if states have statistically significant differences in their temperatures? Do you want to give people a concept of a range of possible outcomes for each state? Do you want to show there is less data in some states than others? Each of these questions would likely lead to a different visualisation, and herein lies the issue with the current landscape of visualising spatial uncertainty (and by extension uncertainty with too much context). Uncertainty plots made without a decision in mind are messy and directionless. Since they are designed to depict any \"uncertainty\" they are not the most effective in any particular case and become functionally useless.\n\n<center>![](map1.png)</center>\n<center>![](map2.png)</center>\n<center>![](map3.png)</center>\n\n(These are just screenshots from the Visumap paper, I will propperly site stuff later since I'm just word dumping for now)\n\nLooking at these maps, I ask you, do you get an intuitive sense of \"uncertainty\"? I dont. Can you answer any of the questions above with these maps? I can't. So what is the point of depicting the uncertainty in this way? Similar to the way we produce research to throw it back into the echochamber of academia, visualising uncertainty like this implies the reason we include uncertainty is just to.... include it. It is devoid of motivation and appears entirely performative.\n\n\n### Notes to add\n- visualisation with a purpose goes beyond uncertainty. If you want to compare the variance of different areas of your data, you dont want to visualise the uncertainty associated with a metric you should just visualise the variance. Getting the correlation between two time series is easier with a scatter plot than a line plot (cite). Make the goal of your visualisation the key decision in how you visualise it, not the data type.\n- all these methods are worse than just plotting two maps side by side (check the base case method for each of these plots). They also only depict a single value for uncertainty which means we can identify which areas have high error (which seems to be a product of the estimate itself in the example case) and nothing else.\n- the estimate is tied to the location, the uncertainty is tied to the estimate, the uncertainty is not tied to the location.\n- The literature doesn't even comment on this distinction of type of decision needed to be made. They just talk about better or worse methods to convey uncertainty and errors associated with different types. They don't talk about errors associated with specific types of questions even though it is SHOCKINGLY obvious from the literature.\n- \"What uncertainty do we care about\" decides how you quantify it and what aspect of the data it is connected to, \"Why do we care about it\" decides how you plot it.\n- Hypothetical outcomes are good for scenario uncertainty (like forecasts that fork off and make two outcomes). Intervals/distributions are good for statistical uncertainty?. There is a connection with enough outcomes, which is why a large number of hypothetical outcomes can be used as a discreetised version of statistical uncertainty.\n- You cannot make an uncertainty plot without uncerstanding the 1) question a person has and 2) the method they can access the data.\n- Things like animation and interactivity just add another \"axis\" to the plot (similar to adding colour or something). Eventually with enough \"axis' we are not really summarising information anymore, just showing all of it. Looking at plots they generally convey one key piece of information and maybe some extra info. If a single plot is worse than two separate plots, and the swapping between mental effort is less than the effort to understand the bizzare graphics, you have made a bad plot. \n- What do we compare plots to? Nothing? There is no placebo. People being able to answer general questions about uncertainty does not mean a plot is conveying uncertainty. Sometimes I think two plots should be a baseline case for understanding plots.\n- I wonder if HOPS plots were better in that paper because the hypothesis was more appropriate rather than the \"being forced to experience the distribution with time\" thing\n- try offering a visualisation expression for every graph aspect (time, colour, position, shape etc) so you can always assign uncertainty?\n- comparison of plot question like placebo vs drug. Comparing a new drug to a placebo isnt great, should compare to the next best drug on the market. Don't care if it works at all, want to know if it works better than the next best.\n- concept of visualising a statistical test with uncertainty reminds me of infovis but idk how much it relates",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}