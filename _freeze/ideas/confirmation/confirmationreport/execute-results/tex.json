{
  "hash": "85ade3741950a13f777d4ca8a33225c4",
  "result": {
    "markdown": "---\ntitle: \"Plotting Apples, Oranges, and Distributions\"\nsubtitle: \"Graphical Issues Particular to Uncertainty Visualisation\"\nauthor: Harriet Mason\nbibliography: references.bib\ndate: last-modified\nformat: pdf\n---\n\n\n# Motivation\nThink back to the last time you made some sort of data visualisation. What was the purpose of that visualisation? Was it to better understand your data? Was it to help you make a decision? Was it to to communicate that decision to someone else? There are many stages in our analysis that benefit from the power of data visualisation, however this does not mean it is always done with success. Visualization is an important step in exploratory data analysis and it is often utilised to **learn** what is important about a data set. The importance of data driven discovery is highlighted by data sets such as Anscombe’s quartet [@anscombe] or the Datasaurus Dozen [@datasaurpkg]. Each of the pairwise plots in these data sets have the same summary statistics but strikingly different information when visualised. Instead of having to repeatedly check endless hypothesis to find interesting numerical features, visualisations **tell** us what is important about the data set. This powerful aspect of data visualization is poorly or seldom used in later stages when we are communicating our findings, specifically with respect to uncertainty. \n\nUtilising visualisation can give people a more complete understanding of risks. Studies asking participants to sketch a distribution allowed them to better compute statistics about that distribution and improve predictions [@Hullman2018; @Goldstein2014]. While there is some evidence that confidence provided in text form only are less likely to be misinterpretated than grahics [@Savelli2013], text is insufficient to express more complicated aspects of a distribution, such as mass. The confusion caused by visualisation could also be due to a lack of expose, since @Kay2016 found that people exposed to the same uncertainty visualisation get better at making judgements the more they are exposed to them. Additionally, visualisation allows for interactive graphics that provide a more in depth understanding of probability [@Potter2009a; @Ancker2009] and infographics that make uncertainty more accessible for people with poor numeracy skills [@Ancker2009]. \n\nDespite these benefits, there is a reasonable amount of annecdotal and survey evidence that we don't visualise uncertainty as often as we should. Some economists suggest that visualisation authors are responding to incentives that make it tempting to avoid visualising uncertainty, even if those incentives are based more in perception than reality [@Manski2020]. A survey conducted by @Hullman2020a found that majority of visualisation authors agreed that expressing uncertainty is important and should be done more often than it currently is, some even agreed that failing to do so is tantamount to fraud. Despite this, only a quarter of respondents included uncertainty in 50% or more of their visualisations [@Hullman2020a]. Meaning participants were convinced that visualising uncertainty is morally important but were able to provide self sufficient reasoning that allows them to avoid doing it. The study by @Hullman2020a found that the most common reasons authors don't visualise uncertainty in practice despite knowing it's moral importance are: not wanting to overwhelm the audience; an inability to calculate the uncertainty; a lack of access to the uncertainty information; and not wanting to make their data seem questionable [@Hullman2020a].\n\nIf decision markers are not presented with the uncertainty about an estimate the data analysts have, for all intents and purposes, made the decision for the decision maker. Upon further interviews @Hullman2020a found that authors believed uncertainty would overwhelm the audience and make their data seem questionable because decision makers are unable to understand uncertainty. This belief, while pervasive, is not true. While some research suggests that laypeople cannot understand complicated concepts in statistical thinking (such as trick questions on hypothesis tests or the difference between Frequentist and Baysian thinking) [@Hoekstra2014; @Bella2005] there is a large amount of research suggesting that presenting uncertainty information improves decision making, both experimentally [@Joslyn2012; @Savelli2013; @Kay2016; @Fernandes2018] and in practice [@Al-Kassab2014]. As a matter of fact, doing what many authors currently do (providing only a deterministic outcome with no uncertainty) causes decision makers to be *less* decisive and have completely unbounded expectations on an outcome [@Savelli2013]. This reality cannot be avoided by providing secondary or non-specific information such as explaining calculations [@Joslyn2012], explaining the advantages of a recommendation [@Joslyn2012], or expressing uncertainty in vague terms [@Erev_1990; @Olson_1997], all of which are undesirable for decision makers and lead to measurably worse decisions [@Joslyn2012; @Erev_1990; @Olson_1997]. Expressing uncertainty verbally additionally decreases the percieved reliability and trustworthiness of the source [@VanderBles2020].\n\nNot only does communicating uncertainty improve decisions but the mistrust created by communicating certainty in uncertain situations can be exploited. A 6-month survey of anti-mask groups on Facebook during to COVID-19 pandemic showed that the anti-maskers thought carefully about their grammer of graphics and made pursuasive visualisations using the same data as pro-mask groups by exploiting information ignored by the pro-maskers [@Lee2021]. It is understood that deceptive plots can lead viewers to come to incorrect conclusions or significantly overstate effects [@Pandey2015] but these incorrect takeaways cannot be mitigated with instructions in how to correctly understand the plot [@Boone2018]. This evidence indicates we are more likely than not to hurt our message when we ignore uncertainty information and trying to raise the general publics plot literacy is an insufficient strategy to curb conspiracy theories and misguided scientific communication.  In direct contrast to this, displaying numerical estimates of uncertainty information has shown to lead to greater trust in predicitions [@Joslyn2012; @VanderBles2020]. While @Han2009 found people have more worry when presented with uncertainty reguarding health outcomes, this worry is not a bad thing if the concern is warranted given the ambiguous situation.\n\nThe disconnect between the research supporting uncertainty and the consensus aganinst may not be entirely driven by a lack of understanding of the literature. For example, at least one interviewee from the study by @Hullman2020a claimed that expertise implies that the signal being conveyed is significant, but also said they would omit uncertainty if it obfuscated the message they were trying to convey [@Hullman2020a].  Other authors who were capable of calculating and and representing uncertainty well did not do it, and were unable to provide a self-satisfying reason why [@Hullman2020a]. These conflicting motivations are acknowledged in the paper itself where @Hullman2020a says:\n\n> \"It is worth noting that many authors seemed confident in stating rationales, as though they perceived them to be truths that do not require examples to demonstrate. It is possible that rationales for omission represent ingrained beliefs more than conclusions authors have drawn from concrete experiences attempting to convey uncertainty\". \n\nAn overwhelming consensus among visualisation authors seems to be that uncertainty is secondary to estimations. There is a belief help by those that work with data that the uncertainty associated with an estimate (the noise) only exists to hide the estimate itself (the signal). From this view, uncertainty is only seen as additional or hindering information, therefore despite its alleged importance, when simplifying a plot uncertainty the first thing to go. This belief is also reflected in the development of new uncertainty visualisations. Often when trying to visualise a high problem, uncertainty is relegated to unimportant aesthetics in the plot, often of lower importance than the estimate [@Correll2018; @Lucchesi2017]. This is not uncommon with high dimensional data considering spatial temporal data often Cases where uncertainty is not relegated to an undesirable aesthetic instead incorporate interactivity to allow users to explore the complicated space themselves [@Potter2009; @Potter2009a]. Even the literature about uncertainty communication expresses an implicit belief that it is of secondary importance to the estimates or context of the data.\n\nThe use of uncertainty in high dimensional environments is especially important in energy data. Large models that incorporate spatial-temporal data from many sources and systems are used to predict energy uses in the short and long term. Understanding how to improve and make better decisions in these models is imperative in both the daily operation of the energy sector as well as the transition from fossile fuels to clean energy. Therefore the energy sector is an incredibly relevant application of research in uncertainty visualisation techniques.\n\n\n{{< pagebreak >}}\n\n\n\n# Thesis Overview \nThe overarching theme of my thesis is a change in the way we understand uncertainty visualisations, specifically in the case of communication. This work will be divided into three chapters.\n\n## Chapter 1: A new theoretical framework\nChapter 1 discuss the current state of the research surrounding uncertainty visualisation and describe two fundamental mistakes in the way researchers conceptualise uncertainty visualisations. The first mistake is in the role of distributions in the visualisation framework, where irrelevant distributions are used to answer questions and relevant distributions are ignored. These distribution issues are mistakenly reported as visualisation issues in the literature, ignoring a fundamental statistical problem in the way we visualise uncertainty. The second mistake is a belief that uncertainty visualisation will improve while uncertainty is seen as of low importance. I highlight how a lot of research in imporoving uncertainty visualisation reflects the belief that uncertainty is inherrently unimportant. I suggest that there is no overarching best uncertainty visualisation, but rather uncertainty visualisations should depend on a motivating question. Finally I provide a framework for visualising uncertainty that mitigates these issues by ensuring the visualisation author decides on the motivations and of their graphic, can correctly identify the relevant distribution and the aspects of the distribution that are needed for their motivation; and correctly assigns priority to these apects by using the correct aesthetics.\n\n## Chapter 2: Applications of the framework\nChapter 2 applies this framework and investigates its practical usefullness through experiemnts and discussions with AEMO. The purpose of data visualisation is insight, but due to time limits or other constraints, most visualisation studies use multiple benchmark tests as a substitute for measuring the complicated phenomena of insight [@North2006]. Unfortunately the validity of these results hinge on the large insights we gain from graphics being the sum total of these small incremental insights which may not always be true [@North2006]. Specifically in uncertainty visualisation, there is a focus on performance and accuracy based measures that assume more predictabe behaviour from people than what research on human decision making suggests. The work in Chapter 2 should avoid these common pitfalls that arrise from experimental plot evaluations by working closely with people at AEMO. Our partners at AEMO will be able to provide detailled and open ended descriptions of the beenfits and struggles of our uncertainty visualisation techniques. This allow us to directly see the improvements (or lack thereof) in insight due to the suggested framework.\n\n## Chapter 3: Translation\nChapter 3 will be a translation of chapter 1 and 2 into an R package. This will make this research more accessible and allow others to easily implement this visualisation framework in their own work.\n\n\n{{< pagebreak >}}\n\n\n\n# Chapter 1: A new theortical framework\n## An overview of the current ideas\nCurrent research in visualising uncertainty seems to have a focus on designing new options for visualising uncertainty for specific types of data. Typical papers comparing visualisation methods seem to fit into one or both of the follow categories:\n\n1) a paper that suggests or compares uncertainty visualisations that depict a single distribution.\n\n2) a paper that suggests or compares uncertainty visualisations for a specific type of data  \n\nGenerally the goal seems to be to increase the number of options we have in our \"visualisation bag\" so that we always have a plot at the ready if we want to visualise uncertainty. These new plots also tend to focus on a catch all plot that is \"best for making decision\" or \"best for visualising spatial uncertainty\". Data visualisation is commonly utilised as a tool in data exploration, so it is not uncommon for a data analyst to make a plot with only a vague goal and use it to pull out a large number of adjacent observations. For example, a data analyst might make a scatter plot to find out the relationship between two variables, but in doing do also finds out that one variable has discrete outcomes over a range of values. It is common knowledge that any single plot cannot reveal all information about a data set, all plot types obfuscate and uncover different types of information, however this aspect of plotting becomes more apparent in uncertainty visualisation. Often when discussing \"uncertainty\" information, we expect readers to be able to draw information from a plot that was not estimated, prioritised, or visualised. Communicating uncertainty can be boilled down into two simple steps, first we need to quantify the uncertainty, and second, we need to communicate it [@Webster2003]. The two issues I have noticed in uncertainty visualisation literature each come from one of these steps. The first issue is failing to identify the correct distribution (quantifying) and the second is failling to select a visualisation that highlights the important aspects of the distribution (communicating). These issues run deep in the literature, but they are easiest to understand with an example that I will repeatedly return to as we develop this idea.\n\n## Visualising the wrong distribution\n### An example: comparing HOPs, error bars, and violin plots\nThe issues that are rampant in the uncertainty visualisation literature are easily seen if we zoom in on one example. The study done by @Hullman2015 is a great illustration in the importance of having a clear motivation when you design a graphic. It is important to keep in mind that while I am primarily discussing one paper to illustrate a point, the issues I am brining to light are the standard in the uncertainty visualisation literature. This paper is no outlier.\n\nThe study by @Hullman2015 asked participants to provide some numerical properties of a distribution using a hypothetical outcome plot (HOPs), an error bar plot or violin plot. Participants were given questions relating to individual and multiple distributions. When shown a single distribution participants were asked about the mean of the distribution, the probability of an outcome being above some threshold (indicated on the plot with a red dot), or the probability of an outcome being between two given values. When shown two distributions the participants were asked \"How often is measurement of solute B larger than the measurement of solute A?\", and when shown three distributions \"How often is measurement of solute B larger than the measurement of solute A and solute C?\". @fig-examples shows an example error bar plot and violin plot for the two distribution case. There was also a high and low variance case for every plot and question combination. The study decided which technique was better using absolute error between the subjects response and the true value. They specified that the error bars are not confidence intervals as they represent the true underlying distribution, not the sampling distribution of the mean. They found that the HOPs reliably outperformed the violin and error bar plots for all the two and three distribution questions, but were no better than the violin or error bar plot in the other univariate cases (and in some cases worse). \n\n\n::: {#fig-examples .cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![Error bar plot](confirmationreport_files/figure-pdf/fig-examples-1.pdf){#fig-examples-1}\n:::\n\n::: {.cell-output-display}\n![Violin plot](confirmationreport_files/figure-pdf/fig-examples-2.pdf){#fig-examples-2}\n:::\n\nAn example of the plots shown in the two variable tasks given in [@Hullman2015]. The example question provided with this plot was 'In what percentage of vials is there more of solute B than A (Probability(B > A)?'\n:::\n\n\nMy first issue with the plots in @Hullman2015 study is that the violin plot and error bars are visualising a different distribution to the HOPs plot. The error bar plot and the violin plot in @fig-examples visualise the marginal distributions of solutions A and B and provides no information on the joint distribution. The HOPs plot is similar to a slope graph except instead of connecting the outcomes from A and B with a line, they are connected through frames of an animation. The HOPs, just like a slope graph, depicts a relationship between two varibles, i.e. the *joint* distribution. This key feature can be used to improve upon the HOPs and direct our attention to the distribution that is even more useful for answering this question.\n\nTwo alternative graphics that could be used to answer the question \"In what percentage of vials is there more of solute B than A (Probability(B > A)?\" are provided in @fig-alternatives. The scatter plot depicts the joint distribution of the two solutions and uses colour to highlight the Bernoulli distribution that is more closely aligned with the question. The stacked bar exclusively visualises the Bernouli distribution that describes the event $B>A$ and ignores the joint distribution highlighted by the scatter plot. Through this process of moving from the marginal distributions, to the joint distribution to the bernouli distributionm we moved the information needed to answer the question \"What is $P(B>A)$?\" from something you needed to calculate in your head (when looking at the error bar plots) to something you can **see** in the bar chart. While this process is illuminating, it is important to avoid whittling down the problem **too** much. Providing a categorical decision alone is somewhat useless, it is important to ballance advice with uncertainty estimates as a ballance of the two results in the most accurate decisions [@Joslyn2012]. \n\n\n::: {#fig-alternatives .cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![Scatter Plot](confirmationreport_files/figure-pdf/fig-alternatives-1.pdf){#fig-alternatives-1}\n:::\n\n::: {.cell-output-display}\n![Stacked Bar Chart](confirmationreport_files/figure-pdf/fig-alternatives-2.pdf){#fig-alternatives-2}\n:::\n\nTwo plots that could be used as alternatives to answer to question 'In what percentage of vials is there more of solute B than A (Probability(B > A)?'. The scatter plot in (a) focuses on the relationship between the concentration of solute A and solute B, while the stacked bar chart in (b) highlights the frequency with which each solute is greater than the other.\n:::\n\n\nNot only is the distribution depicted in the visualisation different, but the features of the distribution depicted are also different. In discussing the concept of a \"best\" visualisation, we rarely discuss which *features* of the distribution are being displayed in the graphic. The way we currently look at visualisation would classify the error bar plot and the violin plot as visualisations of a \"distribution\", the scatter plot is a visualisation of a \"relationship\" while the bar plot is a visualisation of \"amounts\" (@wilke2019fundamentals), but this categorisation hides a lot of important details about drawing information from a graph. In this example the violin plot and the scatter plot both showed that each solution had an independent marginal normal distribution, the error bar (although technically a plot for visualising distributions) gives no concept of mass and would not give you the ability to identify even a simple distribution. Not only is it important to select a distribution that is appropriate for our question, it is also important to *show* the aspect of that distribution that holds the relevant information. The example asked about the *frequency* of a particular outcome which translates to visualising the *outcomes* of the joint distribution **or** the *parameter* of the Bernoulli distribution since these are the aspects of each distribution that hold the information we are looking for. If we visualised features of the distribution that don't hold the information we are looking for, the information is difficult to assertain and is more likely to be found through potentially faulty heuristics.\n\n@fig-bad depicts two graphics that have the of the same distributions as those in @fig-alternative, but each plot visualises an aspect of the distribution that is less relevant to the question. The 2D error bar plot (or circle?) highlights the mean and the values that are within the 95% confidence range. Since none of the parameters of the joint distribution are directly related to the question at hand, visualising the mean and significance instead of the outcomes made it harder to answer \"What is $P(B>A)$?\". Since the parameter of the Bernouli distribution *is* directly related to the question, visualising outcomes makes it harder to answer the question.\n\n\n::: {#fig-bad .cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![2D error bar plot](confirmationreport_files/figure-pdf/fig-bad-1.pdf){#fig-bad-1}\n:::\n\n::: {.cell-output-display}\n![Scatter plot of bernouli](confirmationreport_files/figure-pdf/fig-bad-2.pdf){#fig-bad-2}\n:::\n\n::: {.cell-output-display}\n![alternative to 2d error bar plot](confirmationreport_files/figure-pdf/fig-bad-3.pdf){#fig-bad-3}\n:::\n\nTwo plots that are not useful in answering the question 'In what percentage of vials is there more of solute B than A (Probability(B > A)?'. While the distributions visualised are correct, we have visualised the incorrect feature of the repsective distributions. Plot (a) visualises mean and significance thresholds of for values (b) the.\n:::\n\n\nNot only does visualising the incorrect distribution that highlights incorrect features related to a question make it difficult to answer correctly, it might also completely disorientate the viewer. In the discussion of the study @Hullman2015 notes: \n> \"In light of the common usage of error bars for presenting multiple independent distributions, it is noteworthy how poorly subjects using these representations performed on tasks asking them to assess how reliably a draw from one variable was larger than the other...Many subjects reported values less than 50%, which are not plausible given that the mean of B was larger than the mean of A.\"\n\n\n### Selecting the correct distribution\nIn order to fairly compare two uncertainty visualisations, they need to provide the same information. This is rarely done in plots comparing distribution visualisations. The important distinction between the distribution displayed and the distribution required to answer a question is often ignored in our discussions good or bad uncertainty visualizations. This means that studies identifying some graphics as better than others may only do so because the two plots displayed different distributions. @fig-distdraw illustrates how we think about distributions when performing statsitical tests compared to when we create visualisations. When we compute statistics or perform a hypothesis test, we typically put a lot of thought into what the correct distribution is for our question, however, when we perform visualisation we typically plot a collection of normal marginal distributions and ignore the actual distribution required. This extends to our perception of visualisation in general as most of the visualisation we consider to be for \"distributions\" are actually just tools for visualising marginal distributions. \n\nThe relationship between plots and the distributions they present are not entirely alien to the current literature of visualisaiton, it is just a topic that is touched on sporadically.  @Wickham2011 discusses product plots and how different displays result in depictions of different marginal, conditional, and joint distributions. A common complaint about choropleth maps is that they depict distributions that depend on populations as depending on land, and visualisation that correct for this, such as hex maps effectively correct for this [@Kobakian]. The lineup protocol expands the idea of considering the distribution that we are visualising and expands it to considering a visualisation to be a single outcome of a larger distribution and ask visualisation authors to consider the distribution that might have generated a plot [@Buja2009; @Wickham2010]. This method can be used to identify false findings and perform complicated statistical tests [@Chowdhury; @Hofmann2012]. Unfortunately visualisation papers that properly consider the information depicted in a plot and visualisation papers that identify better depictions of uncertainty distributions are often mutually exclusive.\n\n![Illustration depicting the difference between the distributions we use for testing vs the visualisation we visualise](incorrectdistributions.jpeg){#fig-distdraw}\n\nAn example that might be more familiar to the average statistician is something I like to refer to as \"the error bar problem\". If you have ever looked at two overlapping error bars and said \"oh these variables are not statistically significantly different\" you have used error bars incorrectly. While it is true that error bars that do not overlap implies statistical significance, overlapping error bars do not imply the converse in true. The same is true for a lot of the other ad-hoc statistical tests we use error bars for. A study done by @Bella2005 asked participants to adjust two error bars until the means were \"just\" statistically significantly different, and most people adjusted the error bars until they were just touching. In the case of independent confidence intervals that just touch, the p-value of the associated two-tailled t-test is about 0.006 [@Schenker2001]. @Bella2005 also found that few people could incorporate changing information about independence that arrises from repeated measure design and most participants were ignorant to the fact that error bars are used for both confidence intervals and standard error bars, two wildly different indicators of precision. \n\nThis is a classic example of expecting readers to draw conclusions about a distribution that was not visualised. Error bars typically represent the 95% confidence interval of a sampling distribution, most commonly the significant values of a t-distribution. This means that each error bar provides the range of significant values through the two end points, and a vague indicator of variance with the length of the bar, *of each variable independently*. What an error bar does **not** depict is the t-distribution associated with a difference of two means, the equivalent statistical test we utilise error bars for. The visualisation itself is not the problem, trying to draw conclusions that requires information that was not visualised is.\n\nThe papers that go on to cite these misunderstandings about error bars discuss the work as though the problems are caused by error bars themselves. They suggest that error bars should be avoided as a visualisation tool, ignoring the fact that these fundamental misunderstandings in uncertainty will likely follow other visual encodings of t-distributions. Maybe some visual aspect of error bars encourages these extrapolations, but the HOPs example I drew on at the start of this chapter illustrates that this problem of expecting people to answer questions that are not directly related to the visualised distribution is larger than questions about significance. \n\nThis view of understanding uncertainty visualizations opens up a new gap in the literature. For example, if we accept that visualizing a collection of t-distributions is not a substitute for a collection of F-tests or paired t-tests, how *should* we visualise uncertainty if we want to draw those conclusions? Rather than shutting down the discussion on the usefulness of visualisation, I beleive it opens it and highlights areas of improvement in our current work.\n\n### Idenfitying the relevant features\nDifferent features of a distribution have different questions they are mode adept adept at answering. The apsect of a distribution that are typically depicted in graphics can be organised into four key features. @fig-features depicts these four features along with an example of how they are typically depicted in graphics. Each of these four features have a set of questions they are better equip to answer. The four features are:\n\n1) **Mass** describes the PMF/PDF or CMF/CDF of the functions. Depictions of mass can inform us of the mode, likely or impossible values, and whether or not we have an identifiable distribution. \n\n2) **Samples** are a set of actual or simulated outcomes of a distribution. Our data falls into this category as it can be seen as an outcome of some \"underlying\" distribution. Outcomes can also be simulated through techniques such as bootstrapping or random sampling. Outcomes are useful to answer questions about frequency, however since outcomes have a connection to mass (depending on the structure of the graphic, a mass plot may just be a smoothed plot of a visualisation of a sample) they can sometimes be used to answer the same questions.\n\n3) **Parameters** are the statistics that are related to our distribution. They can be the sufficient statistics of the distribution, such as the mean, variance, minimum and maximum, or they can be other statistics such as the or correlation, median and mode. Visualising a specific parameter of our distribution typically gives us freedom because it allows us to express any aspect of a plot in terms of a single value. Unfortunately this flexibility means that the set of question questions a parameter plot can answer are limited. Questions about the mean, median, maximum, minimum, correlation, values of significance, etc would usually need multiple plots to answer.\n\n4) **Exchangability** illustrates whether or not a elements in a sequence of random variables from this distribution can be swapped. Rather than answering questions about exchangability, it is something we typically want to highlight in our data. Exchangability is often expressed by having features touch in a graphic. A time series inechangability is expressed using a line plot, spatial data's inechangability is expressed using a map and the exchangability of randomly sampled data is often expressed using points. Exchangability is adjacent to continuity because visually connected features in a graphic are also used to express discretised outcomes of a continuous process (such as a ridgeline plot).\n\n![The four main features of a distribution graphics are typically used to represent](distfeatures.jpeg){#fig-features}\n\nThis is not an exhaustive list of every possible feature of a distribution, but rather the four most commonly visualised aspects of a distribution. Additionally, these features are not entirely distinct, particular visualsiations of parameters or samples may also double as a depiction of mass. There are uncertainty visualisation taxonomies that cross the boundaries of this taxonomy or introduce, for example @Grewal2021 categorised uncertainty visualisations based on discreteness of the distribution (a sub aspect of mass) and the domain expertise (which was similar to the number/complexity of the features depicted).\nConsidering this, a graphic that is designed to depict one feature of a distribution may depict multiple features of multiple distributions. For example, the scatter plot in figure , in the same way that a depiction of a particular distribution may depict multiple.\n\n\n### The scope of the problem\nExpecting readers to draw information about a distribution that was not is not the only problem with quantifying uncertainty, but it is the apsect of the issue with a clearer solution. An adjacent issue is *how much* uncertainty we should include when trying to quantify our associated distribution. Obviously the correct answer is somewhere between \"every possible outcome\" which would result in an unbounded uncertainty and \"a simple confidence interval based on a set of strict and unrealistic assumptions\" that would result in an interval that is far too narrow. Unfortunately between those two extremes the solution is largely a judgement decision which can sometimes be overwhelming. This is why software that provides *some* uncertainty visualisation as a default, such as forecasts in the `fable` package are useful [@fable]. It prevents authors omitting uncertainty through inaction. There is the possibility, however, that default uncertainty visualisations facilitate the poor understanding of which conclusions are relevant to the uncertainty visualised.\n\nThis issue is not helped by the fact that the term \"uncertainty\" lacks a commonly accepted definition in the literature. @Lipshitz1997 even commented that “there are almost as many definitions of uncertainty as there are treatments of the subject” . This mishmash of terminology leads to a large body of work, all claiming to finding the best visualisation or expression of of \"uncertainty\" but most don't even seem to agree on what uncertainty is. The most encompassing definition of uncertainty I have seen comes from @utypo who define uncertainty as **\"any deviation from the unachievable ideal of completely deterministic knowledge of the relevant system\"**. This definition does not completely align with the distribution conceptualisation I discussed earlier in this chapter, but the cases where they diverge are rare and can still be handled by focusing on the relevant information. More commonly, uncertainty is defined using a taxonomy rather than a strict definition. There are a few of taxonomies for uncertainty, but, just like the definition, most of them are a subset of the one laid out  by @utypo.\n\n@fig-taxonomy is an illustration of the taxonomy presented by @utypo. In this taxonomy, there are three things we need to consider for each \"uncertainty\" we encounter through the modelling process. First, consider the source of the uncertainty. Is this uncertainty coming from inaccurate measurements or a poorly defined model? This is the *location* of the uncertainty. Second, consider how well you can quantify this uncertainty. Do you know exactly how much measurement error there is in each observation or are you not even aware if there is or isn't measurement error? This is the *level* of your uncertainty, and it ranges from discrete to total ignorance. Finally, consider how this uncertainty came into existence. Is it a result of a naturally random process (epistemic) or is it due to imperfect information and could be improved (aleatory). This is the *nature* of your uncertainty. @utypo then go on to describe mapping our uncertainty in a 3D space that is defined by its location, level, and nature, but I think the taxonomy is more easily understood as a series of questions we need to consider when asking how much unertainty we should express to our audience.\n\n![Illustration of the taxonomy described in @utypo](taxonomyvis.jpeg){#fig-taxonomy}\n\nWhile information about the sources of our uncertainty and the type of uncertainty may seem like an unimportant secondary step in uncertainty visualisation, communicating these features of uncertainty helps decision makers make more informed choices. @Padilla2021 found that low forecaster confidence or high model uncertainty both contribute to more conservative judgements by decision makers. Failling to communicate the nature of your uncertainty can result in underestimation or overestimation of failure probabilities [@Kiureghian2009]. Additionally @Gustafson2019 found that the framing of our uncertainty, (i.e. informing the reading if the uncertainty came from a lack of knowledge, approximations, unknown unknowns, or disagreement among parties) does not have a significant effect on the belief in the estimates, perceived credibility, or behavioral intentions of the decision makers. This means communicating secondary information about your uncertainty is unlikely to negatively affect your communication and is important in understanding the scope of your uncertainty.\n\n## The implicit importance of uncertainty\nOften our goal in visualisation is not to perform a statistical test or to make a decision based on a single estimate, but rather to identify underlying structure in our data or make a decision on a large number of connected systems. When there are many features of the data that are competing for our interest, we often don't have as much flexibility in how we choose to visualise uncertainty. Complicated features of our distributions, such as mass, are thrown to the wayside to make way for for complicated features of the data such as spatial dependence.\n\n### An example: spatial uncertainty\nTrying to depict the uncertainty of an estimates with a spatial aspect is incredibly difficult. Even displaying an estimates in a spatial context can have problems. Cartograms perform poorly on countries such as Australia which is largely vast open spaces contsrasted by dense  coastal cities [@Kobakian]. Maintaining the geographical structure while avoiding incorrect \n\n\n\n- check what kind of question they can answer\n\n- [@Correll2018] They make value-suppressing uncertainty palettes. Suggest using it to endocde the uncertainty in the colour apect of the graph (should be directly integrated in a shared chart). Found that Juxtaposed maps, by introducing a second search task to the identification task (searching for the proper values in two, rather than one map), would have poorer performance than other conditions. (read the paper to find out details)\n\n- [@Lucchesi2017] This paper is discusses different ways to present uncertainty in a map. A lot of them made it easy to SEE that there was uncertainty, but it was still difficult to compare across locations and tell if areas are statistically the same (which I would argue is a large part of uncertainty). Rotation is pretty useless, the pixelation was interesting but had above issue, the grid colour wasnt a bad idea, but I wouldnt use a second colour in the scale (if i did use diverging palet, it would be on the statistic).\n\n- Because we view uncertainty as a secondary feature of the data, and not as something we have constructed, we don't think about uncertainty visualisations with the \"goal of the uncertainty\" in mind.\n\n### Hierarchy\n*general hierarchy*\n- Why do we need a plot that can \"see\" everything? Plots are similar to people in the sense that expecting one person to have a complete and flawless view of something as complicated as society ignores that one of the things that has allowed humanity to flourish is that there are so many of us. It allows each person to specialise and be good at something important to them rather than trying to be mediocre at everything. Similarly, no plot can see every possible thing you might want to know about your data. Leaning into and uncovering the strengths of each plot is the best way to research them because it reflects the ways in which we actually incorperate plotting into our diagram. While general principals are helpful, fundamentally each plot for each question should be as unique as the person who made it.\n- mapping the most important feature to the highest aspect\n- placebo rule, studys compare to placebo is bad, should compare to next best alternative (should compare plots that have the feature of interest at a similar importance ranking)\n- Ask yourself what questions can be answered with each aesthetic of the plot. Is this the hierarchy of information you want to express.\n- the visualisation we use to find information out for ourselves do not need to be the same visualisations we use to communicate those interesting findings. using one graphic to get a lot of information makes sense in the exploratory stage, but less so in the communiating stage. Our goal is not to present a lot of information so that we might find the information we want, it is to give the relevant information for a deicison or for understanding. Instead of making visualisations that tell us interesting features, we need to pick visualsiatin that highlight the interesting features we have found.\n- when you use plots for EDA you often will make hundreds to try and find interesting features, when you present a plot for communication or decisions, you can only show a small handful.\n\n\n*Too many features*\n\n- What should become apparent comparing these plots is that there is a constant trade off between information relevant to the our question and secondary information that provides context. \n\n- INATTENTIVE BLINDNESS\n\n- There are only so many aesthetics we can map information to, and those aesthetics have a hierarchy of how easy they are the process. Additional secondary information can simultaneously give important context and clutter the visualisation, so it is important to understand our motivations when creating a visualisation. \n\n- The people who don't plot uncertainty because they think its unimportant and the people who relegate uncertainty to the lowest ranking aesthetic on the list are both saying the same thing, \"I think the uncertainty is unimportant\". Including uncertainty is worth very little if its impossible to interpret and the last thing I notice in the plot. \n\n*Additional conflicting information*\n- It is important to avoid highlighting irrelevant aspects of the plot as that will confuse viewers and make it harder for them to draw conclusions.\n\n### How to include uncertainty\n\n*Natural Mappings*\n\n## Organise Paper Notes\n\n- [@Wickham2012] Glyph maps are a good way to visualize spatial temporal data (read: not uncertainty). The ordering is determined by space, and the glyph is a line plot. Different scales allow us to look at trends in global variance, local variance. Aggregation allows investigations of trends.\n\n- [@Hofmann2012] It compares polar and cartesian co-ordinates and finds that polar co-ordinates are a higher powered test. It also compares multiple ways of showing distribution to see which has more power.\n\n- [@Kale2021]. Adding means to uncertainty visualizations has small biasing effects on both magnitude estimation and decision-making, consistent with users relying on visual distance between distributions as a proxy for effect size visualization designs that support the least biased effect size estimation do not support the best decision-making, suggesting that a chart user’s sense of effect size may not necessarily be identical when they use the same information for different tasks.\n\n- [@Nathonours] finds that a scatter plot is better than a line plot if you want to convey the correlation between two time series. Considering the correlation is a feature of the joint distribution, it makes sense that a depiction of the joint distribution (a scatter plot) outperformed between the two time series, and a line plot is the marginal distribution of each time ser (its also like a flipped slope graph?)\n\n- [@Gschwandtnei2016] Could have to do with making sure you match your question with an intuitive plot feature. The paper looks at different types of temporal uncertainty and discusses the differences and how to best visualise each one. Has some interesting methods to visualise intervals. Gradients were best for probability and ambiguation was best for start and end time estimation\n\n- [@Fernandes2018] This paper has several visualisation specific findings. We found that cumulative distribution function (CDF) plots and low-density quantile dotplots produce more accurate and consistent decisions compared to other uncertainty visualizations, textual displays, and displays with no uncertainty. The types of displays that we have found to be the best for supporting decision-making in the transit have also been shown to be more accurate at estimating probability intervals more generally. Alternatively their question was best answered with the CDF (what is the probability I will miss the bus if I leave now). Reflective with real costs and benefits which are not always quantifiable, they have to weigh those up on their own.\n\n- [@Padilla2022] visualizing the same data in different ways had differential effects on the changes we observed in risk perceptions. Cumulative y-axis will most reliably lead to people perceiving greater pandemic risk because cumulative y-axis can show only an upward or flat trend. In contrast, incident y-axis can be highly variable and even reduce viewers’ perceived pandemic risk, if the charts exhibit a downward trend. Basically aspects on the plot other than the the distribution and hierarchy also have an effect on the way the plot is percieved (and what information is highlighted)\n\n- [@Vanderplas2015] plotting features can work for you or against you. The sine illusion causes bars on a sine curve to seem longer at the peaks and troughs (because people view the perpendicular area). This means increases in variance in a time series plot are harder to identify if they are paired with an increase in slope. The illustion is very pervasive and the you have to go to extreme measures to remove its effect.\n\n- [@Maceachren2012] The first paper organises different types of uncertainty and then matches each type of uncertatiny to a visual aspect of a graph. Experiment 1 tested the extent to which symbols have intuitive applications. Participants response times were the same across symbols, etc. Found fuzziness, location, (and sightly less value) were the best for visualising uncertainty. Arrangement, size and transpacency slightly less so. Saturation, hue, orientation and shape are unacceptable. Only one direction was deemed intuitive by participants (fuzziness: more fuzzy=less certain; location: further from center=less certain; value: lighter=less certain; arrangement: poorer arrangement=less certain; size: smaller=less certain; transparency: more obscured=less certain). While iconic sign-vehicles can be more intuitive and more accurately judged when aggregated (than are abstract sign-vehicles), the abstract sign-vehicles can lead to quicker judgments.  Experiment 2 assessed task performance when multiple symbols appeared.\n\n- [@Hullman2018] Visualising uncertainty as a set of discrete outcomes improves recall of sampling distribution.\n\n\n{{< pagebreak >}}\n\n\n\n# Timeline\nA timetable for completing the thesis and a statement of progress to date (make sure to match everything to apsects of the thesis)\n- (done) Literature review (have made progress in this year) have picked up main parts of the infoviz community - changed topic once got new scholarship (check off to say completed)\n- can pencil in energy aspect now (met with them earlier in the year) - have a need for this work \n- next meeting in June (and have regular meetings monthly over the next year and hopefully build up applications)\n- Have idea of when I think I will have a finished piece of work (literature review)\n- Content for each of the pieces should be mapped out\n- ASC later in year (submitted an abstract for a poster)\n- getting design of experiment clear and question we want to answer clear can go round and round (good idea to have something simple to test) - dont have time for something too complicated in phd need to have it in small managable pieces\n- ASC december 2023 - apsect of literature review + results from preliminary experiemnt\n- IEEE VIS (June 22 2023 poster abstract) - Conference October 2023 (present aspect of literature review)\n- The Rstudio conference (posit conference) - hard to get a spot (worth applying for)\n- Aim for something international in a year (or in last year)\n- Also could do UseR! 2024 (or 2025) & talk about some of the software developed (technical output) \n- Could have an energy specific one (various applied energy)\n- IEEE VIS (march 31st paper due in 2025) - Conference October 2025 (Vienna)\n- Thesis due December 2025\n\n\n{{< pagebreak >}}\n\n\n\n# Bibliography\n\n\n",
    "supporting": [
      "confirmationreport_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}