{
  "hash": "20a5b5772e0d76577c467d10c8621c4c",
  "result": {
    "markdown": "---\ntitle: \"Plotting Apples, Oranges, and Distributions\"\nsubtitle: \"Issues behind the perception of uncertainty visualisations\"\nauthor: Harriet Mason\nbibliography: references.bib\ndate: last-modified\nformat: pdf\n---\n\n\n- Entire report needs to be 10-20 pages\n\n# Motivation/Literature Review\n- Explanation of several reasons uncertainty visualisation needs to be investigated:\n\n## Importance of visualising uncertainty\nA survey conducted by Dr Jessica Hullman found that majority of visualisation authors surveyed agree that expressing uncertainty is important and should be done more often than it currently is, some even agreed that failing to do so is tantamount to fraud [@Hullman2020a]. Despite this, only a quarter of respondents included uncertainty in 50% or more of their visualisations [@Hullman2020a]. This means people are convinced that visualising uncertainty is important from a moral standpoint, but they have still been able to provide self sufficient reasoning that allows them to avoid doing it. \n\nWhile some people may have a lower baseline than the general public, most people get better at understanding uncertainty plots the more they are exposed to them [@Kay2016].\nRefusing to express uncertainty because people don't understand them, prevents people from improving in their ability to understand the plots, causing those that may not be able to understand uncertainty to continue to be bad at it.\n\n- [@Ancker2009] Interactive graphics force people to learn probability and therefore they have a more accurate estimate and understanding of the real probability. Inforgraphic style interactive methods are better for people with poor numeracy skills.\n\n- [@Savelli2013] Participants using predictive intervals were better able to identify unreliable forecasts, expected a narrower range of outcomes, and were more decisive than were participants using deterministic forecasts. Almost no misinterpretations occurred when the predictive interval was expressed in text alone. Participants given the deterministic forecast were less decisive and they thought almost anything could happen.\n\n- [@Pandey2015] Visualising uncertainty well and honestly is important. Deceptive plots led most participants to significantly overstate the message or give an incorrect response. A line chart, then bubble and bars having the biggest effect.\n\n- [@Lee2021] Do a 6-month long observational study of an anti mask group on facebook. Also do analysis on how visualisations spread on twitter. Pro and anti mask groups draw drastically different inferences from similar data. The anti-maksers think carefully about grammar of graphics and make polished and persuasive visualisations. Binary opposition of literacy/illitercy is insufficien. Visualisation researchers do not have a robust body of understanding of how and when to communicate uncertainty.\n\n- [@Goldstein2014] In particular, we find that eliciting an entire distribution from a respondent using a graphical interface, and then computing simple statistics (such as means, fractiles, and confidence intervals) on this distribution, leads to greater accuracy, on both the individual and aggregate level, than the standard method of asking about the same statistics directly.\n\n- [@Hullman2018]Users sketch predictions of the uncertainty prior to viewing the true sampling distribution. Find it is an effective way to improve prediction. Visualising uncertainty as a set of discrete outcomes improves recall of sampling distribution.\n \n- [@Joslyn2012] Uncertainty information improved decision quality overall and increased trust in the forecast.  Overall performance was not improved by either explaining the calculations or by explaining the long-run advantage of following the recommendations, \n\n- [@Han2009] They also had more worry when presented with the range than the point estimate which they attribute to ambiguity version.\n\n- [VanderBles2020] Uncertainty communication did not affect participants trust in the source or elicit psycological resistence. Numerical uncertainty in particular as a numeric range— does not substantially decrease trust in either the numbers or the source of the message. Verbal quantifiers of uncertainty, however, do seem to decrease both perceived reliability of the numbers as well as the perceived trustworthiness of the source. \n\n- [@Al-Kassab2014] There is a lack in the literature of practical evidence of the benefit of information visualization. The main contribution of this paper is to illustrate how, for a major European apparel retailer, the visualization of performance information plays a critical role in improving business decisions and in extracting insights from Redio Frequency Idetification (RFID)-based performance measures.\n\n- [@Boone2018] Doing visualisation is not important, doing visualisation correctly is important. Can't sidestep a bad visualisation with instructions. Instructions reduced misconceptions about the cone of uncertainty but some participants continued to misinterpret the plot. Instruction also caused participants to reduce their damage estimates. Mixed results for effectiveness of instructions, a more nuanced approach might be needed.\n\n## Why people dont visualise uncertainty\nThere is a large amount of literature providing new ways to visualise uncertainty and showing its effectiveness, but much less on why people don't do it. Hullmans study found that the most common reasons authors don't visualise uncertainty despite knowing the importance of it are: not wanting to overwhelm the audience; an inability to calculate the uncertainty; a lack of access to the uncertainty information; and not wanting to make their data seem questionable [@Hullman2020a]. It is hard to read that data visulisation authors have a general inability to calculate or access uncertainty information and remain unconcerned about general incompetence in the field, however this issue does not seem to be completely groundless. A strong definition of uncertainty, a consensus on which uncertainty should be incorporated and which should be ignored, as well as simple methods to visualise it seem to be largely absent in the literature. These gaps are worth discussing at length and form a large part of the motivation behind *Project 1*. Authors failling to visualise uncertainty due to a fear of overwhelming their audience, however, are motivations that can easily be discredited with the current literature.\n\nNot wanting to overwhelm the audience typically links back to a general belief that audiences struggle to understand uncertainty. Rather than asking for a blanket yes/no conclusion on the general publics ability to understand uncertainty, it would be more helpful to focus on the intricacies behind understanding uncertainty. Some research suggests that laypeople cannot understand complicated statistical technicalities (such as the difference between Frequentist and Baysian thinking) [@Hoekstra2014], nor can they reliably translate an error bar plot to an equivalent hypothesis test [@Bella2005], but they can make accurate and efficient decisions factoring the uncertainty into their choices [@Kay2016] [@Fernandes2018]. A study that had account for uncertainty in bus arrival times showed laypeople have ability to make fast and accurate decisions that accounted for uncertainty that was displayed on a small screen [@Kay2016].\n\nThis might suggest that visualisation authors are unaware of the research indicating that laypeople are able incorperate uncertainty into their decisisons, or they are using that reasoning because they themselves don't uncerstand their true motivations to avoid visualising uncertainty. For example, at least one interviewee from Hullman's survey claimed that expertise implies that the signal being conveyed is significant, but also said they would omit uncertainty if it obfuscated the message they were trying to convey; while other authors who were capable of calculating and and representing uncertainty well did not do it, and were unable to provide a self-satisfying reason why [@Hullman2020a]. These conflicting motivations devloid of sound logic are acknowledged by Hullman herself in the paper.\n\n> \"It is worth noting that many authors seemed confident in stating rationales, as though they perceived them to be truths that do not require examples to demonstrate. It is possible that rationales for omission represent ingrained beliefs more than conclusions authors have drawn from concrete experiences attempting to convey uncertainty\" [@Hullman2020a]. \n\nThe gap between what authors desire to visualise uncertainty and the decision makers desire to have uncertainty communicated to them also comes through in the methods we use to express uncertainty. While authors typically prefer to express uncertainty in vague terms and will use reasons such as the one above to justify it, decision makers prefer uncertainty in precise quantitative terms [@Erev_1990], [@Olson_1997]. There does not only seem to be a disconnect between the authors work and the desires of decision makers, but also a second disconnect that exists between the researchers who develop uncertainty visualisations and those that use them.\n\nMany of the reasons people don't visualise uncertainty have a common belief that uncertainty is secondary to estimations. The argument speaks to an unspoken belief of those that work with data, that the uncertainty associated with an estimate (the noise) only exists to hide the estimate itself (the signal). From this view, uncertainty is only seen as additional or hindering information, therefore despite its alleged importance, when simplifying a plot uncertainty the first thing to go. This belief is also reflected in the development of new uncertainty visualisations. \n- [@Manski2020] A discussion of the suggest that researchers dont visualise uncertainty because they are responding to incentives that make the practice tempting. He goes through a bunch of cases where people don't visualise uncertainty and concludes that expressing certitude (point predictions) at most has appeal in limited contexts. It should not be a general practice.\n\n### Cases where people want/need to visualise uncertainty but methods are lacking (such as spatial/temporal)\n- [Potter2009]: make a series of uncertainty plots that are not connected and allow the decision maker to investigate the relationships themselves using interactivity. They made a software that look at ensemble data (mutivariate spacial-temporal data with uncertainty). \n- [@Potter2009a] Sometimes your data has a time aspect and a spatial aspect. They introduce Ensemble-Vis, which is a software that consists of a bunch of linked interactive displays so you can view your data across time/space. Show that the linked display presents a clearer level data analysis.\n\n## Energy forecasts and importance of unertainty vis for clean energy (motivation)\n\n# Thesis Overview \n- Replace projects with (phase/stage/chapter/problems/questions)\n- brief detail of the proposed content of all chapters. \n- description of the theoretical and conceptual framework that underlies the thesis, and procedures to be used in addressing the research questions\n- Project 1: the stuff detailled in PhD writings on blog. Basically a literature review of the current state of uncertainty visualisation\n- Project 2: Maybe an experiment/ application of ideas tests (AEMO application maybe)\n- [@North2006] Discusses how the purpose of visualisation is insight but visualisation evaluations fail to measure that because they use benchmark tests as a substitute, time limits that prevent deep insight etc. Author disagrees that small insights are stepping stones to true insight. Suggests open ended questions.\n- [@Hullman2019] present a taxonomy for characterizing decisions made in designing an evaluation of an uncertainty visualization. Found that existing evaluation practice, particularly in visualization research, focuses on Performance and Satisfaction-based measures that assume more predictable and statistically-driven judgment behavior than is suggested by research on human judgment and decision making. We reflect on common themes in evaluation practice concerning the interpretation and semantics of uncertainty, the use of confidence reporting, and a bias toward evaluating performance as accuracy rather than de- cision quality. We conclude with a concrete set of recommendations for evaluators designed to reduce the mismatch between the conceptualization\n- two part experiment? pt 1 is a visualisation evaluation and the second part is discussing the visualisations with the people who use them.\n- Project 3: Technical outputs (translation of research) - possibly a package\n\n# Project 1 Details\n1) Believe we should be able to get information that is not mapped to any aesthetic of the plot (One plot should provide all information about a distribution of collection of distributions)\n2) Believe we should be able to get information about distributions not depicted in the plot (Unique questions do not unique distributions and I should be able to use one distribution to answer questions about another distribution)\n\n## The current state perspective on visualising uncertainty\nCurrent research in visualising uncertainty seems to have a focus on designing new options for visualising uncertainty for specific types of data. Typical papers comparing visualisation methods seem to fit into one or both of the follow categories:\n\n1) a paper that suggests a new visualisation method for a specific type of data  \n2) a paper that compares two existing methods to idenfiy which is better.  \n\nGenerally the goal seems to be to increase the number of options we have in our \"visualisation bag\" so that we always have a plot at the ready if we want to visualise uncertainty. These new plots also tend to focus on a catch all plot that is \"best for making decision\" or \"best for visualising spatial uncertainty\". Data visualisation is commonly utilised as a tool in data exploration, so it is not uncommon for a data analyst to make a plot with only a vague goal and use it to pull out a large number of adjacent observations. For example, a data analyst might make a scatter plot to find out the relationship between two variables, but in doing do also finds out that one variable has discrete outcomes over a range of values. It is common knowledge that any single plot cannot reveal all information about a data set, all plot types obfuscate and uncover different types of information, however this aspect of plotting becomes more apparent in uncertainty visualisation. Often when discussing \"uncertainty\" information, we expect readers to be able to draw information from a plot that was not estimated, prioritised, or visualised. Frequently visualisation authors hide information through both the distribution they chose to present and the visualisation they chose to use. These three issues run deep in the literature, but it is easiest to explain with an example that I will frequently return back to as I go into detail on these issues.\n\n- [@Webster2003] An editorial comment on communicating climate change uncertainty. They say it is in two steps 1) quantify uncertainty and 2) communicate it. They want to focus on 1) and discuss the ways people can better estimate uncertainty.\n\n## Example: The HOPS, Error Bar, and Violin Plot Comparison\nThe issues that are rampent in the uncertainty visualisation literature are easily seen if we zoom in on one example. The study done by [@Hullman2015] is a great illustration in the importance of understanding your goal when visualising uncertainty. It is important to keep in mind that while I am discussing only one paper (for the time being) to illustrate a point, the issues I am discussing are the standard in uncertainty visualisation literature, this paper is not an outlier.\n\nThe study by [@Hullman2015] asked participants to provide some numerical properties of a distribution using a hypothetical outcome plot (HOPs), an error bar plot or violin plot. Participants were given questions relating to individual and multiple distributions. When shown a single distribution participants were asked about the mean of the distribution, the probability of an outcome being above some threshold (indicated on the plot with a red dot), or the probability of an outcome being between two given values. When shown two distributions the participants were asked \"How often is measurement of solute B larger than the measurement of solute A?\", and when shown three distributions \"How often is measurement of solute B larger than the measurement of solute A and solute C?\". @fig-examples shows an example error bar plot and violin plot for the two distribution case. There was also a high and low variance case for every plot and question combination. The study decided which technique was better using absolute error between the subjects response and the true value. They specified that the error bars are not confidence intervals as they represent the true underlying distribution, not the sampling distribution of the mean. They found that the HOPs reliably outperformed the violin and error bar plots for all the two and three distribution questions, but performed worse when estimating the mean when the variance was high, and when estimating threshold probability when the variance was low. The HOPs were no better than the violin or error bar plot in the other univariate cases. The authors suggest that the issue with estimating the mean comes from participants trying to integrate over large distances and a higher frame rate in the HOPs plot could fix it. \n\n\n::: {#fig-examples .cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![Error bar plot](confirmationreport_files/figure-pdf/fig-examples-1.pdf){#fig-examples-1}\n:::\n\n::: {.cell-output-display}\n![Violin plot](confirmationreport_files/figure-pdf/fig-examples-2.pdf){#fig-examples-2}\n:::\n\nAn example of the plots shown in the two variable tasks given in [@Hullman2015]. The example question provided with this plot was 'In what percentage of vials is there more of solute B than A (Probability(B > A)?'\n:::\n\n\n## Issue 1: If you don't estimate it, you arent going to plot it\nMy first issue with the plots in Hullman's study is that they are visualsing different distributions. The error bar plot and the violin plot in @fig-examples provides information about the independent the marginal distributions of solutions A and B and provides no information on the joint distribution. The HOPs plot is similar to a slope graph except instead of connecting the outcomes from A and B with a line, they are connected through frames of an animation. The HOPs, just like a slope graph, depict a relationship between two varibles and this a *joint* distribution instead of two marginal distributions. This key feature can be used to improve upon the HOPs and direct our attention to the distribution that is even more useful for answering this question.\n\nTwo alternative plots are provided in @fig-alternatives. The scatter plot depicts the joint distribution of the two solutions and uses colour to highlight the details of a related Bernoulli distribution. The stacked bar chart is the final evolution of this process as it sets the distribution of the binary outcome $P(B>A)$ as the primary distribution of interest and entirely drops both the marginal and joint distribution. Through this process we moved the information needed to answer the question \"What is $P(B>A)$?\" from something you needed to calculate in your head (when looking at the error bar plots) to something you can *see* in the bar chart. \n- [@Joslyn2012] It is important to avoid whittling the problem down too much though. Providing categorical decision advice alone did not improve decisions, however, combining decision advice with uncertainty estimates resulted in the best performance overall.\n- [@Wickham2011] paper discusses how different partitions of a product plot results in different marginal and conditional distributions being depicted as the primary focus. \n\n\n::: {#fig-alternatives .cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![Scatter Plot](confirmationreport_files/figure-pdf/fig-alternatives-1.pdf){#fig-alternatives-1}\n:::\n\n::: {.cell-output-display}\n![Stacked Bar Chart](confirmationreport_files/figure-pdf/fig-alternatives-2.pdf){#fig-alternatives-2}\n:::\n\nTwo plots that could be used as alternatives to answer to question 'In what percentage of vials is there more of solute B than A (Probability(B > A)?'. The scatter plot in (a) focuses on the relationship between the concentration of solute A and solute B, while the stacked bar chart in (b) highlights the frequency with which each solute is greater than the other.\n:::\n\n\nThe distinction between the distribution required to answer our question and the marginal distribution of our variables is often ignored in our discussions good or bad uncertainty visualisatios. @fig-distdraw illustrates how we typically go about thinking of distributions in order to answer a statistics question. When we compute statistics or perform a hypothesis test, we typically put a lot of thought into what the correct distribution is for our question, however, when we perform visualisation we typically plot a collection of normal marginal distributions, ignoring the actual distribution required. The distribution that tells us whether or not two variables are significantly different from each other is different to the distribution of each variable. The confidence intervals associated with each point prediction are not interchangeable with the confidence interval of the paths of a 5 step ahead path forecast. The binary distribution depicting which of two outcomes is bigger is different from the marginal distribution of each of those outcomes. The examples of how much this distribution swap heuristic exists in visualisation (and not really in any other area of statistics) is baffling.\n\n![Illustration depicting the difference between the distributions we use for testing vs the visualisation we visualise](incorrectdistributions.jpeg){#fig-distdraw}\n\nThis is idea is adjacent to something I woud like to call \"the error bar problem\". The problem is that without considering the appropriate distribution, this method often leads us astray. As a matter of fact, I can't think of a single unit in my undergraduate degree that didn't make this mistake at least once.  \n- For example let's say we want to identify all the estimates in our linear regression are significantly different from 0. If we wanted to answer this question with statistics, we would typically perform an F-test using an F-distribution. If we wanted to visualise this question, we would create an error bar plot that depicts each estimates t-distribution and use those as proxy information to answer whatever ANOVA test we want. Do you see the problem?\n- If you ignore this clear distinction, you are going to misuse uncertainty techniques and beleive they are interchangable for every hypothesis, and thus draw incorrect conclusions. If you have ever used an error bar for a hypothesis that is not idenfiying a statistically significant range of values (using it to identify if two variables are statistically different from each other is also incorrect) then you are guilty of the exact issue I am talking about.\n- [@Bella2005] They get a bunch of people to adjust error bars until the two means are \"just\" statistically significantly different. Most people think this is when they are not overlapping but that actually gives a p value much less than 0.05. People also don't know the difference between confidence intervals and SE bars.\n- [@Correll2014] This paper investigates drawbacks of bar charts with error bars, and considers a set of alternatives designed to more effectively communicate the implications of mean and error data to a general audience. They select gradient and violin plots because of issues with bar charts which mean the error encoding needs to be visually symmetric and continuous which I found interesting. \n- [W. S. Cleveland and R. McGill] There are tasks where asymmetric encodings outperform symmetric encodings; for in- stance, comparing ratios can be done quickly and more accurately with bar charts as compared to dot plots or other encodings where area under the bar is more difficult to estimate [5].\n\nThis view of understanding uncertainty visualizations opens up a new gap in the literature. For example, if we accept that visualizing a collection of t-distributions as a substitute for a collection of F-tests is incorrect, how *should* we visualise it? \n\n*Part 2 Taxonomy*\n- [@Gustafson2019] They test the effects of four distinct uncertainty frame types: lack of knowledge (deficient), modelling approximations and measurement error (technical), Unknown unknowns (scientific), disagreement among parties (consensus). We find portraying scientific findings using uncertainty frames usually does not have significant effects, with an occasional exception being small negative effects of consensus uncertainty.\n- [@Kiureghian2009] They prove that propper attention should be paid to the cateogy of the risk and failure to do so may result in underestimation or overestimation of the failure probability. Proved with math?\n- [@Walker2003] Adopt a general definition of uncertainty as \"any deviation from the unachievable ideal of completely deterministic knowledge of the relevant system\". Discuss three dimensions of uncertainty: location (where the uncertainty comes from), level (spectrum from deterministic to total ignorance) and nature of uncertainty (due to imperfect knowledge or inherent variability). Suggesting uncertainty is a three dim In the current situation, different analysts use different terms for the same kinds of uncertainty, and some use the same term to refer to different kinds. This makes it extremely difficult for those who have not participated in the actual work to understand what has been done. New information can either decrease or increase uncertainty. The typology is very easy to understand and well-written. I like it very much. Provide an uncertainty matrix which is a graphical overview of the essential features of uncertainty. The goal of decision-making in the face of uncertainty should be to reduce the undesired impacts form surprises, rather than eliminating them.\n- [@Padilla2021] Participants had to do resource allocation with some fake budget for a forecast of nighttime temp.The first found that people use both uncertainty from quantile dotplots and forecaster confidence. When forecasters were less sure, people made more conservative judements. Second study found people became more conservative when forecaster confidence was low or model uncertainty increased. Researchers suggest both sources of uncertainty should be presented.\n\n## Issue 2: Section on distribution aspects (What aspect of the distribution are important)\n- [@Buja2009] [@Wickham2010] suggests the lineup protocol to test if what we see in a plot is actually there. Esentailly makes an uncertainty visualisation of plots themselves using hypothetical outcomes.\n- [@Chowdhury] shows use of infoviz to show distribution of high dimensional hard to conceptualise situation of plots of low dimensional projections of data.\n- [@Hofmann2012] Infoviz is directly comparable to statistical tests and the ways we use graphical elements can have an impact on the power of the test. This paper tries to estimate the power of a statistical test performed using infoviz. It compares polar and cartesian co-ordinates and finds that polar co-ordinates are a higher powered test. It also compares multiple ways of showing distribution to see which has more power.\n- [Grewal2021] looks categorising uncertainty visualisations based on at how current visualisation methods compare based on x=discreetness of uncertainty and y=how field specific the audience is.\n\n- Even the concept of some plots being used for \"distributions\" and plots for \"relationships\" is a little silly. In this example the violin plot and the scatter plot both showed that each solution had a marginal normal distribution, but while the error bar (although technically a plot for visualising distributions) gives no concept of mass leaves you unable to identify the existence of even a common distribution. \n- Additionally, you might argue that the scatter plots didn't visualise the \"distribution\" but rather, hypothetical outcomes of it.\n\n- Frequency based questions are easily answered with a HOPs plot OR ANY PLOT OF HYPOTHETICAL OUTCOMES. \n\n- High dimensional visualisation techniques also can be viewed through this frameoworok and is the reason I find the tour to be such a valuable tool in visualisation. The tour using because it tries to give a view of the overarching joint distribution. A parallel co-ordinate plot and a SPLOM both provide a look at all of the bivariate joint distributions, or a subset of them.\n\n(Plots can give impressions of different things)\n- specific values (mean, 95% CI intervals)\n- Order\n- Outcomes (frequency)\n- Mass\n- Connectedness\n- parameters (location, other variables, etc)\n\n### Issue 3: If you dont plot it, I won't see it (Hierarchy)\n\nThere are only so many aesthetics we can map information to, and those aesthetics have a hierarchy of how easy they are the process. Additional secondary information can simultaneously give important context and clutter the visualisation, so it is important to understand our motivations when creating a visualisation. \n\nThe error bar plot and the violin plot in @fig-examples puts implies we should be unconcerned with the relationship between the solutions and especially unconcerned with specific questions about it (such as \"What is $P(B>A)$?\"). The scatter plot in @fig-alternatives highlights that the relationship between the solutions is of importance and allows us to see think about the two variables as they occur together. This plot still maintain some information about the marginal distributions illustrated in the violin and error bar plots (you can tell that we seem to have to have two independent normal distributions) however it is no longer of primary importance, the plot is now telling us that the general relationship between the solutions is the most important feature.\n\nWhat should become apparent comparing these plots is that there is a constant trade off between information relevant to the our question and secondary information that provides context. \n\n\nThe primary issue I have with this study (and by extension most visualisation studies) is the philosophy driving the motivation. An error bar plot is good at displaying a range of statistically significant values. A violin plot is good at depicting the frequency of outcomes of a distribution. Both can be used to get a general idea of the variance or central value of a distribution, but the significant values, and the frequency of individual values is not all the information we might need from a distribution. While an animated HOPs plot was better at depicting probability, or frequency of an outcome, than an error bar plot, that does not mean it is the \"best\" at answering the question \"In what percentage of vials is there more of solute B than A (Probability(B > A)?\". @fig-alternatives depicts several plots that I actually think would do a better job than the HOPs plot used in the experiment.\n\n\n*HIERARCHY DISCUSSION HERE* Lets take a look at the accuracy rank of an error bar.\n- placebo rule, studys compare to placebo is bad, should compare to next best alternative (should compare plots that have the feature of interest at a similar importance ranking)\n- paper Wanted to get a better understanding of peoples ability to assess if A>B, rather than asking a binary question about significance.\n\n![Explicit highlighting of what information is contained in the error bar plot and high important the plot communicates that information to be](feature.jpeg)\n\n*ERROR BAR HIERARCHY EXAMPLE HERE* This is why the conversation and research around error bars is very frustrating to read. \n- I think this issue is largely why error bar plots get a lot of flack as being a bad plot, I think they are just being misused. \n - In the paper, Hullman does acknowledge that error bars are typically used to identify significance, but\n- people couldn't answer that question with ANY DISTRIBUTION PLOT\n- Ask yourself what questions can be answered with each aesthetic of the plot. Is this the hierarchy of information you want to express.\n- mapping the most important feature to the highest aspect\n- So after zooming in on the uses (and misuses) of error bar plots, it should be clear to see why error bar plots *are* valid visualisations of significance\n- First of all, the study ignores the questions that would be ideal for an error bar or violin plot.\n- they assume that people first make some probability estimate (they find P(A>B)) and then use that to assess significance. \n- Statistical significance is related to sampling distributions which they didn't care about\n- \"If there is special interest in knowing whether people make qualitative errors , we can set various qualitative errors such as 0.95 during the analysis phase and count the frequency of errors rather than their average magnitude\" or just make an error bar plot.\n- They argue that a normal distribution was the most favorable condition for the error bar and violin because it leads to a symmetric plot where the widest point is also the mean (so you acknowledge that violin plots are better at expressing the mode of a distribution???)\n- There is also some evidence in their results that a question so removed from the purpose of the plot just confuses people. \"In light of the common usage of error bars for presenting multiple independent distributions, it is noteworthy how poorly subjects using these representations performed on tasks asking them to assess how reliably a draw from one variable was larger than the other\", Many subjects reported values less than 50%, which are not plausible given that the mean of B was larger than the mean of A.\" Did not find any obvious patterns of errors. \"We speculate that many subjects simply had no idea how to make a good guess\"\n\n\n*INCORPERATING CONTEXT HERE*\n- With this example in mind, we now understand both the distribution we visualise and the method we use should be directly related to the uncertainty we want to ask questions about. These are the key points we need to keep in mind every time we visualise uncertainty, however even the things that this paper got right (i.e. in order to answer questions about an uncertainty distribution we need to show the uncertainty distribution) get lost when researchers try to reintroduce the context of the data. Papers discussing new ways to visualise \"temporal uncertainty\" or \"spatial uncertainty\" seem to forget the uncertainty is related to a distribution, not a type of data. If I ask you to explain to me what \"temporal uncertainty\" is, you may try and describe uncertainty related to a forecast, or variance of a variable, or even model residuals. These are not \"temporal uncertainty\", they are the distribution of a prediction, a variable, or of a model. None of these things are specific to temporal data. This may just sound like me being pedantic about poor naming, but this incorrect framing of uncertainty leads to very inconsistent research. Because we view uncertainty as a secondary feature of the data, and not as something we have constructed, we don't think about uncertainty visualisations with the \"goal of the uncertainty\" in mind.\n- The people who don't plot uncertainty because they think its unimportant and the people who relegate uncertainty to the lowest ranking aesthetic on the list are both saying the same thing, \"I think the uncertainty is unimportant\". Including uncertainty is worth very little if its impossible to interpret and the last thing I notice in the plot. \n- You can chose to include the temporal aspect of the plot in some capacity, however it is not by accident that there is a general struggle to visualise uncertainty when there are so many competing ideas. This changes the way we view problems such as visualising spatial uncertainty. The question of how to constuct uncertainty plots for complicated data types (such as spatial data) is no longer driven by desiging an abstract uncertainty visualisation method for spatial data. The goal would be to find the best way to combine the most appropriate uncertainty visualisation for the relevant question and the best plot for the type of data. These methods may be able to be combined onto a single plot, however there may be cases where two different plots (one for the data and one for the uncertainty) is more appropriate and effective. \n- There is an unspoken assumption prevailing in uncertainty visualisation that the uncertainty distribution associated with one hypothesis can just be swapped out for another. An assumption that the literature itself seems to disprove since there are many papers that are \"contradictory\" if you view them through the lense of finding a best plot, but strangely consistent when you reframe the conclusions through the lense of the \"best plot for this hypothesis\". Unfortunately this evidence is almost always used to highlight a failing in the plot, rather than a failing in the question.\n- people being able to identify uncertainty is not what is important because 1) all these perform worse than two side by side plots, 2)\n- check what kind of question they can answer\n\n*SPATIAL UNCERTAINTY EXAMPLE*\n- [@Wickham2012] Glyph maps are a good way to visualize spatial temporal data (read: not uncertainty). The ordering is determined by space, and the glyph is a line plot. Different scales allow us to look at trends in global variance, local variance. Aggregation allows investigations of trends.\n- [@Correll2018] They make value-suppressing uncertainty palettes. Suggest using it to endocde the uncertainty in the colour apect of the graph (should be directly integrated in a shared chart). Found that Juxtaposed maps, by introducing a second search task to the identification task (searching for the proper values in two, rather than one map), would have poorer performance than other conditions. (read the paper to find out details)\n- [@Lucchesi2017] This paper is discusses different ways to present uncertainty in a map. A lot of them made it easy to SEE that there was uncertainty, but it was still difficult to compare across locations and tell if areas are statistically the same (which I would argue is a large part of uncertainty). Rotation is pretty useless, the pixelation was interesting but had above issue, the grid colour wasnt a bad idea, but I wouldnt use a second colour in the scale (if i did use diverging palet, it would be on the statistic).\n\n## (Other examples) Tying the reasons together\nThis is a large reason why the current literature on uncertainty distributions (specifically with respect to error bar plots) is so misguided. The outcome of many visualisation papers can be reconsided when you think about the underlying distributions selected and the hierarchy of information provided.\n- I argue that the problem is not that people are incorrectly reading error bars, but that error bars are being incorrectly used.\n\nTwo themes in these papers\n- Relevant features should be of primary importance\n- Irrelevant features distract and lead to bias\n\n*Collection of Visualisaion papers*\n- [@Nathonours] finds that a scatter plot is better than a line plot if you want to convey the correlation between two time series. Considering the correlation is a feature of the joint distribution, it makes sense that a depiction of the joint distribution (a scatter plot) outperformed between the two time series, and a line plot is the marginal distribution of each time ser (its also like a flipped slope graph?)\n- [@Kobakian]hexagon tile map was significantly more effective for spotting a real population related data trend model hidden in a lineup.\n- [@Kale2021] Including aesthetics that highlight an irrelevant aspect of the plot will confuse people . Adding means to uncertainty visualizations has small biasing effects on both magnitude estimation and decision-making, consistent with users relying on visual distance between distributions as a proxy for effect size visualization designs that support the least biased effect size estimation do not support the best decision-making, suggesting that a chart user’s sense of effect size may not necessarily be identical when they use the same information for different tasks.\n- [@Gschwandtnei2016] Could have to do with making sure you match your question with an intuitive plot feature. The paper looks at different types of temporal uncertainty and discusses the differences and how to best visualise each one. Has some interesting methods to visualise intervals. Gradients were best for probability and ambiguation was best for start and end time estimation\n- [@Fernandes2018] This paper has several visualisation specific findings. We found that cumulative distribution function (CDF) plots and low-density quantile dotplots pro- duce more accurate and consistent decisions compared to other uncertainty visualizations, textual displays, and displays with no uncertainty. The types of displays that we have found to be the best for supporting decision-making in the transit have also been shown to be more accurate at estimating probability intervals more generally. Alternatively their question was best answered with the CDF (what is the probability I will miss the bus if I leave now). Reflective with real costs and benefits which are not always quantifiable, they have to weigh those up on their own.\n- [@Padilla2022] visualizing the same data in different ways had differential effects on the changes we observed in risk perceptions. Cumulative y-axis will most reliably lead to people perceiving greater pandemic risk because cumulative y-axis can show only an upward or flat trend. In contrast, incident y-axis can be highly variable and even reduce viewers’ perceived pandemic risk, if the charts exhibit a downward trend. Basically aspects on the plot other than the the distribution and hierarchy also have an effect on the way the plot is percieved (and what information is highlighted)\n- [@Vanderplas2015] plotting features can work for you or against you. The sine illusion causes bars on a sine curve to seem longer at the peaks and troughs (because people view the perpendicular area). This means increases in variance in a time series plot are harder to identify if they are paired with an increase in slope. The illustion is very pervasive and the you have to go to extreme measures to remove its effect.\n- [@Maceachren2012] The first paper organises different types of uncertainty and then matches each type of uncertatiny to a visual aspect of a graph. Experiment 1 tested the extent to which symbols have intuitive applications. Participants response times were the same across symbols, etc. Found fuzziness, location, (and sightly less value) were the best for visualising uncertainty. Arrangement, size and transpacency slightly less so. Saturation, hue, orientation and shape are unacceptable. Only one direction was deemed intuitive by participants (fuzziness: more fuzzy=less certain; location: further from center=less certain; value: lighter=less certain; arrangement: poorer arrangement=less certain; size: smaller=less certain; transparency: more obscured=less certain). While iconic sign-vehicles can be more intuitive and more accurately judged when aggregated (than are abstract sign-vehicles), the abstract sign-vehicles can lead to quicker judgments.  Experiment 2 assessed task performance when multiple symbols appeared.\n\n### Sorting the questions and experiments\n- Which of these questions should the \"best\" uncertainty visualisation allow me to answer?\n- My solution to this problem is to design uncertainty visualisation rules that are irrelevant to the data, but instead focus on the question being asked. I would review the research in uncertainty visualisation and reframe the conclusions with the goal of finding the best uncertainty method for each question, rather than for the type of data. I would then define rules for uncertainty visualisation so that authors know the correct way to express uncertainty for their purpose. Using this framework, the concept of designing a method for visualising temporal uncertainty becomes pointless.\n- There is a large amount of acknowledgement that different plots hide and uncover different sources of information, so structuring experiments that allow us to have a better understanding of how these work would be more fruitful.\n- Assumption that answering small questions are intermediate steps in answering big questions.\n- Identifying which plots can answer a large number of questions and which questions they can answer is the experiement I suggest in project 2.\n- there is a habit of comparing plots to NOTHING (error bar significance test) or to an alternative that is NOT \"next best\" (same level on the hierarchy)\n- the way we perform studies on data visualisation is antithetical to the way we use data visualisations.\n- I fundamentally disagree that we should be asking what plot is the best at depicting a distribution, and instead ask which questions different plots are better at identifying. These papers often ask one question (or a handful of similar questions) and compare the response on two or three plots. What would be more effective would be asking a large range of questions, with a large range of underlying distributions, and trying to match up the plots to their ideal question or case scenario. Finding a \"best\" plot for all case scenarios is a fruitless activity. \n\n- Example questions\nSignificance questions\n- statistical significance of one value\n- significant difference between variables\n- Joint significance \nProbability Questions\n- Which outcome is more probable \nEstimate questions\n- What is the mean, mode and median of this distribution\nOrdinal questions\n- What is the maximum/minimum outcome you would expect from this distribution\n- Between which values would 50% of the data appear\nVibe Questions\n- Describe the distribution of the distribution \n- Do the variables have identifiable distributions\n- Does the variable have a discrete or continuous distribution\n- Are there any impossible values in the distribution\n- Does this distribution have more than one mode?\n\n*Final Paragraph*\nWhy do we need a plot that can \"see\" everything? Plots are similar to people in the sense that expecting one person to have a complete and flawless view of something as complicated as society ignores that one of the things that has allowed humanity to flourish is that there are so many of us. It allows each person to specialise and be good at something important to them rather than trying to be mediocre at everything. Similarly, no plot can see every possible thing you might want to know about your data. Leaning into and uncovering the strengths of each plot is the best way to research them because it reflects the ways in which we actually incorperate plotting into our diagram. While general principals are helpful, fundamentally each plot for each question should be as unique as the person who made it.\n\n## Section notes\n- (not sure if the taxonomy should go here)\n- What would be ideal is a general application of methods to visualise uncertainty that does not require complete knowledge of all plots that exist. This framework would guide authors towards effective visualisations and allow them to build up knowledge of underlying statistical ideas.\n- rethinking uncertainty visualisation in terms of the motivation of the plot (the research question) instead of \"best\" uncertainty visualisation.\n- Show that we can find a best distribution visualisation and reintroduce context to get a good uncertainty plot for that question and context.\n- Classification of current plots and research questions that have been shown to be effective for those types of plots\n- Reintroduce context\n\n# Timeline\n- A timetable for completing the thesis and a statement of progress to date\n- ASC later in year (submitted an abstract for a poster)\n- IEEE VIS (June 22 2023 poster abstract) - Conference October 2023 \n- Aim for something international in a year (or in last year)\n- Also could do UseR! & talk about some of the software developed (technical output)\n- Could have an energy specific one (various applied energy)\n- IEEE VIS (march 31st paper due in 2025) - Conference October 2025 (Vienna)\n- Thesis due December 2025\n\n# Bibliography\n\n\n",
    "supporting": [
      "confirmationreport_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}