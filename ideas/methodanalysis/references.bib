@article{Gschwandtnei2016,
  author = {Gschwandtnei, Theresia and B{\"{o}}gl, Markus and Federico, Paolo and Miksch, Silvia},
  doi = {10.1109/TVCG.2015.2467752},
  issn = {10772626},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  keywords = {Bars,Data visualization,Encoding,Image color analysis,Uncertainty,Visualization},
  month = {jan},
  number = {1},
  pages = {539--548},
  pmid = {26529717},
  publisher = {IEEE Computer Society},
  title = {{Visual Encodings of Temporal Uncertainty: A Comparative User Study}},
  volume = {22},
  year = {2016}
}

@article{Hullman2015a,
abstract = {Many visual depictions of probability distributions, such as error bars, are difficult for users to accurately interpret. We present and study an alternative representation, Hypothetical Outcome Plots (HOPs), that animates a finite set of individual draws. In contrast to the statistical background required to interpret many static representations of distributions, HOPs require relatively little background knowledge to interpret. Instead, HOPs enables viewers to infer properties of the distribution using mental processes like counting and integration. We conducted an experiment comparing HOPs to error bars and violin plots. With HOPs, people made much more accurate judgments about plots of two and three quantities. Accuracy was similar with all three representations for most questions about distributions of a single quantity.},
annote = {Supplementary material for the other paper of the same name.},
author = {Hullman, Jessica and Resnick, Paul and Adar, Eytan},
doi = {10.1371/JOURNAL.PONE.0142444},
file = {:Users/hmas0003/Library/Application Support/Mendeley Desktop/Downloaded/Hullman, Resnick, Adar - 2015 - Hypothetical Outcome Plots Outperform Error Bars and Violin Plots for Inferences about Reliability of Va.pdf:pdf;:Users/hmas0003/Library/Application Support/Mendeley Desktop/Downloaded/Hullman, Resnick, Adar - 2015 - Hypothetical Outcome Plots Outperform Error Bars and Violin Plots for Inferences about Reliability of(2).pdf:pdf},
issn = {1932-6203},
journal = {PLOS ONE},
keywords = {Normal distribution,Probability density,Probability distribution,Random variables,Sea water,Statistical distributions,Statistical inference,Vision},
mendeley-groups = {4. Experiments on Plots},
month = {nov},
number = {11},
pages = {e0142444},
pmid = {26571487},
publisher = {Public Library of Science},
title = {{Hypothetical Outcome Plots Outperform Error Bars and Violin Plots for Inferences about Reliability of Variable Ordering}},
url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0142444},
volume = {10},
year = {2015}
}
@article{Kale,
abstract = {Fig. 1. We present two experiments (E1 and E2) evaluating four different uncertainty visualizations (from left to right): bar graphs with error bars, bar hypothetical outcome plots (HOPs), static line ensembles, and line HOPs. Abstract-Animated representations of outcomes drawn from distributions (hypothetical outcome plots, or HOPs) are used in the media and other public venues to communicate uncertainty. HOPs greatly improve multivariate probability estimation over conventional static uncertainty visualizations and leverage the ability of the visual system to quickly, accurately, and automatically process the summary statistical properties of ensembles. However, it is unclear how well HOPs support applied tasks resembling real world judgments posed in uncertainty communication. We identify and motivate an appropriate task to investigate realistic judgments of uncertainty in the public domain through a qualitative analysis of uncertainty visualizations in the news. We contribute two crowdsourced experiments comparing the effectiveness of HOPs, error bars, and line ensembles for supporting perceptual decision-making from visualized uncertainty. Participants infer which of two possible underlying trends is more likely to have produced a sample of time series data by referencing uncertainty visualizations which depict the two trends with variability due to sampling error. By modeling each participant's accuracy as a function of the level of evidence presented over many repeated judgments, we find that observers are able to correctly infer the underlying trend in samples conveying a lower level of evidence when using HOPs rather than static aggregate uncertainty visualizations as a decision aid. Modeling approaches like ours contribute theoretically grounded and richly descriptive accounts of user perceptions to visualization evaluation.},
annote = {Second Pass? Yes

Summary: They compare an uncertainty animation that shows one possible outcome for each frame with static error bar plots. They find that participants who view the animated frames (HOPs) can identify the correct underlying distribution with lower sample sizes. Relate questions to real applications ("is employment decreasing") which is nice.},
author = {Kale, Alex and Nguyen, Francis and Kay, Matthew and Hullman, Jessica},
file = {:Users/hmas0003/Library/Application Support/Mendeley Desktop/Downloaded/Kale et al. - Unknown - Hypothetical Outcome Plots Help Untrained Observers Judge Trends in Ambiguous Data.pdf:pdf},
keywords = {Index Terms-uncertainty visualization,hypothetical outcome plots,psychometric functions},
mendeley-groups = {4. Experiments on Plots},
title = {{Hypothetical Outcome Plots Help Untrained Observers Judge Trends in Ambiguous Data}},
url = {https://www.nytimes.com/interactive/2017/11/28/upshot/what-the-tax-}
}
@article{Potter2009a,
abstract = {Scientists increasingly use ensemble data sets to explore relationships present in dynamic systems. Ensemble data sets combine spatio-temporal simulation results generated using multiple numerical models, sampled input conditions and perturbed parameters. While ensemble data sets are a powerful tool for mitigating uncertainty, they pose significant visualization and analysis challenges due to their complexity. In this article, we present Ensemble-Vis, a framework consisting of a collection of overview and statistical displays linked through a high level of interactivity. Ensemble-Vis allows scientists to gain key scientific insight into the distribution of simulation results as well as the uncertainty associated with the scientific data. In contrast to methods that present large amounts of diverse information in a single display, we argue that combining multiple linked displays yields a clearer presentation of the data and facilitates a greater level of visual data analysis. We demonstrate our framework using driving problems from climate modeling and meteorology and discuss generalizations to other fields. {\textcopyright} 2009 IEEE.},
annote = {Summary: Sometimes your data has a time aspect and a spatial aspect. They introduce Ensemble-Vis, which is a software that consists of a bunch of linked interactive displays so you can view your data across time/space. Show that the linked display presents a clearer level data analysis.},
author = {Potter, Kristin and Wilson, Andrew and Bremer, Peer Timo and Williams, Dean and Doutriaux, Charles and Pascucci, Valerio and Johnson, Chris R.},
doi = {10.1109/ICDMW.2009.55},
file = {:Users/hmas0003/Library/Application Support/Mendeley Desktop/Downloaded/Potter et al. - 2009 - Ensemble-vis A framework for the statistical visualization of ensemble data.pdf:pdf},
isbn = {9780769539027},
journal = {ICDM Workshops 2009 - IEEE International Conference on Data Mining},
keywords = {Coordinated and linked views,Ensemble data,Statistical graphics,Uncertainty},
mendeley-groups = {4. Experiments on Plots},
pages = {233--240},
title = {{Ensemble-vis: A framework for the statistical visualization of ensemble data}},
year = {2009}
}
@techreport{Kobakian,
abstract = {The choropleth map display is commonly used for communicating spatial distributions across geographic areas. However, when choropleths are used the size of the geographic units will influence the understanding of the distribution derived by map users. The hexagon tile map is presented as an alternative display for visualizing population related distributions effectively. Visual inference is used to measure the power of the hexagon tile map design, and the choropleth is used as a comparison. The hexagon tile map display is tested using a distribution that is directly related to the geography, with values monotonically increasing from the NorthWest to SouthEast areas of Australia. This study finds in a hexagon tile map lineup the single map that contains a population related distribution is detected with greater probability than the same data displayed in a choropleth map. These findings should encourage map creators to implement alternative displays and consider a hexagon tile map when presenting spatial distributions of heterogeneous areas.},
annote = {Second Pass? Maybe

Summary: Choropleth maps are bad at showing distributions when a country has a lot of large rual areas with little populations, and areas with highly dense populations. A hexagonal map is presented as an alternatvie and people can usually pick up on patterns easier with this display. People are more confidence with a choropleth map, probably because it is more farmiliar.},
author = {Kobakian, Stephanie and Cook, Dianne},
file = {:Users/hmas0003/Library/Application Support/Mendeley Desktop/Downloaded/Kobakian, Cook - Unknown - Comparing the Effectiveness of the Choropleth Map with a Hexagon Tile Map for Communicating Cancer Statistics.pdf:pdf},
keywords = {Index Terms-statistics,geospatial,popula-tion,visual inference},
mendeley-groups = {4. Experiments on Plots},
title = {{Comparing the Effectiveness of the Choropleth Map with a Hexagon Tile Map for Communicating Cancer Statistics}}
}
@article{Wickham2012,
abstract = {The multivariate spatio-temporal nature of climate data makes it difficult to draw all of the aspects simultaneously. This paper describes the conceptualization and construction of a type of display, a glyph-map, that can show these multiple aspects. Glyph-maps are a specialization of multivariate glyph plots. Each spatial location is displayed with one glyph that represents the multiple measurements, often recorded over time, at that location. Glyph-maps allow the discovery of both local and global structure, with a particular focus on temporal relationships, important for studying climate change. They provide alternatives to colored, facetted maps, or statistical summaries such as principal components. Glyph-maps have been used sporadically for spatio-temporal data, and with the ideas and software described in this paper, it will be easier to produce them. The conceptualization described here also will enable developing interactive versions of glyph-maps, and make it simpler to explore the perceptual effects of different scales. The methods are developed for rectangular gridded data, but with some clever processing, which is explained, it is possible to get good glyph-maps of irregularly gridded data. Guides and reference marks, for different types of glyphs used in the glyph-maps, are also discussed. {\textcopyright} 2012 JohnWiley & Sons, Ltd.},
annote = {Second Pass? Maybe

Summary: Glyph maps are a good way to visualize spatial temporal data. The ordering is determined by space, and the glyph is a line plot. Different scales allow us to look at trends in global variance, local variance. Aggregation allows investigations of trends.},
author = {Wickham, Hadley and Hofmann, Heike and Wickham, Charlotte and Cook, Dianne},
doi = {10.1002/env.2152},
file = {:Users/hmas0003/Library/Application Support/Mendeley Desktop/Downloaded/Wickham et al. - 2012 - Glyph-maps for visually exploring temporal patterns in climate data and models.pdf:pdf},
issn = {11804009},
journal = {Environmetrics},
keywords = {Data mining,Exploratory data analysis,Spatiotemporal data,Statistical graphics,Visualization},
mendeley-groups = {4. Experiments on Plots},
month = {aug},
number = {5},
pages = {382--393},
title = {{Glyph-maps for visually exploring temporal patterns in climate data and models}},
volume = {23},
year = {2012}
}
@article{Padilla2022,
abstract = {People worldwide use SARS-CoV-2 (COVID-19) visualizations to make life and death decisions about pandemic risks. Understanding how these visualizations influence risk perceptions to improve pandemic communication is crucial. To examine how COVID-19 visualizations influence risk perception, we conducted two experiments online in October and December of 2020 (N = 2549) where we presented participants with 34 visualization techniques (available at the time of publication on the CDC's website) of the same COVID-19 mortality data. We found that visualizing data using a cumulative scale consistently led to participants believing that they and others were at more risk than before viewing the visualizations. In contrast, visualizing the same data with a weekly incident scale led to variable changes in risk perceptions. Further, uncertainty forecast visualizations also affected risk perceptions, with visualizations showing six or more models increasing risk estimates more than the others tested. Differences between COVID-19 visualizations of the same data produce different risk perceptions, fundamentally changing viewers' interpretation of information.},
annote = {Second Pass? Maybe

Summary: wanted to assess how well covid 19 forecast visualisations convey risk. Mostly were converned with if it increased concept of risk or not, did not assess if people were more ACCURATE in their judgements. As with many people they condemed the confidence interval and supported the ensemble vis method.},
author = {Padilla, Lace and Hosseinpour, Helia and Fygenson, Racquel and Howell, Jennifer and Chunara, Rumi and Bertini, Enrico},
doi = {10.1038/s41598-022-05353-1},
file = {:Users/hmas0003/Library/Application Support/Mendeley Desktop/Downloaded/Padilla et al. - 2022 - Impact of COVID-19 forecast visualizations on pandemic risk perceptions.pdf:pdf},
isbn = {0123456789},
issn = {2045-2322},
journal = {Scientific Reports 2022 12:1},
keywords = {Computer science,Human behaviour,Psychology,Scientific data},
mendeley-groups = {4. Experiments on Plots},
month = {feb},
number = {1},
pages = {1--14},
pmid = {35132079},
publisher = {Nature Publishing Group},
title = {{Impact of COVID-19 forecast visualizations on pandemic risk perceptions}},
url = {https://www.nature.com/articles/s41598-022-05353-1},
volume = {12},
year = {2022}
}
@article{Hullman2019,
abstract = {Understanding and accounting for uncertainty is critical to effectively reasoning about visualized data. However, evaluating the impact of an uncertainty visualization is complex due to the difficulties that people have interpreting uncertainty and the challenge of defining correct behavior with uncertainty information. Currently, evaluators of uncertainty visualization must rely on general purpose visualization evaluation frameworks which can be ill-equipped to provide guidance with the unique difficulties of assessing judgments under uncertainty. To help evaluators navigate these complexities, we present a taxonomy for characterizing decisions made in designing an evaluation of an uncertainty visualization. Our taxonomy differentiates six levels of decisions that comprise an uncertainty visualization evaluation: the behavioral targets of the study, expected effects from an uncertainty visualization, evaluation goals, measures, elicitation techniques, and analysis approaches. Applying our taxonomy to 86 user studies of uncertainty visualizations, we find that existing evaluation practice, particularly in visualization research, focuses on Performance and Satisfaction-based measures that assume more predictable and statistically-driven judgment behavior than is suggested by research on human judgment and decision making. We reflect on common themes in evaluation practice concerning the interpretation and semantics of uncertainty, the use of confidence reporting, and a bias toward evaluating performance as accuracy rather than decision quality. We conclude with a concrete set of recommendations for evaluators designed to reduce the mismatch between the conceptualization of uncertainty in visualization versus other fields.},
annote = {Second Pass? Yes

Summary: They read 86 papers that did an experiments assessing some uncertainty visualisation and characterised the general way it is done with a taxonomy. They also offer reccomendations on how to design a study that assesses visualisation quality. There is a lot of interesting points here about researchers getting data for a study before really unpacking sources of uncertainty and implictions of some responses or methods of collecting these responses.},
author = {Hullman, Jessica and Qiao, Xiaoli and Correll, Michael and Kale, Alex and Kay, Matthew},
doi = {10.1109/TVCG.2018.2864889},
file = {:Users/hmas0003/Library/Application Support/Mendeley Desktop/Downloaded/Hullman et al. - 2019 - In Pursuit of Error A Survey of Uncertainty Visualization Evaluation.pdf:pdf},
issn = {19410506},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Uncertainty visualization,probability distribution,subjective confidence,user study},
mendeley-groups = {4. Experiments on Plots},
month = {jan},
number = {1},
pages = {903--913},
publisher = {IEEE Computer Society},
title = {{In Pursuit of Error: A Survey of Uncertainty Visualization Evaluation}},
volume = {25},
year = {2019}
}
@article{Hullman2015,
abstract = {Many visual depictions of probability distributions, such as error bars, are difficult for users to accurately interpret. We present and study an alternative representation, Hypothetical Outcome Plots (HOPs), that animates a finite set of individual draws. In contrast to the statistical background required to interpret many static representations of distributions, HOPs require relatively little background knowledge to interpret. Instead, HOPs enables viewers to infer properties of the distribution using mental processes like counting and integration. We conducted an experiment comparing HOPs to error bars and violin plots. With HOPs, people made much more accurate judgments about plots of two and three quantities. Accuracy was similar with all three representations for most questions about distributions of a single quantity.},
annote = {Second Pass? Maybe

Summary: This is the paper where I go OFF about error bar plots and this papers use of them. Their point seems to be that HOPs are better than error bar and violin plots for comparing relative oucomes, which they probably are because ERROR BARS AND VIOLIN PLOTS ARE NOT HOW I WOULD TRY TO CONVEY RELATIVE OUTCOMES. I feel like some of the concusions drawn from this paper are willfully ignorant to their issues in methodology},
author = {Hullman, Jessica and Resnick, Paul and Adar, Eytan},
doi = {10.1371/journal.pone.0142444},
file = {:Users/hmas0003/Library/Application Support/Mendeley Desktop/Downloaded/Hullman, Resnick, Adar - 2015 - Hypothetical outcome plots outperform error bars and violin plots for inferences about reliability of(3).pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
mendeley-groups = {4. Experiments on Plots},
month = {nov},
number = {11},
pmid = {26571487},
publisher = {Public Library of Science},
title = {{Hypothetical outcome plots outperform error bars and violin plots for inferences about reliability of variable ordering}},
volume = {10},
year = {2015}
}
@article{Kale2021,
abstract = {Uncertainty visualizations often emphasize point estimates to support magnitude estimates or decisions through visual comparison. However, when design choices emphasize means, users may overlook uncertainty information and misinterpret visual distance as a proxy for effect size. We present findings from a mixed design experiment on Mechanical Turk which tests eight uncertainty visualization designs: 95% containment intervals, hypothetical outcome plots, densities, and quantile dotplots, each with and without means added. We find that adding means to uncertainty visualizations has small biasing effects on both magnitude estimation and decision-making, consistent with discounting uncertainty. We also see that visualization designs that support the least biased effect size estimation do not support the best decision-making, suggesting that a chart user's sense of effect size may not necessarily be identical when they use the same information for different tasks. In a qualitative analysis of users' strategy descriptions, we find that many users switch strategies and do not employ an optimal strategy when one exists. Uncertainty visualizations which are optimally designed in theory may not be the most effective in practice because of the ways that users satisfice with heuristics, suggesting opportunities to better understand visualization effectiveness by modeling sets of potential strategies.},
annote = {Second Pass? Yes

Summary: Compare effectiveness of confidence intervals, HOPS plots, PDFs and quantile dot plots with and without means. The paper discusses the way including the mean can bias the way people interpret a plot (in some cases for the better and other for the worse). People interpret plots with heuristics which you can have work for or against you. Also everyone may not interpret a plot in the same way so taking the average or majority is not always effective, some people may interpret a plot in two different ways depending on the question they are being asked to answer. Ranking visualisation methods without the context of a question is fundamentally arbitrary.},
archivePrefix = {arXiv},
arxivId = {2007.14516},
author = {Kale, Alex and Kay, Matthew and Hullman, Jessica},
doi = {10.1109/TVCG.2020.3030335},
eprint = {2007.14516},
file = {:Users/hmas0003/Downloads/Visual_Reasoning_Strategies_for_Effect_Size_Judgments_and_Decisions.pdf:pdf},
issn = {19410506},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Uncertainty visualization,data cognition,graphical perception},
mendeley-groups = {4. Experiments on Plots},
number = {2},
pages = {272--282},
pmid = {33048681},
title = {{Visual reasoning strategies for effect size judgments and decisions}},
volume = {27},
year = {2021}
}
@article{Correll2014,
abstract = {When making an inference or comparison with uncertain, noisy, or incomplete data, measurement error and confidence intervals can be as important for judgment as the actual mean values of different groups. These often misunderstood statistical quantities are frequently represented by bar charts with error bars. This paper investigates drawbacks with this standard encoding, and considers a set of alternatives designed to more effectively communicate the implications of mean and error data to a general audience, drawing from lessons learned from the use of visual statistics in the information visualization community. We present a series of crowd-sourced experiments that confirm that the encoding of mean and error significantly changes how viewers make decisions about uncertain data. Careful consideration of design tradeoffs in the visual presentation of data results in human reasoning that is more consistently aligned with statistical inferences. We suggest the use of gradient plots (which use transparency to encode uncertainty) and violin plots (which use width) as better alternatives for inferential tasks than bar charts with error bars.},
annote = {Second Pass? Yes

Summary: They did an experiment to check alternatives to error bars including gradient and violin plots. The details of the experiment are in the body of the paper, but they select gradient and violin plots because of issues with bar charts which mean the error encoding needs to be visually symmetric and continuous which I found interesting.},
author = {Correll, Michael and Gleicher, Michael},
doi = {10.1109/TVCG.2014.2346298},
file = {:Users/hmas0003/Downloads/Error_Bars_Considered_Harmful_Exploring_Alternate_Encodings_for_Mean_and_Error.pdf:pdf},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Visual statistics,crowd-sourcing,empirical evaluation,information visualization},
mendeley-groups = {4. Experiments on Plots},
number = {12},
pages = {2142--2151},
pmid = {26356928},
publisher = {IEEE},
title = {{Error bars considered harmful: Exploring alternate encodings for mean and error}},
volume = {20},
year = {2014}
}
@article{Hoekstra2014,
abstract = {Null hypothesis significance testing (NHST) is undoubtedly the most common inferential technique used to justify claims in the social sciences. However, even staunch defenders of NHST agree that its outcomes are often misinterpreted. Confidence intervals (CIs) have frequently been proposed as a more useful alternative to NHST, and their use is strongly encouraged in the APA Manual. Nevertheless, little is known about how researchers interpret CIs. In this study, 120 researchers and 442 students—all in the field of psychology—were asked to assess the truth value of six particular statements involving different interpretations of a CI. Although all six statements were false, both researchers and students endorsed, on average, more than three statements, indicating a gross misunderstanding of CIs. Self-declared experience with statistics was not related to researchers' performance, and, even more surprisingly, researchers hardly outperformed the students, even though the students had not received any education on statistical inference whatsoever. Our findings suggest that many researchers do not know the correct interpretation of a CI. The misunderstandings surrounding p-values and CIs are particularly unfortunate because they constitute the main tools by which psychologists draw conclusions from data.},
annote = {Second Pass? Maybe

Summary: this paper is about how researchers and students misinterpret CI. There is some good information about people being bad at interpreting confidence intervals and hypothesis tests, but I am not sure what they consider to be a "misinterpretation". Some misunderstandings are bigger than others and they are pretty stern on interpreting the CI as a frequentist. I.e. "We can be 95% confident that the true mean lies between 0.1 and 0.4" is incorrect, and only "if we were to repeat the experiment over and over again, then 95% of the time the confidence interval contains the true mean". There are different costs in different mistakes, something that is not captured by this paper.},
author = {Hoekstra, Rink and Morey, Richard D. and Rouder, Jeffrey N. and Wagenmakers, Eric Jan},
doi = {10.3758/s13423-013-0572-3},
file = {:Users/hmas0003/Downloads/s13423-013-0572-3.pdf:pdf},
issn = {15315320},
journal = {Psychonomic Bulletin and Review},
keywords = {Confidence intervals,Inference,Significance testing},
mendeley-groups = {4. Experiments on Plots},
number = {5},
pages = {1157--1164},
pmid = {24420726},
title = {{Robust misinterpretation of confidence intervals}},
volume = {21},
year = {2014}
}
@article{Bella2005,
abstract = {Little is known about researchers' understanding of confidence intervals (CIs) and standard error (SE) bars. Authors of journal articles in psychology, behavioral neuroscience, and medicine were invited to visit a Web site where they adjusted a figure until they judged 2 means, with error bars, to be just statistically significantly different (p < .05). Results from 473 respondents suggest that many leading researchers have severe misconceptions about how error bars relate to statistical significance, do not adequately distinguish CIs and SE bars, and do not appreciate the importance of whether the 2 means are independent or come from a repeated measures design. Better guidelines for researchers and less ambiguous graphical conventions are needed before the advantages of CIs for research communication can be realized. Copyright 2005 by the American Psychological Association.},
annote = {Second Pass? Yes

Summary: Pretty interesting paper. They get a bunch of people to adjust error bars until the two means are "just" statistically significantly different. Most people think this is when they are not overlapping but that actually gives a p value much less than 0.05. People also dont know the difference between confidence intervals and SE bars.},
author = {Bella, Sarah and Fidler, Fiona and Williams, Jennifer and Cumming, Geoff},
doi = {10.1037/1082-989X.10.4.389},
file = {:Users/hmas0003/Downloads/Researchers Misunderstand Confidence Intervals and Standard Error Bars.pdf:pdf},
issn = {1082989X},
journal = {Psychological Methods},
keywords = {Confidence intervals,Error bars,Standard error,Statistical cognition,Statistical reform},
mendeley-groups = {4. Experiments on Plots},
number = {4},
pages = {389--396},
pmid = {16392994},
title = {{Researchers misunderstand confidence intervals and standard error bars}},
volume = {10},
year = {2005}
}
@article{Correll2018,
abstract = {Understanding uncertainty is critical for many analytical tasks. One common approach is to encode data values and uncertainty values independently, using two visual variables. These resulting bivariate maps can be difficult to interpret, and interference between visual channels can reduce the discriminability of marks. To address this issue, we contribute Value-Suppressing Uncertainty Palettes (VSUPs). VSUPs allocate larger ranges of a visual channel to data when uncertainty is low, and smaller ranges when uncertainty is high. This non-uniform budgeting of the visual channels makes more economical use of the limited visual encoding space when uncertainty is low, and encourages more cautious decisionmaking when uncertainty is high. We demonstrate several examples of VSUPs, and present a crowdsourced evaluation showing that, compared to traditional bivariate maps, VSUPs encourage people to more heavily weight uncertainty information in decision-making tasks.},
annote = {Summary: They make value-suppressing uncertainty palettes. Suggest using it to endocde the uncertainty in the colour apect of the graph (should be directly integrated in a shared shart). Basically dark colours are certain and loght colours are uncertain. Did a crowdsourced experiment that showed people heavily weighed uncertainty information in decision-making tasks. People made more errors when value and uncertainty were not well correlated and there were no reliable visual landmarks in common amongst the value and uncertainty charts. Continuous maps performed poorly, there is a trade off in quantization error and perceptual error. Avoid continuous bivariate color scales when possible, and instead employ discrete scales with a relatively small number of categories.},
author = {Correll, Michael and Moritz, Dominik and Heer, Jeffrey},
doi = {10.1145/3173574.3174216},
file = {:Users/hmas0003/Downloads/3173574.3174216.pdf:pdf},
isbn = {9781450356206},
journal = {Conference on Human Factors in Computing Systems - Proceedings},
keywords = {Color perception,Semiotics,Thematic maps,Uncertainty visualization},
mendeley-groups = {4. Experiments on Plots},
pages = {1--11},
title = {{Value-suppressing uncertainty Palettes}},
volume = {2018-April},
year = {2018}
}
@article{Fernandes2018,
abstract = {Everyday predictive systems typically present point predic-tions, making it hard for people to account for uncertainty when making decisions. Evaluations of uncertainty displays for transit prediction have assessed people's ability to extract probabilities, butnotthe qualityoftheir decisions.Ina con-trolled, incentivized experiment, we had subjects decide when to catchabus using displays withtextual uncertainty, uncer-tainty visualizations, or no-uncertainty (control). Frequency-based visualizations previously shown to allow people to bet-ter extract probabilities (quantile dotplots) yielded better deci-sions. Decisions with quantile dotplots with 50 outcomes were (1) better on average, having expected payoffs 97% of optimal (95% CI: [95%,98%]), 5 percentage points more than con-trol (95% CI: [2,8]); and (2) more consistent, having within-subject standard deviation of 3 percentage points (95% CI: [2,4]),4percentage points less than control (95% CI: [2,6]). Cumulative distribution function plots performed nearly as well, and both outperformed textual uncertainty, which was sensitive to the probability interval communicated. We dis-cuss implications for realtime transit predictions and possible generalization to other domains.},
annote = {Second Pass? Yes

Summary: Similar to the bus uncertainty paper except it is a controlled environment and a clinical test. They want to see if people actually make better decisions with the uncertainty visualisation. This paper has SEVERAL interesting findings. 1) dot plots slay again, 2) the better understanding of probabilities translates to better descisions, 3) people get better at making decisions where they incorperate the uncertainty over time.},
author = {Fernandes, Michael and Walls, Logan and Munson, Sean and Hullman, Jessica and Kay, Matthew},
doi = {10.1145/3173574.3173718},
file = {:Users/hmas0003/Downloads/3173574.3173718.pdf:pdf},
isbn = {9781450356206},
journal = {Conference on Human Factors in Computing Systems - Proceedings},
keywords = {Cumulative distribution plots,Dotplots,Mobile interfaces,Transit predictions,Uncertainty visualization},
mendeley-groups = {4. Experiments on Plots},
pages = {1--12},
title = {{Uncertainty displays using quantile dotplots or CDFs improve transit decision-making}},
volume = {2018-April},
year = {2018}
}
@article{Kay2016,
abstract = {Users often rely on realtime predictions in everyday contexts like riding the bus, but may not grasp that such predictions are subject to uncertainty. Existing uncertainty visualizations may not align with user needs or how they naturally reason about probability. We present a novel mobile interface design and visualization of uncertainty for transit predictions on mobile phones based on discrete outcomes. To develop it, we identified domain specific design requirements for visualizing uncertainty in transit prediction through: 1) a literature review, 2) a large survey of users of a popular realtime transit application, and 3) an iterative design process. We present several candidate visualizations of uncertainty for realtime transit predictions in a mobile context, and we propose a novel discrete representation of continuous outcomes designed for small screens, quantile dotplots. In a controlled experiment we find that quantile dotplots reduce the variance of probabilistic estimates by ∼1.15 times compared to density plots and facilitate more confident estimation by end-users in the context of realtime transit prediction scenarios.},
annote = {Second Pass? Yes

Summary: This is a study investigating how to communicate uncertainty quickly on a small screen (mobile) to the general public. The specific case scenario is communicating bus arrival times.},
author = {Kay, Matthew and Kola, Tara and Hullman, Jessica R. and Munson, Sean A.},
doi = {10.1145/2858036.2858558},
file = {:Users/hmas0003/Downloads/2858036.2858558.pdf:pdf},
isbn = {9781450333627},
journal = {Conference on Human Factors in Computing Systems - Proceedings},
keywords = {Dotplots,End-user visualization,Mobile interfaces,Transit predictions,Uncertainty visualization},
mendeley-groups = {4. Experiments on Plots},
pages = {5092--5103},
title = {{When (ish) is my bus? User-centered visualizations of uncertainty in everyday, mobile predictive systems}},
year = {2016}
}
