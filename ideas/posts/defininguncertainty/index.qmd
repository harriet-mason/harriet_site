---
title: "Defining Uncertainty"
author: "Harriet Mason"
editor: visual
---

## Why this is its own paper
I have thought more about the idea of defining uncertainty mathematically and I actually *DO* think it is possible. I want the record to show the reason I *did not* do this earlier is that *nobody* in the uncertainty literature has defined uncertainty mathematically, and I thought I was supposed to highlight problems in the literature review, not solve them. I have had an idea of how to formalise uncertainty after seeing Xiao-li Meng's work, but an uncertainty formalisation would solve *several* open questions he has postulated in previous papers, and would be so new it would need to be a paper in of itself, not as a sub-section in a literature review. A mathematical definition would be *completely* new and *very complicated*, and I think its difficulty is pretty clear by the fact that *nobody* and *I really mean nobody*, not spiegelhatter or wilkinson or mceachren or any statistician working in this space, has provided an overarching mathematical definition. This is work that would take a long time (and it is the reason I started reading philosophy books because I was actually trying to do this) and even if I did do it, it *should not* be in this paper. I think it might be possible based on the reading I have been doing over the past year to try and formalise this work, *but it is not a quick thing that can just be added to this paper*. The reason I wanted to do a summary of the papers using the larger taxonomy of uncertainty was because it is encompasing of all the taxonomies, and it allows me to show  that there are *no* formal rules to the uncertainty that is used in uncertainty communication and visualisation. Here I am trying to make the point that the lack of a formal definition or of mathematical considerations *has created problems in the literature*. To clearly and mathematically define uncertainty will *be solving this problem*. Something I was told *not to do* because this is a literature review. If a mathematical definition that we are all happy with *needs* to be in this paper, then there is a reasonable chance this paper will never be finished.


#### Maths comments I might relate back to an actual mathematical definition
::: aside
 In the "semantics" section of the book, Leeland Wilkinson provides a series of definitions of words related to uncertainty and mathematical definitions of examples of uncertainty, but those are not mathematical definitions of uncertainty. Every *mathematical* definition of uncertainty is a definition of *probability* OR a definition of an *example* of uncertainty, such as a confidence interval or PDF. I included this here, however, I actually now believe there actually is a mathematical definition, but it would not fit in this paper. If this paper remains a *here are the discussions of uncertainty and there is no mathematical definition* I might leave this here as an explanation as to why, but not suggest that there *isnt* a mathematical definition.
:::

#### An explanation of the building blocks for my mathematical definition
::: aside
I wrote this section between me deciding it would be possible to mathematically define uncertainty and me deciding it would be too large for this paper. I am aware that this is not a formal definition and I cannot say this is the unifying feature of uncertainty without some proof. But, this is something that *can be* defined in mathematical terms and *can* lead to an encompassing definition of uncertainty (i.e. it would be something that the groups of confidence intervals, error bars, and missing data using an information filtration) but that is beyond the scope of this paper. This way of thinking about uncertainty has resolved a lot of my issues with the field and a lot of other issues that have been raised by authors in their summary of the literature. This is very similar to Xiao-li Meng's work that asserts that bias and variance are actually the same thing, with a couple of changes.
::: 

The first problem in the conception and attempts to offer a definition of uncertainty, is that bias *and* variance are both technically uncertainty.
 
Basically the idea is that every analysis has some goal, and every choice we make that cuts out several alternatives is a "restriction" that trades the variance of multiple options into a potential bias. This occurs at *every stage* of the data analysis pipeline, from collecting data to communication. Bias comes from "incorrect information" being passed along the pipeline, while variance comes from "holding on to information". The randomness that is "irriduciable error" (if it exists or not is not relevant to our definition) cannot be differentiated from the bias and variance (I say both bias and variance because at the end of the pipeline it is impossible to know if error is being caused by bias or variance) unless we literally know the function that generated the data. We typically talk about bias and variance in terms of the model, but it exists at every stage of the pipeline. Since every step of the analysis pipeline has an almost infinite set of choices (there is an infinite set of things we can observe, an infinite number of choices that can be made in data cleaning, etc) each of these levels is functionally its *own* random variable. When you make your choice you completely shrink the variance of that level to 0 (it basically goes from a distribution of choices to a single outcome of that distribution) and, the amount this *guess* is wrong by, is bias. 

This idea is very similar to the bias-variance trade off, but it is slightly different because we are assuming there is not a trade-off perse, but that bias is just "cashed in" variance. If selections at each stage are made using feedback, we can reduce the bias without increasing the variance, but it "costs" some data. Assumptions let you reduce the variance with a low data "cost" but there is a higher chance of being incorrect (since there is less validation) and introducing bias. Flexible models like neural networks can reduce the variance but it comes at a high data "cost". If you have a *lot* of data it is better to avoid as many assumptions as possible because increased certainty that comes from more data can only effect error that is still in the form of variance (i.e. we are considering alternatives). This is well established in modelling, it is why big tech companies try to collect as much data as they can and then put it in a neural network and hope they have enough data and enough feedback to avoid the bias at *all* stages of the analysis. Recording *everything* and putting it in the most flexible modle possible means we don't even have to put any poles down, even at the data collection stage

Validation methods (such as cross validation, assumption checks, etc) help to guide the choices we make in the data analysis pipeline, but they can never offer any kind of certainty or truth considering validation methods themselves are also only estimates. Once a variance has been cashed in for bias, its error cannot be reduced by more data. 

When we consider the "uncertainty" of a model we usually only consider the variance of the final stage, the prediction error for *a specific model and a specific set of features*, and ignore the variance that has been converted into bias by our selections that was introduced at earlier stages of the pipeline. Since uncertainty is basically "how true is this estimate or statement", if we had not cashed in our variance for a single outcome of bias, it would appear as variance. The climate scenario uncertainty plots are an example of bias that was converted back to variance for the sake of the visualisation (time series forecasts typically assume away the bias by stating "we are assuming previous trends will continue into the future"). Prediction error is a useful metric because it allows us to see if the bias introduced in earlier stages expressed as variance through an error, however it still cannot account for bias introduced at the sampling stage as that bias also exists in the test set. 

Prediction error is not always feasible or useful in a statistical analysis (such as when we produce a visualisation) so it is unclear exactly *how* we should reintroduce that bias as variance to communicate the pair as uncertainty. It is not uncommon for uncertainty to only be considered at the final stage of an analysis as a confidence interval or estimate range, but this is typically misleading as earlier bias has not been converted back to variance. Factors that typically decrease the variance, such as sample size, can only have an effect on something that is still in a state of variance. This means that an established model that is build on more and more data will have confidence intervals that are incredibly small but if it is build on some bias (such as a biased sample) the confidence intervals will become misleading and will be too small to *actually* predict future data. It is not uncommon for true results or models produced by a different teams to sit entirely outside of the confidence band of another. By hiding the selections we made at earlier stages, when we traded that variance in for bias, we functionally made *point predictions* (they were based on educated guess but point predictions none the less) but *these* point predictions are often invisible to us. 

#### A Forest analogy
Let us consider the data analysis pipeline a dense forest that you need to cross to get to your house. To prevent yourself from getting *too* lost you decide to bring some poles and rope so you can mark where you are going. You know your rope is long enough to get you to your house with enough wiggle room that you can reach the 5 houses either side of your house. This problem is drawn in @fig-forest1. @fig-forest2 depicts the typical data analysis pipeline and resulting uncertainty using this forest analogy. In this method, we navigate the forest by putting one of our poles down every time we notice the trees have changed (i.e. we have move to a new section of the forest). As we move down the line and we put our poles down, we cannot be certain we are always walking in the direction of our house, but we can do small checks or validation to make sure we are vaguely heading in the right direction. When we get to the final stage, we have enough rope remaining to get to the three houses directly in front of us, and we state that band as our "uncertainty". @fig-forest3 depicts the bias variance trade off in the case where we make no decisions, record as much information as possible, and check every single model with every single variable and keep every alternative as a possible option. This method will result in no bias (although it is often impossible to check the entirety of these spaces so it cannot) but a huge amount of variance.  In an effort to avoid putting down any poles, we have managed to navigate the forest, but have essentially no idea which of the 10 houses are ours. Of course, with sufficient feedback we can whittle down this vast array of choices (which is what a neural network can do) but feedback from an increased sample size can only influence variance, it cannot influence bias that was previously variance. @fig-forest4 shows an alternative way to perform the data analysis process to consider variance at stages beyond model uncertainty (such as the scenario uncertainty that comes with different human choices in climate change models). In this scenario you navigate the forest to a certain point, and then clone yourself three times and each put down a new pole at different distances to each other. All four versions of yourself will then walk in an identical fashion from the pole to the end of the forest, and each have your own path with your own confidence interval. This is how the [climate scenario uncertainty](https://climatechange.chicago.gov/sites/production/files/2016-07/scenariotempgraph_0.jpg) is constructed. 


::: {#fig-forest layout-ncol=2}

![Data analysis Goal](images/analogy/fig1.jpg){#fig-forest1}

![The typical modelling process](images/analogy/fig2.jpg){#fig-forest2}

![Making zero assumption at every stage](images/analogy/fig3.jpg){#fig-forest3}

![An example of "scenario uncertainty" with a confidence interval](images/analogy/fig4.jpg){#fig-forest4}

An analogy of walking through the forest to help understand how our conceptualization of uncertainty and bias can influence what we consider to be uncertainty and what we consider to be bias. 
:::

#### The influence of the "goal" of an analysis
Prediction error is also useful because the *goal* of our analysis is clearly defined in a metric. This is not always the case. When you attempt to quantify or visualise it you will quickly find yourself asking, "uncertainty about... what?". Do you mean uncertainty on an estimate? On a forecast? How many steps ahead is this forecast? Are we only considering the uncertainty in the estimate or in the parameters or are we considering the possibility of measurement error or biased inputs? These questions start to highlight another invisible problem in uncertainty definitions, signal and noise can only be untangled in the presence of a motivating question. 

The idea that uncertainty can only be defined in the presence of a motivating question is well grounded in most areas of statistics. The entire process of data analysis, from deciding what should be observed as data through to communicating that data in a plot is governed by human decision and the goal of an analysis. At the philosophical level, applied statistics is simply taking real world entities and boiling them down into probabilistic objects, an ontological process that is largely dependent on our goals [@Otsuka2023]. When we move onto data provenance the issue persists, as what is kept as data and what is tossed away is determined by the motivation of an analysis and what was previously noise can be shown to become signal depending on the resolution of the question [@Meng2014]. After moving onto modelling this issue continues as each research question can be can be categorised as descriptive, predictive, or causal, each of which has its own appropriate statistical methods and motivation agnostic model selection leads to signal devoid of meaning [@Carlin2023]. Even at the final stages of visualisation a lack of understanding of the motivating question make it difficult to untangle what is signal and what is noise, leaving many uncertainty visualisation studies with conflicting results [@Kinkeldey2014]. These cases highlight that uncertainty is defined at *every* stage in relation to our motivating question. Discussions of uncertainty cannot be had if we are not clear *what* we are uncertain about. Once it is established what we are uncertainty about, we can consider the other elements of uncertainty that need to be defined.

This concept is a little harder to define mathematically, but we *can* consider that every analysis has some metric that is the target "goal" of the analysis. Refusing to define this target goal from the begining does not mean you avoid this problem, it just increases the bias and variance issues described earlier, because there is no way to check assumptions or correct the variance with data. This is actually why I was thinking about the concept of *suffient visual statistics*. If a graphic has a piece of information mapped to it (which we should know because we made the graphic ourselves) then that information can be extracted with a percecptual task and the graph has a *sufficient visual statistic*. I also think the idea of sufficient statistics is useful because it establishes that we can only know if we have extracted all the relevant information from our *if a metric we are collecting data for has been defined*. If a piece of information cannot be extracted with a perceptual task, it can only be found with assumptions and we introduce a bias at the communication stage. If the information does exist but it reuires a heuristic or some collection of perceptual tasks that might be harder for some people than others, we have maintained variance at the communication stage. A complicated visualisation with a *huge* amount of information can technically be used to answer a lot of questions, but the mental overload would create variance. This is why it is important to have your *goal* in mind when designing a visualisation and not assume it can or should be able to do everything. I suspect we can work out retroactively which visualisation contain which visual statistics, by systematically working our way through the grammar of graphics and then information that is contained in simple perceptual tasks. 

Even if this cannot be used to give a mathematical definition of uncertainty, it is still soemthing that *should* be considered. This might be obvious when it is read, but the conversation about whether or not we should consider what was previously variance to be variance when we communicate our analysis, or if avoiding defining a *goal* metric can have detrimental effects on our communication are things that are rarely discussed although it is important. 

::: aside
Di said this has already been shown and I believe it, but then I don't understand why the uncertainty visualisation literature seems to ignore it. This is what I was trying to show with the plot and question section of the excel spreadsheet. I was trying to highlight that when you ask participants to answer a question, to mark them as correct or incorrect you need to define a metric, and if the plot was created using a pipeline that *ignores* that metric, it will be passing on incorrect information and lead to mistakes when it is being read. Expecting any plot to be able to answer any question that has the basis of some random metric simply because the metric is related to uncertainty and so is the plot seems ridiculous to me, but its harder to find a paper that *doesn't* do this than a paper that *does*.
:::

## The starting point of Xiao-li Meng's multi-resolution, source and phase concepts
I think this is a really good way of conceptualising uncertainy because it breaks the concept up into its components. Resolution is untangling signal and source inside each layer, source is combining signal (and noise) from two different sets, and phase is transferring signal and noise along the pipeline. 

![](images/multiframework.jpg)

### Multi-resolution
```{r}
#| output: asis
#| echo: false
img_files <- fs::dir_ls("images/multiresolution", glob="*.jpg")
cat("::: {layout-ncol=2}\n",
    glue::glue("![]({img_files})\n\n\n"),
    ":::",
    sep = ""
)
```

## Random notes that may or may not be relevant
- I am not sure if an assumption should be considered a layer *lock* or an influx of artificial data (or both because *they* are actually the same thing)
- Uncertainty only exists in the face of inference, so descriptive statistics has no uncertainty. Since it's goal is to describe things *as they are*, they cannot be wrong since they are a statement of fact. The average of the sample *is* the mean and that is that.
- In this same vein, I don't think unsupervised learning or exploratory visualisation (which under this framework are the same thing) have uncertainty either. Their goal is to bring forward as much information as possible and I'm not sure how you can have uncertainty if there is no goal (or nothing to be uncertain about)
- Then I feel like prediction and estimation are both *easy* to define their uncertainty. I have no idea how causal inference fits in though. I feel like they are still in *estimation* but instead of estimating something in the future that is unseen, they are trying to estimate a specific direction of relationship.
- can this entire framework be conceptualised in terms of data (n) and variables (p)? i.e. can we force model assumptions and alternative models and everything to be in terms of just data and variables. Then we only need to define uncertainty in terms of data and variables. 
- The relationship between n and p is already established with things like regularisation.
- Mention computation time? Checking with data is computationally expensive.
- Need to detail that only the previous layers information passes through and is relevant.
- Can only get feedback from data at the final level of the pipeline.
- The estimates of uncertainty *itself* can have bias and variance. Should look into how that interacts with the pipeline.
- The pipeline should be considered as stages as what can be done with our data:
(1) observe data
(2) operations across variables (combining data etc) to make *sample with variables we actually want*
(3) operations on individual variables (square, log, etc) *i feel like this should be included in the modelling though*
(4) operations that combine variables into a single output (i.e. the model that is used to generate a response)
- It feels like (1) and (2) are "is this sample observations from the population I want" and (3) and (4) are like "am I combining these variables to get the target value I want".
- trying to work out how to spend your data so that you can minimise bias and variance reminds me of economomics optimisation problems.
- also thinking about self regulation with enough data. i.e. you can have a splines model but if your data is linear with enough data it will become the same model. The variable selection and the model selection can both be considered using the resolution framework, but I also feel like the model is not really a filtration but rather a transformation.
- bias-variance trade off only works if you are assuming your
- to have variance you need to vary *something*. Like you need new samples or different models or soemthing. Without something that varies, you dont have variance.
- if your model is biased it is stable with respect to resampling, BUT new predictions jump around it. If your model has variance it is unstable with respect to new samples, but then how does it react to new models?
- n-CV, LOOCV, test/training, etc all have thier own bias and variance because it is taking the goal estimate *up* a level.
- Descriptive does no inference, exploratoy/unspervised has a functional form and wants to *find* predictors and responses, prediction/estimation wants to perform inference to find a true unknown value (prediction because it hasn't happened and estimation because we cannot know. Although we are pretty good at predicting celestial bodies because we can see the entire length which I think is interesting).
- Variance and uncertainty are not synonymous. Uncertainty can be described with variance, but variance is a metric that is estimated with certainty
- I wonder if the way machine learning boundaries are fractals is related to this concept or can be understood with this concept. Like there is randomness
- Something ahout a linear model being contained within a neural network because a neural network can *be* a linear model if it is true with enough data. Therefore we can view the linear model as a *subset* of a neural network in a way. I wonder if we can do this for every type of model.
- Is a sampling distribution signal supression or new information? I think it depends on the signal. Like if you are talking about outcomes from a population, you should 
- In order to visualise something, it needs to be able to be expressed mathematically, so uncertainty *at every level of* needs to be expressed mathematically or have a way to be expressed mathematically. I also need to show how it can be calculated. We need to try and do this for all levels from sampling to feature engineering to modelling to communication.
- There are a lot of distinctions in data quality and stuff because it is meta-information, but meta information *is* data because it needs to be considered in the signal supression.
- Think of Di's LDA paper. The first paper did an exploratory analysis and found a signal which they mentioned in the paper. THE next paper had a comment about how that signal was not significant and tested it using the randomised data and regenerating the 
- OK I have rethought the classes of statistics and I think they are:
  1) descriptive/exploratory/unsupervised: no uncertainty
  2) estimation/causal: Trying to estimate an unobservable latent value
  3) prediction/forecasting: Trying to predict a non-existent value that is yet be observed (i.e. even if your model is perfectly correct it could still be wrong) - celestial bodies are forecasting but its super correct.
- There are measures for uncertainty within levels (sometimes), but not across levels. Like there are probably metrics for the uncertainty caused by data *at that level*. Before it is converted to something else. I wonder if they follow this guideline. Actually I dont think those are uncertainty metrics, I think those are just like variance, without inference there is nothing to be uncertain about.
- You cant optimise more than one metric

### Titles
- Defining Uncertainty
- Turn what was Qualitative to something Quantitative: Transforming Uncertainty with a Mathematical Definition
- Uncertainty through the data pipeline
