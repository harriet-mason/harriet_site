---
title: "Ignore the Data Type"
author: "Harriet Mason"
date: "2023-04-01T00:00:00Z"
image: feature.jpeg

editor: 
  markdown: 
    wrap: 72
knitr:
  opts_chunk: 
    warning: FALSE
    message: FALSE
    echo: FALSE
---
<!--
### Untangling Infinite Threads
#### A story about you who works in academia
Hello you. You are a budding academic researcher, starting on your journey to do important work and send it out into the public. How exciting. Unfortunately, these are a few steps between you doing your important work that you have been thinking about and that work being seen as a change in society. Lets look at a run through of how this process works:

1) You do research that is useful for the general public and they should know about it.
2) You spend years trying to get this research published in an academic journal. Most of this time is spent not improving the research but rather making small tweaks to match each journals formatting, length and content rules. and hope the entire time that nobody else does this exact work and publishes it before you (which has happened to people I know).
3) after years of effort and tweaking (that is usually irrelevant to the actual research) you may finally get your work published.
4) At this point in the process, your paper has been read by the authors (probably) and the peer reviewers of your paper, lets say about 5 people in total. This is very likely the only people who will read your paper in full.
5) Your paper is then tossed into the research abyss. I like to imagine it as a big room with a huge pile of papers falling out of a tube in the middle. Papers with no identifiable features are flung randomly on the floor and only people who also belong to some large academic institution can access this room.
6) People who are researching the topic you have written your paper on walk into this metaphorical room with a blindfold on and grab fist fulls of papers. They are much more likely to grab your paper if it has a well known author, institution, or journal attached to it, or you somehow got stuck to a paper that has one/all of those things. These people never access your paper through the journal it was published in, only through this blindfold shoving into a bag way, so the years you spent formatting your paper to look like other papers in the journal turned out to be unnoticed by the readers and pointless for you.
7) Of the people who walked into the room and grabbed your paper in a blind collection, some of them will read the title, abstract and maybe some other details, even less will give a skim read of the whole document. Almost none will read the entire thing. 
8) Of those that skim read your research, some will 
9) Occasionally a paper or two will esape this room and a journalist or blogger will show parts of it to the general public. They may misrepresent your work, but most people will not check and even more cannot check because they cannot get behind the pay wall. Ultimately your work is more likely than not to remain trapped in this room forever, contributing to more work that shoots out the pipe into the pile of papers, all of which will remain trapped in this room forever.

Looking at this process, at this performance of information sharing, you wonder if you should just go into industry. Maybe it wont be so bad to work at a huge corporation. You are already working long hours, you might as well make some money. Qualifying for a low income health care card was simultaneously the high and low point of last week, which shines a depressing light on your life. The hours might suck, but they already suck in academia. The last time you spent a long period of time with people in the department, you timed that they all worked 10 hours per day and most of their github repositories have a commit made every day of the past year. Although moving to industry might mean spending your days performing highly unethical work while choking on the boots of monkey torturer Elon Musk. That might actually be worse. You spend a week trying to work out if you would rather degrade yourself for a billionaire CEO or a prestigious journal editor. At least the CEO would kind of pay you. Maybe you should just quit everything and become a gardener. You would continue to live below the poverty line, stuck in a share house with 20 year old, but being outside a lot might relax you. Either because of the sunk cost or because job interviews revolt you, you decide to continue with your PhD. Plus, your dad seemed really happy that you were doing it and it would be easier to finish the degree than deal with a lifetime of him expressing disappointment that you didn't finish it. Now that you have gone through your weekly emotional turmoil of considering quitting, deciding against it, and then trudging back to your computer to do your work, you can discuss the problem at hand. This murkey inefficient academic system has given birth to a collective understanding that says one thing, and research that says the opposite. By deciding to stay in academia you have decided to work with this cycling paper pile, put on your blindfold and grab fistfulls of papers, hoping someone might one day grab yours. So if we are going to work with this nightmare, lets talk about why its ruining your research.

#### Acadmia stress but this time it's mine
If you could not tell, that long winded second person narrative was actually about me. I bet you feel surprised by this unexpected information. You may assume this relates to my research (that despite the section above I am still doing) because there are good methods to visualise uncertainty but people are just unaware of them, and that is somewhat true. I did spend the first blog post of this series discussing how the general consensus and attitude towards visualising uncertainty does not align with the literature on the topic. It reminds me of when I was in highschool and told everyone there was no evidence that long term weight loss was possible despite a large number of clinical trials (if you don't believe me, google it) and was essentially treated like a basket case. I actually spent a weekend in a manic state in 2016 and made a stop motion [Youtube video](https://youtu.be/fFuQEKQhBn0) about it, but I digress. The actual link I want to make this week, is that this terrible organisation system for academia and information in general makes it hard to see the forest for the trees, and for broad and somewhat poorly defined topics such as "visualising uncetainty" I feel like I have a concussion from being hit in the face by the branches. 
-->

Part 1 established that a large reason people do not visualise uncertaitny is because of a view that it is of secondary imporance to estimates or other contextual information. Here I discuss methods of visualising uncertainty as an independent area of interest directly related to a hypothesis rather than as a by product of a data type or estimate. I present an example where visualising uncertainty without this framework leads leads to poor visualisations. 

### Introduction
When we talk about visualising uncertainty, the conversation often assumes that the *context* around the uncertainty is the most important factor, while the *hypothesis* related to the uncertainty and the reason we made the plot are considered secondary. Creating new visualisation techniques with this mindset leads to plot designs that are disconnected from the reasons people visualise uncertainty in practice. Here I suggest an alternative philosophy for making uncertainty visualisations by highlighting a few key ideas. First of all, I suggest that the hypothesis tied to an uncertainty estimate is more important than the context. Second I explain why context is considered to be the primary focus and why it shouldn't be. Finally I provide a suggestion on how to make a good plot by considering both the context and hypothesis and combining the best plot for each.

### Context is King
The way we talk about data always puts the type of data we have as the central aspect of the plot. It makes sense when you think about it. The data is not just in a vacuum, numbers have order and when we plot our data we always want to make relationships that are connected to that order apparent. Our axis have ascending numbers so we want the numbers to ascend and then any pattern seen in the data can be described in terms of our axis increasing. Two continuous variables should be depicted with scatter plots, spatial data with a map, time series with a line plot. 

Sometimes we mess with the relationship between variables and a little (and sometimes more than a little). If the variables are skewed we transform them but this still allows us to interpret the increasing relationship, although it is now in percentages. If there is no order to our variables What if there genuinely is no order to our variables? Then we order the variables based on the data itself but we still want to see if groups appear that might have some connecting factor. 

Sometimes we mess with this relationship a lot. There are many statistical methods we abandon the "context" of the data all together. Dimension reduction is a great example of this. Principal components and t-sne take our variables, use them to create a new (sometimes very difficult to understand) context and then plot the data in that space. Since these are statistical methods, they may be seen as a secondary to the data visualisation question I originally posed, but the visualization and dimension reduction are all in pursuit of the same goal. Since we are looking for a relationship first and foremost, we are willing to transform the space, getting further and further from our original variables, to find a relationship. Making our way back to the variables is seen as secondary. It is the use of dimension reduction that makes this implict goal in all data visualisation very explicit.

### The goal should trump
Implicit in that context conversation was the goal of our visualisation, *to find a relationship between our data pattern and the variable*. You may hear that and say *obviously* but making this assumption apparent is key to understanding is where we run into problem with our uncertainty visualisation. When I was talking about the context, I was trying to make it apparent that we warp and often transform the context in pursuit of our goal of finding a pattern. The steps we take in visualising data is always in pursuit of finding a pattern. When we visualise uncertainty we aren't always looking for relationships in the same way we are with typical data visualisation. Often the reason for uncertainty visualisation is to see *how valid these relationships are*, however, as I will discuss later, uncertainty plots have an almost infinite number of goals. So, Since the goal has changed, should we stick with the same visualisation rules and follow the same process of finding the best visualisation? 

# Under here is bit of an idea mess
--------------- 

#### Problems with putting the context above the goal
- Spatial uncertainty visualisations
- Time series correlation paper


#### Uncertainty information cannot be transferred (HOPS vs Bar Chart Example)
Uncertainty doesn't exist unless there is a decision. This seems counter intuitive when you think about data in general. Randomness is all around us and inherent in life, so it doesn't make sense that it can only exist with respect to a decision. Let me rephrase, quantifying (and therefore expressing through visualisation) only makes sense with respect to a decision. I slightly touched on this in my previous post, but I want to make this very clear now, the general concept of uncertainty (randomness or incomplete information) is a much broader category that contains the uncertainty we refer to when we visualise uncertainty (uncertainty related to a decision). This is because visualisations and estimates are made by a person with a purpose. They do not exist naturally in society, floating around with the wind, so it cannot take on the same purposeless existence of "uncertainty". The second we start quantifying, we are making decisions about what is important and what is relevant, there is no longer an unbiased natural uncertainty floating around, there is a quantified uncertainty that we have built for a purpose. Just in the way that this uncertainty was quantified for a purpose, it should be visualised with that purpose in mind. To discuss visualising uncertainty as though there is no purpose, no decision to be made, feels purposely obtuse. This is not alien in statistics, as a matter of fact confidnece intervals (the most common representations uncertainty) are derrived from hypothesis tests (a decision), however the literature seems to enjoy pretending it is. 

There is one paper in particular that comes to mind that VERY MUCH highlights how much of a problem this "uncertainty without purpose" issue is a plague in the field. I have seen MULTIPLE papers discussing how people are unable to understand confidence intervals and the ignorance of the method is upsetting. Participants were given two estimates with error bars and asked to move the estimates until they would no longer be considered significantly different. Basically every participant moved the error bars until they were just touching. Unfortunately the error bars were for the hypothesis test of "what is the 95% confidence interval of value A" and not "is value A different from value B" which would have a different interval. Is this a sign that people cannot take uncertainty from one decision and translate it to another? Yes, as a matter of fact it is strong evidence for the "express uncertainty related to the decision, not related to the model or data" point I'm trying to make. Is this a sign people cannot understand "uncertainty"? No. As a matter of fact, I would argue the participants understand uncertainty better than most people writing about uncertainty. The participants understood they were asking them to make a decision so the uncertainty depicted must be related to the decision because why would you be asked to make a decision with completely irrelevant uncertainty information. That is nonsensical and yet it is what they were asked to do. Another paper gives people HOPS plots and goes "well people can tell which value is bigger with a hops plot when they can't tell with error bars so it depicts uncertainty better" completely ignoring the fact that, again, that error bars ask "are these values significantly different to each other" not "is value A bigger than value B". The uncertainty the HOPS plot predicts includes "is this value bigger than the other", it doesn't include "are these values statistically different" which I assume it would fail on, just like the error bars. This general concept of "uncertainty" that exists without a decision is generating a lot of questionable literature and methods and causing perfectly good visualisations to be thrown out because we were using them for the wrong questions.

-- The way we talk about uncertainty is incorrect. There are two reasons 1) we connect uncertainty to its context, and in doing so don't recognise 2) uncertainty must be connected to a specific hypothesis.
-- Work through example of HOPS paper. Also mention temporal correlation paper Di talked about.
-- There is no such thing as temporal "uncertainty". You can have uncertainty related to a forecast, or time series estimate, you can have uncertainty related to a parameter that is given its context in "time" 
- You cannot translate between multiple types of uncertainty. I cannot use an error bar plot to determine which outcome is more likely, I cannot use a hypothetical outcome plot to tell you the statistically significant set of outcomes.
- visualisation with a purpose goes beyond uncertainty. If you want to compare the variance of different areas of your data, you dont want to visualise the uncertainty associated with a metric you should just visualise the variance. Getting the correlation between two time series is easier with a scatter plot than a line plot (cite). Make the goal of your visualisation the key decision in how you visualise it, not the data type.
- The literature doesn't even comment on this distinction of type of decision needed to be made. They just talk about better or worse methods to convey uncertainty and errors associated with different types. They don't talk about errors associated with specific types of questions even though it is SHOCKINGLY obvious from the literature.
- Hypothetical outcomes are good for scenario uncertainty (like forecasts that fork off and make two outcomes). Intervals/distributions are good for statistical uncertainty?. There is a connection with enough outcomes, which is why a large number of hypothetical outcomes can be used as a discreetised version of statistical uncertainty.
- I wonder if HOPS plots were better in that paper because the hypothesis was more appropriate rather than the "being forced to experience the distribution with time" thing

### Section 2: The Importance (or sometimes lack therefof) of context.
But what do we do when the uncertainty does not fit nicely with the plot of the data? This brings me to the second thing I have noticed with uncertainty visualisation, we love context but if its hard, we ignore the context. This is not just a problem for data with mutiple dimensions (e.g. spatial temporal data), it is also a problem when we have complicated models. Let me draw what I mean to give a better understanding.

<center>![](contextplots.jpeg)</center>

This distinction between too much and a little bit of context is implicit and important when we talk about uncertainty. The second the context and the decision are at odds with each other, we drop one. This is an issue when we have too much information, regardless of the model data. That being said, the typical method of visualising uncertainty with context may create more confusion than clarity. Take the depiction of a linear regression with a confidence band above. What is that the uncertainty for? That is, what decision is it related to? Typically when you make a linear regression, that bound will be an interval on your prediction, i.e. we are 95% certain the TRUE blue line lies inside that interval. That may seem intuitive, but we apply a lot of other hypothesis to this blue line that are not actually the same. For example, if I have a single predictor, does a straight line fitting inside this interval mean our coefficient is not significant? What about if I have multiple predictors? Another person might use the confidence interval as a confidence band for the data and assume new data will liekly fall into it. These decisions are all slightly different to the decision that constructed the band, and to plot the context with the uncertainty returns us to our previous issue, that the context creates the uncertainty, not the decision. Once we are operating under the assumption that the context created the uncertainty, people freely use the uncertainty to check decisions that are irrelevant to the uncertainty information, generating more information, not less.

With this in mind, you might have arrived at the question I have been sitting on, leaving me unable to work on the main idea of they thesis. Why is plotting uncertainty, specifically with spatial data, treated as a unique problem that needs to be solved. To me it is neither unique, nor is there an inherrent reason for it to be solved. Rather I would suggest that there is a greater issue which is that we don't have methods to effectively include context in uncertainty estimates, nor understanding on whether or not its even a good idea. Lets consider a simple map of Australia with an estimated average temperature for that state. If you express uncertainty information with that estimate, you need to have some goal in mind. Do you want people to be able to identify if states have statistically significant differences in their temperatures? Do you want to give people a concept of a range of possible outcomes for each state? Do you want to show there is less data in some states than others? Each of these questions would likely lead to a different visualisation, and herein lies the issue with the current landscape of visualising spatial uncertainty (and by extension uncertainty with too much context). Uncertainty plots made without a decision in mind are messy and directionless. Since they are designed to depict any "uncertainty" they are not the most effective in any particular case and become functionally useless.

<center>![](map1.png)</center>
<center>![](map2.png)</center>
<center>![](map3.png)</center>

(These are just screenshots from the Visumap paper, I will propperly make them in R later since I'm just word dumping for now)

Looking at these maps, I ask you, do you get an intuitive sense of "uncertainty"? I dont. Can you answer any of the questions above with these maps? I can't. So what is the point of depicting the uncertainty in this way? Similar to the way we produce research to throw it back into the echochamber of academia, visualising uncertainty like this implies the reason we include uncertainty is just to.... include it. It is devoid of motivation and appears entirely performative.

-- Context is of secondary importance when looking at plots. So long as it is considered the primary imporance we wont make useful plots. I can convey uncertainty without context, I cannot convey uncertainty without a hypothesis
-- If you say "what if I have uncertainty related to some interval" my follow up question will always be "uncertainty about **what**". 
-- Because of the way we think about data, where the context and type of data you have is always the most important, we translate that to uncertainty even though it is not the case.
-- Trying to invent new plots often bad when made through this context. Example with spatial plots. Key criticism is that they are WORSE THAN TWO PLOTS.
-- Two plots may be better than one. A lot of papers that talk about "spatial uncertainty" mean "expressing uncertainty but keep the spatial context". 
-- this means the plots are basically useless for many
-- Sometimes you have to drop the context because it is secondary to the point. that is not to say you shouldn't express it, but rather if you are trying to convey many ideas, very often multiple plots do better. 
-- What takes time is processing "bits" of information. We want people to process those "bits" as quickly or efficiently as possible. It is not always the case that
- all these methods are worse than just plotting two maps side by side (check the base case method for each of these plots). They also only depict a single value for uncertainty which means we can identify which areas have high error (which seems to be a product of the estimate itself in the example case) and nothing else.
- Things like animation and interactivity just add another "axis" to the plot (similar to adding colour or something). Eventually with enough "axis' we are not really summarising information anymore, just showing all of it. Looking at plots they generally convey one key piece of information and maybe some extra info. If a single plot is worse than two separate plots, and the swapping between mental effort is less than the effort to understand the bizzare graphics, you have made a bad plot. 
- alternatively, animation or interactivity is just a way to show multiple plots that are connected. Often two plots are better than one idea. 
- What do we compare plots to? Nothing? There is no placebo. People being able to answer general questions about uncertainty does not mean a plot is conveying uncertainty. Sometimes I think two plots should be a baseline case for understanding plots.

###X Section 3: Trying to Combine the Uncertainty and the Context.
-- Try to find best uncertainty visualization for spatial data is misguided endeavor. 
-- I suggest trying to find the best uncertainty plot for your data and then combine it with the best context plot. 
- "What uncertainty do we care about" decides how you quantify it and what aspect of the data it is connected to, "Why do we care about it" decides how you plot it.
- try offering a visualisation expression for every graph aspect (time, colour, position, shape etc) so you can always assign uncertainty? Infoviz stuff?

### ?? Discreteness of visualisation and level of uncertainty estimate
In my previous post I made a distinction between statistical and scenario uncertainty. When people try to convey the disaster of climate change they present multiple forecasts, each giving a different scenario for carbon emissions. The data of temperature and carbon emissions are presented as line plots because that is the data, but the uncertainty is presented as multiple possible outcomes. The multiple outcomes are not a product of a line plot, but rather a decision. What happens if we do something about climate change, and what happens if we don't? To look at the plot below and think of it as a method for "time series" rather than a method for "uncertaint event that information about can change the course of" would be rediculous.

<center>![Image by Katharine Hayhoe, from the 2017 Climate Science Special Report by the U.S. Global Change Research Program.](scenarioforecast.png)</center>

It is obvious from the plot above two things are happening. First, the data method typically trumps visualising uncertainty, which makes sense as I have spoken before about the way uncertainty is seen as a second class citizen.



