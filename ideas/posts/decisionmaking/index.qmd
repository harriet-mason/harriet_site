---
title: "Decision Making Problem"
author: "Harriet Mason"
---

![Visualisation of the process of transforming raw data to a visualisation insight. Summarising the data using common statistics removes information from our data. Once the data is in a format that represents the varibales of interest, organising that information in a visualisation makes it easier to extract insights from these statistics and convert them to insights. If information is removed at the statistics stage, it cannot be added back in at the visualisation stage, as it is meerly an efficient technique to organise and present information to enable a large range of insights. ](visprocess.jpeg){#fig-location width=70%}

Decision making tasks are often described by authors as a more realistic version how plots are used in practice. While we do not disagree with this statement, exactly *why* these tasks are different to value extraction tasks is never explained. While the task may be more similar to how plots are actually used, that does not necessarily mean it is a better *experimental* environment for evaluating plots. Let us consider this in more detail. A normal value extraction task involves:

1) correctly interpret the question
2) extract the specified value
3) report the value

On the other hand, a decision making task using an uncertainty visualisation involves:

1) Correctly interpret the question
2) Use some risk utility function to set a threshold for an "acceptable level of risk"
3) Extract the value for from the plot that will be compared to the threshold
4) Make a judgement based off that value

The key aspect that separates a decision making task from a value extraction task, for uncertainty visualisations, is the inclusion of a utility function. While this might seem like a small change, it actually completely warps the experiment in unexpected. 

The first issue is that individuals will all have their own individual risk utility function, that is, how much they want to avoid or engage with risks or uncertainty. Therefore decision making experiments that have an additional layer of noise that value extraction experiments do not. We cannot be sure if participants answered differently from the ground truth because a visualisation was difficult to read *or* because the participants risk utility function did not align with the one set by the authors. Several authors have offered solutions to this issue, however the problem is deeper than any of them seem to realise.

@Hullman2016 suggested providing a utility framework for each experiment, to instruct the participants in how to account for the uncertainty information. This is a method that seems to have been adopted by @Fernandes2018 who describe the following scenario to participants who are trying to maximise the coins in their experiment:
> Subjects gain coins for every minute they are able to continue an activity that is valuable to them (e.g., watching TV at home) before going to the bus stop, and gain a bonus for arriving at their intended destination early. Subjects incur a coin penalty for time spent waiting at a stop for a bus to arrive.

Any payment scheme that is incorporated into an experiment will also implicitly set up a utility framework, as it is assumed participants will try to maximise their payments. In tasks such as this, the ground truth of each question is typically selected to be the value a ration agent would select and visualisations are evaluated on the basis of how far the participants responses are from that ground truth. The problem with this, is that all the complexity of the question is in *working out* the best response, and it has little to do with the visualisation itself. The visualisation aspect of these studies becomes a value extraction experiment. 

@Cheong2016 tested multiple different visual representations of uncertainty for representing the likelihood of a house being burned down based on its location. Their payment scheme, which paid out $0.10 for a correct choice (i.e. staying when the house was not burned down or leaving when the house was burned down) and 0 for an incorrect choice (i.e.leaving when the house didn't burn down or staying when the house burned down), meant participants were incentivised to base their entire leave/stay decision on whether or not the likelihood of a fire at their house is above or below 50%. If the participants *did* correctly identify the optimum strategy and answer accordingly, the "decision making experiment" was actually *just a value extraction experiment*. The decision making aspect ceased to matter, all that mattered was identifying if the probability of a fire was greater than 50%. Interestingly, despite this very obvious and simple tactic to maximize payout from the experiment, it seems like many participants did not adopt it, instead acting much more warily as though they were considering whether or not they would *actually* evacuate in the event of a fire. This means it is likely that introducing a payment scheme or a utility function creates too much mental labour for the participants to answer "correctly", and they would rather stick with their own risk utility function that feels *natural* than go through the mental labour of adopting another. Setting up an experiment that requires participants to do laborious calculations or strenuous mental effort for a small marginal benefit is unlikely to result in successful participant responses because it is not in the "spirit" of visualisation. (*Cite Gap 16: Examples of value extraction decision making tasks*)

It is clear that audiences primarily interact with visualisations through their "System 1" brain [@daniel2017thinking]. This aligns with other authors comments that visualisation is primarily about "gists" [Spiegelhalter2017]. In this sense, asking participants a question that requires them to shift to their "System 2" is entirely contrary to *why* we use visualisation. The exact reasons we describe visualisation to be so powerful, that they are able to communicate complicated information fast and efficiently *is a product of the system 1 brain* [@daniel2017thinking], however this means displaying information using visualisations have the same weakness' as system 1 thinking, and visualisation authors need to be aware of this. Asking participants to answer questions that require complicated calculations does not reflect how visualisations are used, and will likely be ignored by the participants. While we agree with @Hullman2016 who suggested that what determines an appropriate ground truth is largely a philosophical exercise, we differ in the sense that we do not think it should be done, lest you create a needlessly complicated value extraction task.

Additionally, several papers mention "risk-aversion" in participant decisions as a negative by-product of a particular visualisation design, however that displays a failure to understand that risk aversion is *not* a mistake, but rather it *reveals the utility of decreased uncertainty*. It may be interesting to investigate why a particular visualisation elicits increased risk-aversion, for example is it due to poorer estimates or increased awareness of negative outcomes, however simply reporting this as "risk aversion" conflates these factors. The work that advocates for transparency in risk-communication gives you whiplash when it's subtext argues that transparency leads to "incorrect" conclusions. There have been other discussions on the appropriateness of discussing risk-aversion as a bias [@Vranas2000], but few have made the point that if uncertainty holds some *value* there is not technically a "right" decision at all. 
The final issue with decision making tasks is that calculating "optimum" choices is not a task we use uncertainty for, it is a task that utilises *risk*. Risk and uncertainty are slightly different, as risk is known probabilities and uncertainty is unknown probabilities, so uncertainty is what you get when you cannot accurately define risk [@Spiegelhalter2017]. Communicating risk allows people to weigh up options and make an optimum decision, uncertainty hedges information to let people know there may not even be an optimum strategy. In this sense, many "uncertainty" decision making experiments are actually just risk decision making experiments. Not only this, but the inclusion of uncertainty information should *not* impact the choices of a rational agent. A well behaved rational agent *should* ignore uncertainty information, so long as the estimate provided is still the best case. This point seems to be lost on many uncertainty visualisation authors. @Zhao2023 displayed a model's prediction and its estimated uncertainty and asked participants if they wanted to submit the model estimate as their answer, or make their own prediction. At no point did the authors seem to realise a rational agent would accept the model estimate every time, no matter how uncertain the information was, and participants were actually disqualified from the study for taking this approach to answering the questions. (*Cite Gap 17: Examples of studies where the correct choice was to ignore the uncertainty information*)

This leaves us with several issues in decision making experiments. They are noisy because of participants individual utility functions and when that is not the case they are thinly veiled and over complicated value extraction experiments. Additionally, authors don't even seem to agree how participants should incorporate uncertainty information, with some authors designing complicated utility functions, and others not realising the best response to their experiment is to ignore uncertainty information entirely. For these reasons, uncertainty visualisation experiments should either be designed with these caveats in mind and be designed to untangle why a particular visualisation will elicit a specific decision, or the field should steer away using decision quality and related metrics as a basis for evaluating visualisation performance.