---
title: "Plotting Apples, Oranges, and Distributions"
subtitle: "An alternative taxonomy to prevent information inequality in uncertainty visualisations"
bibliography: references.bib
author: Harriet Mason
institute: "Supervisors: Di Cook, Sarah Goodwin, Emi Tanaka, and Ursula Laa"
format: 
  revealjs: 
    theme: [default, custom.scss]
    slide-number: true
editor_options: 
  chunk_output_type: console
logo: appleorangelogo.jpeg
---
```{r}
#| label: setup
#| include: false
knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE
)
```

## The Structure of This Presentation
### Part 1
- Quiz Show

### Part 2
- Normal Presentation

::: {.notes}

This talk will be split into two sections. Half of it will be a quiz show, and the other half will be a normal, serious, presentation.

:::

## The Structure of This Presentation
### Part 1: Quiz Show
- Motivation (Q1-4)
  - The need for uncertainty visualisations
  - The need for a new taxonomy
- Chapter 1 (Q5-8)
  - Details of the visual information taxonomy 

::: {.notes}

The first section will be the work in the first chapter of my PhD. It will be in the form of a quiz show where my research over the past year will be fed to you as explanations or preamble to a quiz question. Questions 1 to 4 will be related the current work in uncertainty visualisation and questions 5 to 8 will detail the taxonomy of visual information I designed that allows us to better compare plots.

:::

## The Structure of This Presentation
### Part 2: Serious Presentation
- The Rest of the PhD
  - Chapter 2 & 3
  - Timeline
  
::: {.notes}

The second section will be an explanation of the work I plan to do for the rest of my PhD, and a timeline for the next few years. This section will not be a quiz show and will just be a normal presentation. Feel free to interrupt at any point with questions, either about the content itself or clarifying questions for the quiz. So if nobody has any questions about the structure...

:::

# Are You Smarter Than A Big 4 Consultant?

::: {.notes}

... lets get quizzing. One of my housemates works at EY as a consultant. She has very little statistics knowledge. She *did* take introduction to econometrics several years ago, however she failed the exam. During this presentation you will have to answer 8 multiple choice questions that involve extracting some information from a data visualisation. She and a couple of her team members have completed this quiz. Your goal as a group is to perform better than the average score for the consultants.

The quiz will be no fun if nobody answers it. Each question will get an explanation and then you will have 20 seconds to answer in the flux quiz.
:::

::: {style="text-align: center; margin-top: 6em"}
# The Current Literature 
::: {.notes}
Without further ado, lets jump into the first 4 questions.
:::
:::

::: {style="text-align: center; margin-top: 6em"}
## Question 1
### Needing a Good Visualisation
::: {.notes}
Question 1
:::
:::

## Anscombe's Quartet
:::: {.columns .v-center-container}

::: {.column width="50%"}
```{r}
#| fig-width: 5
library(tidyverse)
mypal <- c("#d53e4f","#F9D423","#66c2a5","#74add1")
# make tidy data
anscombe_tidy <- tibble(x = c(anscombe$x1, anscombe$x2,
                              anscombe$x3, anscombe$x4),
                        y = c(anscombe$y1, anscombe$y2,
                              anscombe$y3, anscombe$y4),
                        Plot = c(rep("Plot 1",11), rep("Plot 2",11),
                                 rep("Plot 3",11), rep("Plot 4",11)))

# plot
anscombe_tidy %>%
  ggplot(aes(x,y)) +
  geom_point(aes(fill=Plot), colour="black", 
             size=3, pch=21, alpha=0.75) +
  facet_wrap(~Plot) +
  theme_classic() +
  theme(aspect.ratio = 1,
        legend.position = "none") +
  scale_fill_manual(values = mypal)
```
:::

::: {.column width="50%"}

```{r}
library(gt)
# show table
anscombe_tidy %>%
  group_by(Plot) %>%
  summarise(x_mean = mean(x),
            y_mean = mean(y),
            x_sd = sd(x),
            y_sd = sd(y),
            correlation = cor(x,y))%>%
  gt(rowname_col = "Plot") %>%
  tab_header(title = "Anscombe's Quartet Summary Statistics")  %>%
  tab_spanner(label = "Mean",
              columns = c(x_mean, y_mean)) %>%
  tab_spanner(label = "Standard Deviation",
              columns = c(x_sd, y_sd)) %>%
  cols_label(x_mean = md("**X**"),
             y_mean = md("**Y**"),
             x_sd = md("**X**"),
             y_sd = md("**Y**"),
             correlation = md("**Correlation**")) %>%
  fmt_number(decimals=2)
  
```

:::

::::

::: {.notes}
Lets get started by talking about the importance of visualisation. This is a plot of the famous data set, Anscombe's quartet. This data set highlights the importance of visualising your data. Since all 4 plots have the same mean, correlation and standard deviation, they appear identical when you use summary statistics to extract interesting information. 

Now, visualisation is not only useful for providing insights.
:::

## The Importance of Visualisation
::: {.incremental}
- Sketching distribution = better predictions ^[@Hullman2018; @Goldstein2014]
- Interactive graphics = better understanding ^[@Potter2009; @Ancker2009]
- Infographics = more accessible ^[@Ancker2009]
:::
::: {.notes}
- Something as simple as sketching a distribution before recalling statistics or making predictions can greatly increase the accuracy of those Estimates.  
- Additionally, since visualisations allow for interactive graphics they provide a more in depth understanding of probability 
- and are more accessible, since infographics can be understood by people with poor numeracy skills. 

Visualisation is important, but it is only well done visualisations that provide these benefits.
:::
## Question 1: Spot the Odd One Out
```{r}
library(countdown)
countdown(minutes = 0, seconds = 20, 
          margin = "0.1em", )
```

```{r}
#| layout-ncol: 2
mypal <- c("#d53e4f", "#fdae61","#F9D423", "#fee08b",
           "#66c2a5", "#66bd63", "#3288bd", "#74add1")
# Make Fake Data
set.seed(1)
ans_vector <- as.vector(as.matrix(anscombe))
dupe_anscombe <- tibble(x1 = sample(ans_vector, 11),
                        x2 = sample(ans_vector, 11),
                        x3 = sample(ans_vector, 11),
                        x4 = sample(ans_vector, 11),
                        y1 = sample(ans_vector, 11),
                        y2 = sample(ans_vector, 11),
                        y3 = sample(ans_vector, 11),
                        y4 = sample(ans_vector, 11))

# Box plot
p1 <- anscombe %>%
  pivot_longer(cols=everything(),
               names_to = "Variable",
               values_to = "Value") %>%
  ggplot(aes(x=Variable, y=Value, fill=Variable)) +
  geom_boxplot() +
  theme_classic() + 
  ggtitle("a) Plot 1") +
  scale_fill_manual(values = mypal) + 
  theme(aspect.ratio = 1/2,
        text=element_text(size=21),
        plot.title = element_text(hjust = 0.5))

# Bad Bubble Chart
p2 <- anscombe %>%
  ggplot(aes(x=x1, y=y1)) +
  geom_segment(aes(x=x1-0.5*x4,xend=x1+0.5*x4,yend=y1)) +
  geom_segment(aes(xend=x1, y=y1-0.5*y4, yend=y1+0.5*y4)) +
  geom_label(aes(size=x2, alpha=y2, colour=x3, label = y3)) +
  theme_classic() + 
  theme(aspect.ratio = 1/2,
        legend.position = "none",
        text=element_text(size=21),
        plot.title = element_text(hjust = 0.5)) +
  ggtitle("b) Plot 2")

# Eight Line Plots
p3 <- anscombe %>%
  mutate(ID=row_number()) %>%
  pivot_longer(cols=x1:y4,
               names_to = "Variable",
               values_to = "Value")  %>%
  ggplot(aes(ID, Value, colour=Variable)) +
  geom_line(size=2, alpha=0.7) +
  theme_classic() + 
  scale_colour_manual(values = mypal) + 
  labs(title = "c) Plot 3", x = "Order in Data Set" )+
  theme(aspect.ratio = 1/2,
        text=element_text(size=21),
        plot.title = element_text(hjust = 0.5))


# Stacked histogram
p4 <- dupe_anscombe %>%
  pivot_longer(cols=everything(),
               names_to = "Variable",
               values_to = "Value") %>%
  ggplot(aes(Value, fill = Variable)) +
  geom_histogram(binwidth = 1) + 
  theme_classic() +
  scale_fill_manual(values = mypal) + 
  ggtitle("d) Plot 4")+
  theme(aspect.ratio = 1/2,
        text=element_text(size=21),
        plot.title = element_text(hjust = 0.5))
p1
p2
p3
p4
```

::: {.notes}
And that leads us to our first question. We often don't know the best way to visualise a distribution, nor do we know which variables should go togehter. On this slide I have four data visualisations. Three of them were made using the Anscombe's quartet data, one was made with fake data that was randomly sampled from the real Anscombe's quartet. Which one of the four plots on this page was not made using the real Anscombe's quartet data?
You have 20 seconds to answer and your time starts now.
:::

## Solution
::::: {.columns}

:::: {.column width="50%"}
```{r}

# Dupe Plot
p4 + ggtitle("Fake Data")

# Dupe Statistics
tibble(x = c(dupe_anscombe$x1, dupe_anscombe$x2,
             dupe_anscombe$x3, dupe_anscombe$x4),
       y = c(dupe_anscombe$y1, dupe_anscombe$y2,
             dupe_anscombe$y3, dupe_anscombe$y4),
       Plot = c(rep("Plot 1",11), rep("Plot 2",11),
                rep("Plot 3",11), rep("Plot 4",11)))%>%
  group_by(Plot) %>%
  summarise(x_mean = mean(x),
            y_mean = mean(y),
            x_sd = sd(x),
            y_sd = sd(y),
            correlation = cor(x,y))%>%
  gt(rowname_col = "Plot") %>%
  tab_header(title = "Fake Data Summary Statistics")  %>%
  tab_spanner(label = "Mean",
              columns = c(x_mean, y_mean)) %>%
  tab_spanner(label = "Standard Deviation",
              columns = c(x_sd, y_sd)) %>%
  cols_label(x_mean = md("**X**"),
             y_mean = md("**Y**"),
             x_sd = md("**X**"),
             y_sd = md("**Y**"),
             correlation = md("**Correlation**")) %>%
  fmt_number(decimals=2)
```
::::

:::: {.column width="50%"}
```{r}
# Real plot
p5 <- anscombe %>%
  pivot_longer(cols=everything(),
               names_to = "Variable",
               values_to = "Value") %>%
  ggplot(aes(Value, fill = Variable)) +
  geom_histogram(binwidth = 1) + 
  theme_classic() +
  scale_fill_manual(values = mypal) + 
  ggtitle("Real Data")+
  theme(aspect.ratio = 1/2,
        text=element_text(size=21),
        plot.title = element_text(hjust = 0.5))
p5

```
::: {style="text-align: center; margin-top: 3em"}
[Results](https://flux.qa/#/presentations/64805896ff94fa54629cd870/6480588eff94fa54629cd86e?tab=polls&poll=648059aaff94fa54629cd881){preview-link="true"}
:::
::::
:::::

::: {.notes}
The correct answer was plot 4, the stacked histogram. If you look at the distribution of x4 it is easy to see that this plot is not using the same data. The consultants, on average, picked out the correct plot 0% of the time. Lets see this group did by comparison.
:::

## Showing The Right Information
```{r}
#| layout-ncol: 2
p1 + ggtitle("Box Plot")
p3 + ggtitle("Line Plot")
p2 + ggtitle("Bubble Plot (?)")
p5 + ggtitle("Stacked Bar Chart")
```
::: {.notes}
The take away from this question is not that you should be able to identify the Anscombe's quartet data no matter how it is presented, but rather that the presentation matters. None of the visualisations shown here identified the interesting features I displayed in the previous slide. These visualisations have the same problem as the numerical summary, because despite being visualisations, they displayed the wrong information. What is rarely discussed about Anscombe's quartet is that we already know the best way to display it to show the important information.
:::

::: {style="text-align: center; margin-top: 6em"}
## Questions 2 & 3
### Decision Making Under Uncertainty
::: {.notes}
Which leads us to our next questions
:::
:::


## ...Is uncertainty important information?
::: {.incremental}
- Only 1/4  of visualisation authors show uncertainty in 50% or more of their graphics. ^[@Hullman2020a]
- Reasons include:
  - overwhelm the audience
  - inability to calculate
  - a lack of information 
  - not wanting to make their data seem questionable

:::
::: {.notes}
- Is uncertainty important information? This isn't a quiz question its rhetorical. The amount we actually visualise uncertainty would lead to believe it isn't. 
- A survey of visualisations authors showed that only a quarter of them included uncertainty in 50% or more of their visualisations. 
- The most common reasons provided were things like 
- a fear of overwhelming the audience, 
- an inability to calculate the uncertainty, 
- not having access to the uncertainty information, 
- and not wanting people to ask them about their results.

When we talk about uncertainty, we usually mean the sampling distribution for some estimate. There is an underlying belief in all the reasons provided by visualisation authors that uncertainty is of secondary importance to our estimates. This comes through in the colloquial ways we talk about uncertainty too. We call uncertainty the noise and the estimate the signal. It implies that uncertainty is something we should brush away to find some truth, as though the uncertainty information is not of importance by itself. 

The next two quiz questions are going to be about that hypothesis.
:::

## A Scenario

::: {.notes}
I am now going to provide a fictitious scenario that sets the backdrop for the next two questions. 
:::

## A Scenario 
::: {style="font-size: 80%;"}
It is currently 9pm and you are sitting at home watching TV.
:::

## A Scenario 
::: {style="font-size: 80%;"}
It is currently 9pm and you are sitting at home watching TV. Tram tracker is predicting your tram to arrive at your stop in 8 minutes.
:::

## A Scenario 
::: {style="font-size: 80%;"}
It is currently 9pm and you are sitting at home watching TV. Tram tracker is predicting your tram to arrive at your stop in 8 minutes. You have been catching this tram for a while, so you know it typically arrives a little early, but occasionally you are left waiting at the tram stop for so long you wonder if the line is down.
:::

## A Scenario 
::: {style="font-size: 80%;"}
It is currently 9pm and you are sitting at home watching TV. Tram tracker is predicting your tram to arrive at your stop in 8 minutes. You have been catching this tram for a while, so you know it typically arrives a little early, but occasionally you are left waiting at the tram stop for so long you wonder if the line is down. The time it takes to walk to the tram is negligible and the tram will leave as soon as it arrives at its stop.
:::

## A Scenario 
::: {style="font-size: 80%;"}
It is currently 9pm and you are sitting at home watching TV. Tram tracker is predicting your tram to arrive at your stop in 8 minutes. You have been catching this tram for a while, so you know it typically arrives a little early, but occasionally you are left waiting at the tram stop for so long you wonder if the line is down. The time it takes to walk to the tram is negligible and the tram will leave as soon as it arrives at its stop. Its freezing cold outside so for every minute you spend at home watching TV you gain 2 utility points, and for every minute you spend at the tram stop waiting, you lose 1 utility point.
:::

## A Scenario 
::: {style="font-size: 80%;"}
It is currently 9pm and you are sitting at home watching TV. Tram tracker is predicting your tram to arrive at your stop in 8 minutes. You have been catching this tram for a while, so you know it typically arrives a little early, but occasionally you are left waiting at the tram stop for so long you wonder if the line is down. The time it takes to walk to the tram is negligible and the tram will leave as soon as it arrives at its stop. Every minute you spend at home watching TV you gain 2 utility points, and for every minute you spend at the tram stop waiting, you lose 1 utility point. If you miss your tram and have to wait for the next one you will automatically get a total of -10 utility points.
:::

## Question 2: Tram Times #1
When is the best time to arrive at the tram stop?
```{r}
library(countdown)
countdown(minutes = 0, seconds = 20, 
          margin = "0.1em", )
```

```{r}
#| output: false

#simulated
set.seed(1)
arrival_times <- as.POSIXct("2023-06-12 9:04:00") + 60*rexp(10000, rate=1/4)
start <- as.POSIXct("2023-06-12 9:00:00")
leave_times <- seq(as.POSIXct("2023-06-12 9:00:00"),
                   as.POSIXct("2023-06-12 9:15:00"),
                   by = 30)
mins_at_home = as.numeric(leave_times-start)/60

# set up for loop
utilitymatrix <- matrix(nrow=length(arrival_times),
                        ncol=length(mins_at_home))
for(i in seq(length(arrival_times))){
  mins_at_stop = pmax(as.numeric(arrival_times[i]-leave_times)/60,0)
  utility = 2*mins_at_home - mins_at_stop
  utility[mins_at_stop<0.00001] <- -10
  utilitymatrix[i,] <- utility
}
average_utility <- colMeans(utilitymatrix)
```

```{r}
# Set leave times & colours
leave_times <- seq(as.POSIXct("2023-06-12 9:00:00"),
                   as.POSIXct("2023-06-12 9:15:00"),
                   by = 60)
leave_times <- leave_times[c(4,5,6,7,8,9)] - 30
line_cols <- mypal[-c(4,8)]

# Generate Data
set.seed(1)
min_arrival <- as.POSIXct("2023-06-12 8:56:00")
max_arrival <- as.POSIXct("2023-06-12 9:20:00")
current_time <- as.POSIXct("2023-06-12 9:00:00")
arrival_times <- as.POSIXct("2023-06-12 9:04:00") + 60*rexp(50, rate=1/4)
tram_times <- tibble(arrival_times = arrival_times)
estimated <- mean(arrival_times)

# Axis Breaks
my_breaks <- seq(min_arrival, max_arrival, by = 60) 
my_labels <- (my_breaks - current_time)/60
my_labels[!(seq(25) %in% seq(5,25,5))] <- ""

# Make plot
tram_times %>%
  ggplot(aes(x=arrival_times))  +
  theme_classic() +
  geom_segment(x=estimated, xend=estimated, 
               y=-1, yend=1, size=1) +
  geom_label(x=estimated, y=1,
             label="9:09pm",
             fill = "black", fontface = "bold",
             colour="white") +
  geom_segment(x=current_time, xend=current_time, colour="grey",
               y=-1, yend=1, size=1) +
  geom_label(x=current_time, y=1,
             label="9:00pm", fill = "grey", colour="white") +
  geom_segment(x=leave_times[1], xend=leave_times[1],
               colour=line_cols[1], y=-1, yend=0.5, size=1) +
  geom_label(x=leave_times[1], y=0.5, label= "A",
             fill = line_cols[1], colour="white") +
  geom_segment(x=leave_times[2], xend=leave_times[2],
               colour=line_cols[2], y=-1, yend=0.5, size=1) +
  geom_label(x=leave_times[2], y=0.5, label= "B",
             fill = line_cols[2], colour="white") +
  geom_segment(x=leave_times[3], xend=leave_times[3],
               colour=line_cols[3], y=-1, yend=0.5, size=1) +
  geom_label(x=leave_times[3], y=0.5, label= "C",
             fill = line_cols[3], colour="white") +
  geom_segment(x=leave_times[4], xend=leave_times[4],
               colour=line_cols[4], y=-1, yend=0.5, size=1) +
  geom_label(x=leave_times[4], y=0.5, label= "D",
             fill = line_cols[4], colour="white") +
  geom_segment(x=leave_times[5], xend=leave_times[5],
               colour=line_cols[5], y=-1, yend=0.5, size=1) +
  geom_label(x=leave_times[5], y=0.5, label= "E",
             fill = line_cols[5], colour="white") +
  geom_segment(x=leave_times[6], xend=leave_times[6],
               colour=line_cols[6], y=-1, yend=0.5, size=1) +
  geom_label(x=leave_times[6], y=0.5, label= "F",
             fill = line_cols[6], colour="white") +
  scale_x_datetime(breaks = my_breaks,
                   labels = my_labels,
                   limits = c(min_arrival, max_arrival)) +
  theme(axis.line.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.title.y = element_blank(),
        axis.text.y = element_blank()) +
  labs(x = "Tram Wait Time (in mins)")
```

::: {.notes}
When is the best time to arrive at the tram stop? There are 6 options on the slide. I can go back to the previous slide if you want, but the utility points should not be considered that deeply, they mostly exist to highlight that there is a correct answer. I recommend you just take a guess and try to answer this question by thinking about when you would actually arrive at the tram stop. You have 20 seconds and your time starts now.
:::


## Solution
The best time to arrive is at time B

```{r}
# answer plot
tram_times %>%
  ggplot(aes(x=arrival_times))  +
  theme_classic() +
  geom_segment(x=estimated, xend=estimated, 
               y=-1, yend=1, size=1) +
  geom_label(x=estimated, y=1,
             label="9:08pm",
             fill = "black", fontface = "bold",
             colour="white") +
  geom_segment(x=current_time, xend=current_time, colour="grey",
               y=-1, yend=1, size=1) +
  geom_label(x=current_time, y=1,
             label="9:00pm", fill = "grey", colour="white") +
  geom_segment(x=leave_times[2], xend=leave_times[2],
               colour=line_cols[2], y=-1, yend=0.5, size=1) +
  geom_label(x=leave_times[2], y=0.5, label= "B",
             fill = line_cols[2], colour="white") +
  scale_x_datetime(breaks = my_breaks,
                   labels = my_labels,
                   limits = c(min_arrival, max_arrival)) +
  theme(axis.line.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.title.y = element_blank(),
        axis.text.y = element_blank()) +
  labs(x = "Tram Wait Time (in mins)")

```

[Results](https://flux.qa/#/presentations/64805896ff94fa54629cd870/6480588eff94fa54629cd86e?tab=polls&poll=6486f0a5ff94fa54629cf0bd){preview-link="true"}


::: {.notes}
The best time to leave was at time B. The consultants, on average, picked the correct time  25% of the time. Lets see how you did.
:::



## If you were wondering...

```{r}
tram_times %>%
  ggplot(aes(x=arrival_times))  +
  geom_dotplot(method="histodot", fill="grey90", binwidth = 60) +
  theme_classic() +
  geom_segment(x=estimated, xend=estimated, 
               y=-1, yend=1, size=1) +
  geom_label(x=estimated, y=1,
             label="9:09pm",
             fill = "black", fontface = "bold",
             colour="white") +
  geom_segment(x=current_time, xend=current_time, colour="grey",
               y=-1, yend=1, size=1) +
  geom_label(x=current_time, y=1,
             label="9:00pm", fill = "grey", colour="white") +
  geom_segment(x=leave_times[1], xend=leave_times[1],
               colour=line_cols[1], y=-1, yend=0.5, size=1) +
  geom_label(x=leave_times[1], y=0.5, label= "A",
             fill = line_cols[1], colour="white") +
  geom_segment(x=leave_times[2], xend=leave_times[2],
               colour=line_cols[2], y=-1, yend=0.5, size=1) +
  geom_label(x=leave_times[2], y=0.5, label= "B",
             fill = line_cols[2], colour="white") +
  geom_segment(x=leave_times[3], xend=leave_times[3],
               colour=line_cols[3], y=-1, yend=0.5, size=1) +
  geom_label(x=leave_times[3], y=0.5, label= "C",
             fill = line_cols[3], colour="white") +
  geom_segment(x=leave_times[4], xend=leave_times[4],
               colour=line_cols[4], y=-1, yend=0.5, size=1) +
  geom_label(x=leave_times[4], y=0.5, label= "D",
             fill = line_cols[4], colour="white") +
  geom_segment(x=leave_times[5], xend=leave_times[5],
               colour=line_cols[5], y=-1, yend=0.5, size=1) +
  geom_label(x=leave_times[5], y=0.5, label= "E",
             fill = line_cols[5], colour="white") +
  geom_segment(x=leave_times[6], xend=leave_times[6],
               colour=line_cols[6], y=-1, yend=0.5, size=1) +
  geom_label(x=leave_times[6], y=0.5, label= "F",
             fill = line_cols[6], colour="white") +
  scale_x_datetime(breaks = my_breaks,
                   labels = my_labels,
                   limits = c(min_arrival, max_arrival)) +
  theme(axis.line.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.title.y = element_blank(),
        axis.text.y = element_blank()) +
  labs(x = "Tram Wait Time (in mins)")
```

::: {.notes}
If you were wondering, this is how the probability distribution for your trams arrival time looked. Maybe with this information you would have picked differently, maybe not, regardless, we are moving on.
:::

## Another Scenario
::: {style="font-size: 80%;"}
After you missed your tram last week you called Public Transport Victoria (PTV) sobbing hysterically.
:::

## Another Scenario
::: {style="font-size: 80%;"}
After you missed your tram last week you called Public Transport Victoria (PTV) sobbing hysterically. Their whole office felt a little embarrassed for you honestly.
:::

## Another Scenario
::: {style="font-size: 80%;"}
After you missed your tram last week you called Public Transport Victoria (PTV) sobbing hysterically. Their whole office felt a little embarrassed for you honestly. PTV apologised for the issues and agreed to put the probability distributions on their tram tracker app.
:::

## Another Scenario
::: {style="font-size: 80%;"}
After you missed your tram last week you called Public Transport Victoria (PTV) sobbing hysterically. Their whole office felt a little embarrassed for you honestly. PTV apologised for the issues and agreed to put the probability distributions on their tram tracker app. Every PTV employee worked day shifts and night shifts to achieve a feat some would say rivals the building of the pyramids.
:::

## Another Scenario
::: {style="font-size: 80%;"}
After you missed your tram last week you called Public Transport Victoria (PTV) sobbing hysterically. Their whole office felt a little embarrassed for you honestly. PTV apologised for the issues and agreed to put the probability distributions on their tram tracker app. Every PTV employee worked day shifts and night shifts to achieve a feat some would say rivals the building of the pyramids. The tram tracker update was ready within 5 days.
:::

## Another Scenario
::: {style="font-size: 80%;"}
After you missed your tram last week you called Public Transport Victoria (PTV) sobbing hysterically. Their whole office felt a little embarrassed for you honestly. PTV apologised for the issues and agreed to put the probability distributions on their tram tracker app. Every PTV employee worked day shifts and night shifts to achieve a feat some would say rivals the building of the pyramids. The tram tracker update was ready within 5 days. 

Tonight you are catching a new tram line that is unfamiliar to you.

:::

## Another Scenario
::: {style="font-size: 80%;"}
After you missed your tram last week you called Public Transport Victoria (PTV) sobbing hysterically. Their whole office felt a little embarrassed for you honestly. PTV apologised for the issues and agreed to put the probability distributions on their tram tracker app. Every PTV employee worked day shifts and night shifts to achieve a feat some would say rivals the building of the pyramids. The tram tracker update was ready within 5 days.

Tonight you are catching a new tram line that is unfamiliar to you. Assuming you have the same utility system as last week (+2 points for being at home, -1 point for waiting at the tram stop, -10 points for missing the tram).
:::

## Question 3: Tram Times #2
When is the best time to arrive at the tram stop?
```{r}
library(countdown)
countdown(minutes = 0, seconds = 20, 
          margin = "0.1em", )
```

```{r}
#| output: false

#simulated
set.seed(1)
arrival_times <- as.POSIXct("2023-06-12 9:9:00") + 60*rnorm(1000, sd=3)
start <- as.POSIXct("2023-06-12 9:00:00")
leave_times <- seq(as.POSIXct("2023-06-12 9:00:00"),
                   as.POSIXct("2023-06-12 9:15:00"),
                   by = 60)
mins_at_home = as.numeric(leave_times-start)/60

# set up for loop
utilitymatrix <- matrix(nrow=length(arrival_times),
                        ncol=length(mins_at_home))
for(i in seq(length(arrival_times))){
  mins_at_stop = pmax(as.numeric(arrival_times[i]-leave_times)/60,0)
  utility = 2*mins_at_home - mins_at_stop
  utility[mins_at_stop<0.00001] <- -10
  utilitymatrix[i,] <- utility
}
average_utility <- colMeans(utilitymatrix)
```

```{r}
# Set leave times & colours
leave_times <- seq(as.POSIXct("2023-06-12 9:00:00"),
                   as.POSIXct("2023-06-12 9:15:00"),
                   by = 60)
leave_times <- leave_times[c(5,6,7,8,9,10)] - 30

# Generate Data
set.seed(1)
min_arrival <- as.POSIXct("2023-06-12 8:56:00")
max_arrival <- as.POSIXct("2023-06-12 9:20:00")
current_time <- as.POSIXct("2023-06-12 9:00:00")
arrival_times <- as.POSIXct("2023-06-12 9:9:00") + 60*rnorm(50, sd=3)
tram_times <- tibble(arrival_times = arrival_times)
estimated <- mean(arrival_times)
# Make plot
tram_times %>%
  ggplot(aes(x=arrival_times)) +
  geom_dotplot(method="histodot", fill="grey90", binwidth = 60)+
  theme_classic() +
  geom_segment(x=estimated, xend=estimated, 
               y=-1, yend=1, size=1) +
  geom_label(x=estimated, y=1,
             label="9:08pm",
             fill = "black", fontface = "bold",
             colour="white") +
  geom_segment(x=current_time, xend=current_time, colour="grey",
               y=-1, yend=1, size=1) +
  geom_label(x=current_time, y=1,
             label="9:00pm", fill = "gray", colour="white") +
  geom_segment(x=leave_times[1], xend=leave_times[1],
               colour=line_cols[1], y=-1, yend=0.5, size=1) +
  geom_label(x=leave_times[1], y=0.5, label= "A",
             fill = line_cols[1], colour="white") +
  geom_segment(x=leave_times[2], xend=leave_times[2],
               colour=line_cols[2], y=-1, yend=0.5, size=1) +
  geom_label(x=leave_times[2], y=0.5, label= "B",
             fill = line_cols[2], colour="white") +
  geom_segment(x=leave_times[3], xend=leave_times[3],
               colour=line_cols[3], y=-1, yend=0.5, size=1) +
  geom_label(x=leave_times[3], y=0.5, label= "C",
             fill = line_cols[3], colour="white") +
  geom_segment(x=leave_times[4], xend=leave_times[4],
               colour=line_cols[4], y=-1, yend=0.5, size=1) +
  geom_label(x=leave_times[4], y=0.5, label= "D",
             fill = line_cols[4], colour="white") +
  geom_segment(x=leave_times[5], xend=leave_times[5],
               colour=line_cols[5], y=-1, yend=0.5, size=1) +
  geom_label(x=leave_times[5], y=0.5, label= "E",
             fill = line_cols[5], colour="white") +
  geom_segment(x=leave_times[6], xend=leave_times[6],
               colour=line_cols[6], y=-1, yend=0.5, size=1) +
  geom_label(x=leave_times[6], y=0.5, label= "F",
             fill = line_cols[6], colour="white") +
  scale_x_datetime(breaks = my_breaks,
                   labels = my_labels,
                   limits = c(min_arrival, max_arrival)) +
  theme(axis.line.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.title.y = element_blank(),
        axis.text.y = element_blank()) +
  labs(x = "Tram Wait Time (in mins)")
```
::: {.notes}
Same principals as before, you have 20 seconds to make a decision.
:::

## Solution
The best time to arrive is at time D

```{r}
# answer plot
tram_times %>%
  ggplot(aes(x=arrival_times))  +
  geom_dotplot(method="histodot", fill="grey90", binwidth = 60)+
  theme_classic() +
  geom_segment(x=estimated, xend=estimated, 
               y=-1, yend=1, size=1) +
  geom_label(x=estimated, y=1,
             label="9:09pm",
             fill = "black", fontface = "bold",
             colour="white") +
  geom_segment(x=current_time, xend=current_time, colour="grey",
               y=-1, yend=1, size=1) +
  geom_label(x=current_time, y=1,
             label="9:00pm", fill = "grey", colour="white") +
  geom_label(x=leave_times[4], y=0.51, label= "D",
             fill = line_cols[4], colour="white") +
  geom_segment(x=leave_times[4], xend=leave_times[4],
               colour=line_cols[4], y=-1, yend=0.5, size=1) +
  scale_x_datetime(breaks = my_breaks,
                   labels = my_labels,
                   limits = c(min_arrival, max_arrival)) +
  theme(axis.line.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.title.y = element_blank(),
        axis.text.y = element_blank()) +
  labs(x = "Tram Wait Time (in mins)")

```

[Results](https://flux.qa/#/presentations/64805896ff94fa54629cd870/6480588eff94fa54629cd86e?tab=polls&poll=6486f074ff94fa54629cf0bb){preview-link="true"}


::: {.notes}
The best time to leave was at time D. The consultants, on average, picked the correct time  75% of the time. Lets see how you did.
:::

## Uncertainty = Better Decisions
::: {.incremental}
- Some research that the general public cannot understand complicated statistics ^[@Hoekstra2014; @Bella2005] 
- ... presenting uncertainty improves decision making both
  - experimentally ^[@Joslyn2012; @Savelli2013; @Kay2016; @Fernandes2018] 
  - and in practice ^[@Al-Kassab2014].
- No uncertainty = unbound expectations ^[@Savelli2013]

:::
::: {.notes}
So the point with these two questions was to give an illustration in of the importance of showing uncertainty, however the benefits of uncertainty visualisation are already well known in the literature.

- While there is some research that the general public cannot understand complicated concepts in statistics
- presenting uncertainty information improves decision making 
- both experimentally 
- and in practice.

As a matter of fact, doing what many authors currently and expressing 

- no uncertainty causes decision makers to have completely unbounded expectations on an outcome.
:::

## Uncertainty = Better Decisions
::: {.incremental}
- Can't avoid visualising uncertainty by...
   - explaining calculations ^[@Joslyn2012]
   - explaining benefits of a recommendation ^[@Joslyn2012]
   - being vague ^[@Erev_1990; @Olson_1997]

:::
::: {.notes}
Also, trying to 

- avoid visualing uncertainty by
- explaining the calculations, 
- or the benefits of your recommendation, 
- or by just being vague,

will not result in the same benefits as explicitly quantifying and expressing your uncertainty 
:::

::: {style="text-align: center; margin-top: 6em"}
## Question 4 
### Current Graphics Categories
::: {.notes}
So I have established the importance of providing a good visualisation as well as the importance of expressing uncertainty, but what is a good uncertainty visualisation. I mean what even is a visualisation of uncertainty. 
:::
:::


## Alternative Taxonomy
::: {fig-align="center"}
<iframe width="1200" height="600" src="https://clauswilke.com/dataviz/directory-of-visualizations.html"></iframe>
:::

::: {.notes}
There are quite a few taxonomies or organisation systems for visualisations. Here is an example of one, it is from the Fundamentals of Data Visualization book and it organises visualisations into the 7 categories: 

- (See Webpage)

There are many other organisation methods, but they all suffer from the same problem. A lot of the distinctions between plots are quite arbitrary and doesn't meaningfully group the visualisations. They are also quite inflexbile, so we end up with a bunch of different names for what is functionally the same plot used on different data. 
:::

## Question 4: Apply the Alternative Taxonomy
Which of these plots are uncertainty visualisations?

```{r}
library(countdown)
countdown(minutes = 0, seconds = 20, 
          margin = "0.1em", )
```

![](6dists.jpeg){fig-align="center" width=80%}

:::{ .notes}
Since we have the category of "uncertainty visualisation" some plots must belong to that group, while others don't. If that was not the case, why even discuss "uncertainty visualisation" as it's own topic. So, your 4th quiz question is "which of these plots are uncertainty visualisations? You have 20 seconds and your time starts now.
:::

## Solution
They all are.


![](6dists.jpeg){fig-align="center"}

[Results](https://flux.qa/#/presentations/64805896ff94fa54629cd870/6480588eff94fa54629cd86e?tab=polls&poll=64882dc7ff94fa54629cf56a){preview-link="true"}


::: {.notes}
They all are. All the plots shown in the previous slide are by some paper or another considered an uncertainty visualisation. 

- 0% of the consultants picked the all the plots. However a bar chart was the only plot nobody picked. Lets see how you did.

This result makes sense when you really think about it.

- Looking at a scatter plot gives us an idea of mass, but that is something that the eye plot also does. 
- A boxplot doesn't give a good idea of mass but it does highlight several percentile thresholds, something that error bars do which we generally consider to be an uncertainty distribution. 
- A bar chart and a choropleth map can both be consided an outcome of a sample just like a scatter plot but also we can consider uncertainty by changing the value the plot express to a probability. 

These plots have more similarities across groups than differences between them. It makes you start to wonder if there is no such thing as an uncertainty visualisation. And that is exactly my point.
:::

## What's an "Uncertainty Visualisation"

![](scribblemess.jpeg){fig-align="center"}

:::{ .notes}
If there is no classification of "uncertainty visualisation", how would we visualise uncertainty? Would we provide an expression of multiple possible values, a probability estimate, some feature that would allow us to identify a distribution?

There isn't really an answer for this. We don't have a good concept of what it even means to visualise a distribution. There is no way to find gaps in our current visualisation literature if we don't even know what that literature covers. We need to have a better language to discuss the information a plot displays. 
:::

::: {style="text-align: center; margin-top: 6em"}
## When comparing visual features, remember...
. . . 

### ...plots can only be compared if they contain the same information.^[@Cleveland1984]

::: {.notes}
Identifying gaps in the current visualisation methods isn't the only reason we need a taxonomy, there is a second, more meta, reason it's needed. In order to compare plots, 

- they need to contain the same information. 

Otherwise, we cannot be sure if one plot outperforms the other because of a gap in the information or a difference in the way that information is displayed. Papers that compare plots that have wildly different information are everywhere in the infoviz literature because once you go beyond expressing a single value, what we define as information becomes complicated. A unified language around what information is contained in a plot will allow us to more easily recognise which plots are appropriate to compare.
:::
:::

::: {style="text-align: center; margin-top: 6em"}
# A Taxonomy for Visualisation Information

:::{ .notes}
So, without further ado, lets dive into the taxonomy and quiz questions 4 to 8.
:::

:::

## The Three Elements of the Taxonomy

![](3elements.jpeg){fig-align="center" width=60%}

::: {.notes}
There are three features of a visualisation that we should consider when we want to understand the information it conveys. 

- First, you need to ask yourself what distribution your plot depicts and if it is the best distribution for the questions we want to answer. 
- Second you need to ask what features of that distribution are depicted and if those features convey the information we want. 
- Finally you need to consider the hierarchy we implicitly give to those features, and ask if we are drawing attention to the information we think is important.
:::

::: {style="text-align: center; margin-top: 6em"}
## Questions 5 & 6
### Distribution
::: {.notes}
Questions 5 and 6 will focus on the first aspect of the taxonomy, the distribution. 
:::
:::

## Question 5: Class A vs Class B #1
```{r}
library(countdown)
countdown(minutes = 0, seconds = 20, 
          margin = "0.1em", )
```
:::: {.columns .v-center-container}
::: {.column width="50%"}
::: {style="font-size: 100%;"}
::: {.incremental}
- Two primary school classes independent height distributions 
- Obsessed with the spirit of competition
- Height competition (?)
  - The winning class gets a pizza party
  - Pizza party not relevant
- How often is a student from Class A taller than a student from Class B?

:::
:::
:::
::: {.column width="50%"}
```{r}
#| fig-width: 5

set.seed(1)
mypal <- c("#66c2a5","#74add1")
# Make violin plot data
n <- 100
classA <- rnorm(n, 135, 20)
classB <- rnorm(n, 120, 10)
violin_data <- tibble(height = c(classA, classB), 
                      class =c(rep("Class A", n), rep("Class B", n)))

# Make violin plot
violin_data %>%
  ggplot(aes(x=class, y=height)) + 
  geom_violin(aes(colour=class, fill=class)) +
  labs(y="Height (in cm)")+
  scale_y_continuous(breaks=seq(80,200, 10), limits=c(80,200)) +
  theme_classic() +
  scale_colour_manual(values = mypal) +
  scale_fill_manual(values = mypal) +
  theme(axis.title.x=element_blank(), 
        axis.text.x=element_text(size=10),
        legend.position = "none",
        aspect.ratio=1)  
```
:::
::::

::: {.notes}
- The plot on the right shows two independent distributions of student heights from two primary school classes, class A and B, that are 
- obsessed with competing with each other. 
- Most recently they have started competing on heights. The game works like this, two students are randomly selected from each class every day and the taller student wins a point for their class. They will play this game for 100 days and
- The winning class gets a pizza party
- That isn't relevant to the statistics I'm just trying to paint a beautiful story
- On how many of the days do we expect class A to win?

You have 20 seconds to answer and your time starts now.
:::

::: {style="text-align: center; margin-top: 6em"}
## Solution
### About 75% of the time!
[Results](https://flux.qa/#/presentations/64805896ff94fa54629cd870/6480588eff94fa54629cd86e?tab=polls&poll=64884e3aff94fa54629cf60f){preview-link="true"}

:::{.notes}
The consultants, on average, got this one right  50% of the time. And you guys by comparison got it right...
:::
:::

## The All Knowing Marginal Distribution
![](incorrectdistributions.jpeg){fig-align="center"}

::: {.notes}
- It purposely made the previous question harder to answer than it needed to be
- I provided you with two marginal distributions but asked a question about a binomial distribution that is related to their joint distribution
- We do this strange thing in visualisation where we display marginal distributions of our variables and then expect them to to answer every possible question we could have about our data, regardless of whether or not those distributions are relevant.
- For statisticians to spend so much time working out which distribution describes which phenomena, it's a little sad to ignore it when generating visualisations
- This is a really pervasive problem to, it's not uncommon for papers to compare plots that depict two different distributions and then compare them using a question only one of them can actually answer
- In this case, since the joint distribution actually has the information I am asking about, this question is much easier to answer if I give you the joint distribution
:::

## Question 6: Class A vs Class B #2
```{r}
library(countdown)
countdown(minutes = 0, seconds = 20, 
          margin = "0.1em", )
```
:::: {.columns .v-center-container}
::: {.column width="40%"}
::: {style="font-size: 85%;"}
::: {.incremental}
- Breaking News! Primary school class is full of sore losers
- and also cheats
- Now class B has a bunch of tall high school students :(
- The plot on the right = the joint distribution of the classes + Bernoulli distribution of event A>B
- How often is a student from Class A taller than a student from Class B?

:::
:::
:::
::: {.column width="60%"}
```{r}
#| fig-width: 5
# Add in tall kids
set.seed(1)
classB <- classB[classB > quantile(classB, 0.2)]
classB <- c(classB, rnorm(0.2*n, 180, 4))
# Scatter plot data
scatter_data <- tibble(classA = classA,
                       classB = classB) %>%
  mutate(biggerA = ifelse(classA>classB, "Yes", "No"))

# Scatter plot
scatter_data %>%
  ggplot(aes(x=classB, y=classA)) +
  geom_point(aes(colour=biggerA)) +
  labs(x="Class B Heights (in cm)", 
       y="Class A Heights (in cm)") +
  guides(colour=guide_legend(title="Is A>B?")) +
  scale_x_continuous(breaks=seq(80,200, 10), limits=c(80,200)) +
  scale_y_continuous(breaks=seq(80,200, 10), limits=c(80,200)) +
  theme_classic() +
  theme(aspect.ratio=1) +
  scale_colour_manual(values = mypal, limits=c("Yes", "No"))
```
:::
::::

::: {.notes}
Class B loses the original heights competition and 

- demands a rematch. One of the students in Class B somehow convinces his older brother, and some of his friends, 
- to swap classes with the shortest kids in class B. In a shocking turn of events the school also allows this
- Class B now has several tall students in their class that are skewing the results
- The plot on the right depicts the joint distribution of class A and B with colour highlighting the Bernoulli distribution of interest
- In their rematch height competition on how many of the days (out of 100) do we expect class A to win?

You have 20 seconds to answer the question and your time starts now.
:::

::: {style="text-align: center; margin-top: 6em"}
## Solution
### About 55% of the time!
[Results](https://flux.qa/#/presentations/64805896ff94fa54629cd870/6480588eff94fa54629cd86e?tab=polls&poll=64887e73ff94fa54629cf684){preview-link="true"}

:::{.notes}
- The consultants, on average, got this one right  75% of the time. And you guys by comparison got it right...
- This example highlights the importance of making sure you are depicting the correct distribution
:::
:::

::: {style="text-align: center; margin-top: 6em"}
## Question 7
### Features
::: {.notes}
Now, I was a little dishonest in question 6 because I didn't only change the distribution, I also changed the feature. 
:::
:::


## The Four Visual Features
![](distfeatures.jpeg){fig-align="center"}

:::{.notes}

- The visual aspects of a distribution that we typically express in graphics can be organised into four key features. 
- Each of these features are more adept at answering some questions than others.
- The first feature is mass which represents any of the mass functions like the CDF or PDF. Depictions of mass can inform us of the mode, likely values, and whether or not we have an identifiable distribution.
- Samples are actual or simulated outcomes from our distribution. Our data falls into this category. This feature is useful in answering questions about frequency or probability. 
- Parameters are the statistics that are related to our distribution. The mean and variance are examples of parameters. Plots that show parameters are quite flexible since they can depict almost anything, but usually the scope of each plot is limited.
- The last feature is exchangeability which illustrates whether or not elements from a sequence of random variables in this distribution can be swapped. Rather than wanting to answer specific questions about exchangeability, it is usually just something we want to highlight in our data, typically by connecting elements or having them touch. 
- This list is not exhaustive nor is each element entirely distinct. A particular visualisations of a sample may double as a depiction of mass and also highlight something entirely different
- However, these features do provide a solid basis for assessing the information conveyed by a graphic.
- Now, lets get to our quiz question.
:::
## A Real High School Story 
::: {style="font-size: 85%;"}
When I was in high school we had to do very precise scientific experiments with pretty shoddy equipment, so it wasn't unusual to get results that were all over the place. 
:::

## A Real High School Story 
::: {style="font-size: 85%;"}
When I was in high school we had to do very precise scientific experiments with pretty shoddy equipment, so it wasn't unusual to get results that were all over the place. Additionally we weren't allowed to use "shoddy equipment" to explain why our results went against all scientific wisdom. 
:::

## A Real High School Story 
::: {style="font-size: 85%;"}
When I was in high school we had to do very precise scientific experiments with pretty shoddy equipment, so it wasn't unusual to get results that were all over the place. Additionally we weren't allowed to use "shoddy equipment" to explain why our results went against all scientific wisdom. This meant bad luck with your experimental results could result in a terrible grade. 
:::

## A Real High School Story 
::: {style="font-size: 85%;"}
When I was in high school we had to do very precise scientific experiments with pretty shoddy equipment, so it wasn't unusual to get results that were all over the place. Additionally we weren't allowed to use "shoddy equipment" to explain why our results went against all scientific wisdom. This meant bad luck with your experimental results could result in a terrible grade. In year 11 the school captain, a person holding a position chosen based on moral character, told me she solved this problem by fabricating her results. 
:::

## A Real High School Story 
::: {style="font-size: 85%;"}
When I was in high school we had to do very precise scientific experiments with pretty shoddy equipment, so it wasn't unusual to get results that were all over the place. Additionally we weren't allowed to use "shoddy equipment" to explain why our results went against all scientific wisdom. This meant bad luck with your experimental results could result in a terrible grade. In year 11 the school captain, a person holding a position chosen based on moral character, told me she solved this problem by fabricating her results. I said "that's academic fraud though", and she said "yeah" and that was the end of the conversation. 
:::

## A Real High School Story 
::: {style="font-size: 85%;"}
When I was in high school we had to do very precise scientific experiments with pretty shoddy equipment, so it wasn't unusual to get results that were all over the place. Additionally we weren't allowed to use "shoddy equipment" to explain why our results went against all scientific wisdom. This meant bad luck with your experimental results could result in a terrible grade. In year 11 the school captain, a person holding a position chosen based on moral character, told me she solved this problem by fabricating her results. I said "that's academic fraud though", and she said "yeah" and that was the end of the conversation. 

In this hypothetical scenario you are my school captain and you are fabricating your science results.
:::

## A Real High School Story 
::: {style="font-size: 85%;"}
When I was in high school we had to do very precise scientific experiments with pretty shoddy equipment, so it wasn't unusual to get results that were all over the place. Additionally we weren't allowed to use "shoddy equipment" to explain why our results went against all scientific wisdom. This meant bad luck with your experimental results could result in a terrible grade. In year 11 the school captain, a person holding a position chosen based on moral character, told me she solved this problem by fabricating her results. I said "that's academic fraud though", and she said "yeah" and that was the end of the conversation. 

In this hypothetical scenario you are my school captain and you are fabricating your science results. You want statistically significant results, but you don't want to make it obvious you fabricated them.
:::

## A Real High School Story 
::: {style="font-size: 85%;"}
When I was in high school we had to do very precise scientific experiments with pretty shoddy equipment, so it wasn't unusual to get results that were all over the place. Additionally we weren't allowed to use "shoddy equipment" to explain why our results went against all scientific wisdom. This meant bad luck with your experimental results could result in a terrible grade. In year 11 the school captain, a person holding a position chosen based on moral character, told me she solved this problem by fabricating her results. I said "that's academic fraud though", and she said "yeah" and that was the end of the conversation. 

In this hypothetical scenario you are my school captain and you are fabricating your science results. You want statistically significant results, but you don't want to make it obvious you fabricated them. You only want them to be just statistically significant.
:::

## Question 7: Using Error Bars
```{r}
library(countdown)
countdown(minutes = 0, seconds = 20, 
          margin = "0.1em")
```
In which plot are the two means closest together but still statistically significantly different?
```{r}
#| layout-nrow: 1
#| fig-width: 3
# error bar 1 data
error_data1 <- tibble(ymin = c(175, 300), 
                      ymax = c(425, 500), 
                      themean = c(300, 400), 
                      group = c("Group 1", "Group 2"))
error_data2 <- tibble(ymin = c(175, 360), 
                      ymax = c(425, 560), 
                      themean = c(300, 450), 
                      group = c("Group 1", "Group 2"))
error_data3 <- tibble(ymin = c(175, 425), 
                      ymax = c(425, 625), 
                      themean = c(300, 525), 
                      group = c("Group 1", "Group 2"))
error_data4 <- tibble(ymin = c(175, 500), 
                      ymax = c(425, 700), 
                      themean = c(300, 600), 
                      group = c("Group 1", "Group 2"))
mypal <- c("#d53e4f","#F9D423","#66c2a5","#74add1")
# Make error bar plot
p1 <- error_data1 %>%
  ggplot(aes(x=group)) + 
  geom_errorbar(aes(ymin=ymin, ymax=ymax), width=0.2,
                linewidth = 1.5) +
  labs(y="Fabricated Values",
       title = "Plot A")+
  theme_classic() +
  scale_y_continuous(breaks=seq(150,700, 50),
                     limits=c(150,700)) +
  theme(axis.title.x=element_blank(), 
        axis.text.x=element_text(size=10),
        aspect.ratio=1.5,
        text=element_text(size=15),
        plot.title = element_text(hjust = 0.5))

# print plots
p1 + geom_boxplot(aes(y=themean), colour=mypal[1]) 

pb <- p1 %+% error_data2 + 
  geom_boxplot(aes(y=themean), colour=mypal[2])+
  ggtitle("Plot B")
pb

p1 %+% error_data3 + 
  geom_boxplot(aes(y=themean), colour=mypal[3])+
  ggtitle("Plot C")

p1 %+% error_data4 + 
  geom_boxplot(aes(y=themean), colour=mypal[4])+
  ggtitle("Plot D")
```
::: {style="font-size: 70%;"}
*Assume the error bars depict independent confidence intervals*
:::

## Solution
:::: {.columns}
::: {.column width="50%"}
::: {style="font-size: 100%;"}
::: {.incremental}
- [Results](https://flux.qa/#/presentations/64805896ff94fa54629cd870/6480588eff94fa54629cd86e?tab=polls&poll=64888b82ff94fa54629cf69f){preview-link="true"}
- Overlapping Error bars != Insignificance
- Most people think values are statistically significant until the error bars touch. ^[@Bella2005]
- The p-value of the two-tailed t-test for Plot C (on the previous slide) is about 0.006 ^[@Schenker2001]

:::
:::
:::
::: {.column width="50%"}
```{r}
#| fig-width: 5
pb
```
:::
::::


::: {.notes}
- The correct answer is Plot B. The consultants, on average, got this one right  25% of the time. This question illustrates an issue I call the error bar problem
- While it is true that error bars that do not overlap implies statistical significance, overlapping error bars do not imply the converse. 
- However most people think two means are statistically significantly different right up until the error bars touch.
- In the case of independent confidence intervals, the p-value of the two-tailed t-test for error bars that just touch is about 0.006.

The papers that go on to cite these misunderstandings about error bars discuss the work as though the problems are caused by the error bars themselves. They suggest that error bars should be avoided as a visualisation tool, ignoring the fact that these fundamental misunderstandings in uncertainty will likely follow other visual encodings if we don't understand how to improve it. There are two things we could do to improve this visualisation. We could change the distribution so depicts the distribution of the hypothesis test in question, or we could highlight different features that are more relevant to the probability of concern
:::

## Hypothesis Test Visualisation
```{r}
# Generate Data
distA <- tibble(x = rnorm(100, 30, 10),
                y = rnorm(100, 30, 10))
# CIRCLE PLOT
library(ggforce)
ticks <- seq(0,100, 10)
tibble(x=50,
       y=60) %>%
  ggplot(aes(x,y)) + 
  geom_circle(aes(x0=x, y0=y, r = 20))+
  geom_label(label="Null") +
  geom_point(data=distA, colour=mypal[1]) +
  geom_label(x=30, y=30, label="True", colour=mypal[1]) +
  scale_y_continuous(breaks=seq(0,100, 10), limits=c(0,100)) +
  scale_x_continuous(breaks=seq(0,100, 10), limits=c(0,100)) +
  labs(y = "Random Variable B",
       x = "Random Variable A")+
  theme_classic() +
  theme(aspect.ratio=1)
```

::: {.notes}
- For example, here is a visualisation of a hypothesis test where we are testing if the red distribution is significantly different from the black distribution
- Let's say we want to visualise the probability of a type 2 error.
- The number of points (out of 100) from the red distribution that fall in the black circle show that probability
- to find the type 1 error we would swap the features of the two distributions
- Thinking about the features that depict the information we want to convey can lead to unique and more versatile visualisations
:::

::: {style="text-align: center; margin-top: 6em"}
## Question 8
### Hierarchy
::: {.notes}
Now we can move on to our third taxonomy element and final quiz question
:::
:::


## Question 8: Gut Hierarchy

```{r}
library(countdown)
countdown(minutes = 0, seconds = 20, 
          margin = "0.1em")
```

Order the features **location, estimate, and error** by their importance implied by the map below.

![](glyphmap.jpeg){fig-align="center" width=60%}

::: {.notes}
Your final question is, to order the features in this map by their implied importance. You have 20 seconds starting now.
:::

## Solution
1) Location
2) Estimate
3) Error

[Results](https://flux.qa/#/presentations/64805896ff94fa54629cd870/6480588eff94fa54629cd86e?tab=polls&poll=648059aaff94fa54629cd881){preview-link="true"}

::: {.notes}
The correct ordering is location, then estimate, then error. The consultant, on average, got this one right  50% of the time. And you guys by comparison got it right...
:::

## Elementary Tast Hierarchy
The @Cleveland1984 hierarchy:

  1) Position along a common scale

  2) Positions along non-aligned scales

  3) Length, direction (slope), angle (starting from the same origin)

  4) Area

  5) Volume, curvature

  6) Colour value, colour saturation

::: {.notes}
- The concept of a hierarchy of visual features in a graphic isn't new and has a pretty extensive literature. 
- This is an established hierarchy of basic visual features. It is not exhaustive nor mutually exclusive, but it does provide a useful rule of thumb in understanding the level of importance we give information in a graph. 
- This paper is also quite old, so more modern aesthetics such as animation or glyphs aren't not tested. 
- If we look at the previous map, we can see location is mapped to the position, the  estimate is mapped to the colour value, and the error is mapped to the glyph rotation which does not appear in this hierarchy, but is generally considered to be low on the list.

:::

## Using Heuristics
::: {.incremental}
- Take advantage of heuristics
- Error Mapping ^[@Maceachren2012]
  - Good: fuzziness, location, and colour value
  - OK: arrangement, size and transparency
  - Ugly: saturation, hue, orientation and shape
- Mapping direction
  - e.g more fuzzy = higher error

:::
::: {.notes}
- Not only should we considered the hierarchy in the information we display, but also the heuristics that connect some pieces of information to specific elementary tasks. 
- For example, error is best mapped to
- fuzziness, location, and colour value;
- arrangement, size and transparency are an OK second choice;
- but saturation, hue, orientation and shape are unacceptable and have no intuitive connection to uncertainty. 
- Not only do the graphical elements we map our features to matter, but the direction matters too. 
- for exmaple, graphical elements that are more fuzzy convey a higher uncertainty. 

Therefore we should not only keep the hierarchy of information in mind when we map features of our distribution, but also take advantage of these intuitive mappings when we can. You can't just to put the right features from the right distribution on a graph, you need to allow that information to be efficiently extracted. Including information that is so hard to see that it is functionally invisible is the same as not plotting it at all.
:::

## Pixel Map ^[@lucchesi2021vizumap]

![](pixelmap.jpeg){fig-align="center" width=60%}

::: {.notes}
When you understand the three visual features, you will notice that some newer plots are just plotting a specific feature with a high priority in a distribution where we typically ignore that feature.

This is a pixel map from the Vizumap package which is a visualisation of uncertainty that basically boils down to a choropleth map that swaps the visualisation of a parameter out for a sample. 
:::

## Fin: The Taxonomy of Visual Information
![](3elements.jpeg){fig-align="center" width=60%}

::: {.notes}
So that is it for the taxonomy for information in visualisations, and the end of the quiz and Chapter 1. Congratulations you guys are ___ smarter than a big 4 consultant. 
:::

# Future Work

::: {.notes}
Now, I am going to move on to the more serious section where I discuss future work and its timeline.
:::

## Chapter 2
::: {.incremental}
- Test this framework
- Identify gaps in current methods
- AEMO need high dimensional uncertainty visualisations
- Find lacking areas
- Apply taxonomy to make better visualisations
:::
::: {.notes}
- The goal of chapter 2 will be to test this framework and then apply it to fill gaps in our visualisation tools.
- First we would run some experiments to test the effectiveness of this taxonomy compared to the current ways we think of information in graphics. Once we have some evidence behind the taxonomy, or some version of it, we can use it to design visualisations that fill gaps in the current methods similar to the pixel map.
- My PhD is sponsored by the Australian energy market operator (AEMO) who have a need for effective high dimensional uncertainty visualisations 
- By working with them we will investigate areas where they have found the current visualisation tools to be lacking  
- Then we will apply the taxonomy to try and improve their current methods
:::

## Chapter 3
::: {.incremental}
- Applications of Chapters 1 and 2
- Possible directions
  - Visualisations developed for AEMO
  - Software for visualising uncertainty information in energy data or more broadly
  
:::
::: {.notes}
- Chapter 3 will focus on applying the work established in chapters 1 and 2 to real world outputs. 
- This chapter of the work could go in a number of ways, depending on how the work in chapter 2 pans out. 
- If the research done in chapter 2 is predominantly focused on results in the energy industry, this chapter will be a discussion on the visualisations used by AEMO that were directed by this research, and the real world impacts of those improvements. 
- Another research output could be software that is specific for visualising uncertainty information in energy data, or if the research output is applicable more broadly, tools that makes it easier to implement these principals into the data analysis workflow. 

:::

## Upcomming Key Dates
|Date|Description|
|:-|:---|
|June 2023| Meeting with AEMO team to map out future monthly meetings over the next year and build up applications of uncertainty visualisations|
|June 2023|Confirmation milestone|
|June 2023|Submit poster for IEEE VIS 2023|
|July 2023| Complete chapter 1|
|August 2023| Start initial experiments for Chapter 2|
|December 2023| Attend ASC and present poster on taxonomy|
|May 2024| Progress review milestone |


## Full Timeline
::: {style="font-size: 45%;"}
|Date|Description|
|:-|:---|
|May 2022| Changed topic to uncertainty visualisation with new scholarship funded by AEMO |
|August-November 20232| Took leave from PhD|
|May 2023| Met with Dean Sharafi, Alireza Fereidouni, and Toby Price from the AEMO Systems Design and Transformation team to discuss current issues they are facing in uncertainty visualisation|
|June 2023| Meeting with AEMO team to map out future monthly meetings over the next year and build up applications of uncertainty visualisations|
|June 2023|Confirmation milestone|
|June 2023| Submit poster abstract on aspects of the literature review for IEEE VIS 2023|
|July 2023| Complete chapter 1, a detailed literature review paper that captures the key aspects of the infoviz community and submit to a journal|
|August 2023| Start initial experiments for Chapter 2 of the thesis|
|October 2023| Attend IEEE VIS 2023|
|December 2023| Attend ASC and present poster on taxonomy of uncertainty visualisations with results from preliminary experiments|
|April 2024| Submit paper summarising key findings from experiments |
|May 2024| Progress review milestone |
|September 2024| Attend POSIT::CONF(2024)|
|October 2024| Attend IEEE VIS 2024|
|April 2024| Submit paper for Chapter 3 |
|May 2025| Pre-submission milestone|
|October 2025| Attend IEEE VIS 2025 in Vienna |
|November 2025| Attend UseR! conference to discuss software developed for translation of uncertainty visualisation work (if applicable)|
|December 2025| Submit Thesis |
:::

# End

## Bibliography

```{r}
#| output: false
#| eval: false
library(spelling)
qmd <- "ideas/confirmation_presentation/presentation.qmd"
ignore <- readLines("WORDLIST")
check_spelling <- spell_check_files(
  qmd,
  ignore = ignore,
  lang = "en_GB"
)
if (nrow(check_spelling) > 0) {
  print(check_spelling)
  stop("Check spelling in Qmd files!")
}
```
