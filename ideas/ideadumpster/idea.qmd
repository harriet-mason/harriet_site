---
title: "Idea Dumpster"
editor: source
---

This section is not designed to be read (so please don't read it). Every now and then I have an idea and I don't really have somewhere to put it. It might be good or bad or pointless but I don't want to throw it out. So I just write it down here and might come back to it later, or might throw it out later if it is stupid. 

## Perceptual Experiment thoughts
- Bar chart vs line chart, comparison vs trend (exchangeability) compare plots to their *inexchangeable* alternative?
- Can we generate benchmark tests for abstract plot aesthetics such as animation and interactivity.
- Ideas for goal based perceptual tasks
  - Extract value
  - Which is bigger
  - Correlation (easily found with shape)
  - Odd one out
- Also a question of what it is, you are comparing
  - An estimate
  - Uncertainty
  - Are they even different?
- Can you work out what people see when the see a plot quickly and what they see when they look at it for longer?
  - if the take aways are different
  - if they can be split of shape and stuff
  - give a really short time limit to force someone to do the short glace at the plot and then a longer time limit
- People flipping information time issue vs errors (plots in A B order but question is B>A)
  - For a short task people remember to flip it back so it is just an extra step tha takes longer
  - For longer tasks, people have starting point amnesia and are more likely to make a mistake
    - Like how I forget my ATM card and keep getting scammed
- is it actually bad to map multiple aesthetics to a single variable (e.g. colour and x axis to gdp or something). Does it hurt interpretation in any way?
  - seems to be frowned upon if X is continuous, but encouraged if X is a discrete variable, as if the colour chage highlights meta information about a variable
  - If uncertainty is meta information, its main thing should be to hedge the signal 
    - If I look at a map and a signal is plotted, the VSUP should prevent me from drawing conclusions that are not there.
- Experiment of Uncertainty as meta info
    - I feel like if uncertainty is meta-information it should be applied to the same axis (blur if it is a visual image, position if the varable is mapped to precision, your goal should be to prevent undue signal coming through - think of what actual noise does in a visualisation)
    - I wonder if there are tests that ask users to identify a pattern and perceive of uncertainty as a single joint factor with the signal 
    - These tests would also need to link each visual method to an uncertinty aspect
    - But for a different question (e.g. bands instead of error bars) you would still be able to answer the question. So the motivation is still the dominant factor.

## Random notes that have not had much thought
- I wonder if assumption > sample (outcomes) > PDF > point can be seen as a precision of uncertainty and therefore carry with it a subconscious implication of precision, in the same way we communicate meta information about other variables.
- I wonder how you convey meta-uncertainty in animation
  - Then is uncertainty meta information with meta information? Yikes.
  - subconsciously communicate this with N outcomes 0= very uncertain about uncertainty, infinite = very certain about uncertainty
- As a plot becomes more complicated the perceptual tasks become more difficult to untangle. E.g. the hurricane uncertainty was "technically" mapped correctly but immediately looking at it, I made the same mistake as lay people (partially because I don't know if storms increase in size, and partially because I would assume it is a visualisation to tell me if i need to evacuate or not (my risk) and I dont care where the eye of the storm is, I can where is going to be HIT by the storm)
- I wonder how often academics cite two papers for a claim that have the same root source? It would be especially common if we are alowed to cite literature reviews for stuff
  - I wonder if you can make software that would check for that, like a branching tree of what cited what for which claim.
- It is actually kind of odd how little the distinction between meta information and variable is spoken about.
  - time and space also have this interesting quality of "I can physically be there". We are mapping data to a plan on which we, the reader, exist. Even if we consider exchangeability and continuity, that is something that only time and spatial data technically have. Data is being mapped to the space I live in, I am not trying to imagine myself in the space of the data.
- Error examples: Measurement errors, Sampling error, Events outside the sample, Black Swan Events (pandemic)
- interesting that we give confidence intervals as 95%. Every way we talk about uncertainty is directly related to hypothesis testing which is a very non-intuitive way of thinking about uncertainty. What if we did epistemic and aleatory confidence intervals. This may be a stupid idea when I read it later, not at midnight.
- From the Role of Uncertainty "We can see that trust it is highest when the user is either aware of no uncertainties or mistakenly believes there are no uncertainties". This makes me suspect that people are trusted more when they don't display uncertainty BECAUSE the audience has assumed there is none.  If you don't display uncertainy, I think it is assumed there is none. maybe we could test this by making people aware of uncertainty and not and seeing if their reaction to to the plot when there is no uncertainty displayed mimics their beliefs when they know for certainty there is no uncertainty. Like get them to do a decision making task or something. It makes me think about the cognative test I had to do where uncertainty existed but was not made explicitly clear to me.
- Could be fun to test what testing method provides a better understanding of audience insight? Is that too meta?
- "Open questions generally support visualisation overviews rather than details views" I wonder if the way we ask questions about visualisations changes the way people see them.
- What if I tried to quantify the unknown uncertaintys? Like there are a lot of uncertainties that we dont consider, would it be possible to look at a bunch of historic forecasts made in a social context (so there are lots of external and difficult to measure factors) and assess how often the outcome was in the confidence bands and what defines the cases where they were/weren't.
- Think of how to test assertions vs rebuttals -- Ask people (maybe done) when someone uses uncertainty what do you think of (what does it mean) -- Is there uncertainty in this plot (what is it) if none what would be an addition to the plot to convey it


## Possible Experiment Ideas

### Automatic infintesimally small confidence intervals

-   Idea
    -   Some of the people in Jessica Hullman's paper said they dont visualise uncertainty because everyone knows there is a level of assumed uncertainty in plots.
    -   Jessica then shows a couple plots and points out that this "presumed" level of uncertainty could imply significance or not.
    -   My sister had a book on pregnancy that presented a bar plot counting the average number of drinks a mother had and showed that the trend. I am not sure my sister was even aware there should be uncertainty in the plot. Additionally, the actual *practical significance* between the number of drinks was literally almost nothing, but Eloise still spoke about the difference as clear. I think the text around the plot did not help this (the author spoke about the differences as though they were clear cut).
    -   I also had a conversation with my a friend about why I didn't think eating wheat grass would cure Chrones disease and I said I suspected there was no statistical significance on that, and she said "it didn't matter how small the effect was, if it could help at all it was worth trying".
    -   This makes me feel that in the absence of explicit uncertainty, people assume significance and almost infinitesimal confidence intervals. I feel like this translates naturally from the natural human use of anecdotal evidence.
    -   In general, I don't think we are designed to see a difference between statistical and economic significance and the two are either conflated, or a measured difference in the average is taken as evidence of statistical significance.\
    -   I actually think laypeople might still say value A is better than value B even if they have clearly overlapping confidence intervals.
-   Additional considerations
    -   Jessica's paper also discusses several other ideas that would influence this assumption of significance.
    -   She discusses plot design features such as omitted data orthe scale of the y-axis, that would lead to different assumptions of significance. She gave an example of the GDP of several countries and explains that one country looks to have a particularly strange GDP, however scaling the y-axis so the difference is less noticiable, or including many more countries would probably eliminate this assumption of statistical significance.
    -   I also think the assumption of significance would be HIGHLY related to the viewers prior beliefs on the topic.
    -   James Goldie: uncertainty is confusing to audiences so it is skipped. instead we select cases where the pattern is clear and discuss uncertainty only if it is relevant.
-   Implication
    -   If everyone assumes a trend is highly significant whenever any difference is displayed, not visualising uncertainty becomes highly problematic. Unless your confidence intervals are literally invisible, you are implying a level of uncertainty that is not there.
-   How to test
    -   ...

### Alternative reasons to visualise uncertainty

-   Idea
    -   I find that I almost always visualise uncertainty when I am doing a forecasting model and I suspect it is for a couple reasons
        -   The package I use for time series forecasts has uncertainty displays as the default which means I don't have to think about the appropriate way to visualise the uncertainty
        -   The uncertainty adds something to the plot that isnt uncertainty. It makes it visually very clear where the forecast starts
        -   It also gives an overarching sense of "the further we get into the future, the more uncertain the forecast might be" that I don't think is given in other forecasts (confidence bands around a linear regression don't give the immediate impression that a collection of predictors that are outside the observed data will be more inaccurate)
    -   I also wonder if the secondary nature of uncertainty is implied with the way we present things like model estimates (average first then uncertainty basically always)
-   Implication
    -   If there are reasons outside the "moral" way to visualise uncertainty, we can design software and practices that makes people more likely to visualise uncertainty, which is better practice. Make the best and easiest to understand visualisation the one that includes uncertainty.
-   How to test
    -   ...

### Cognative Overload

-   Idea
    -   some people said they didn't visualise uncertainty because it gives people too much information and creates a cognative overload
    -   I was thinking of testing current uncertainty methods and see how much environmental aspects such as space, importance of decision, time to make decision etc impact how well people can interpret visualisations.
    -   I also wonder if peoples ability to incorperate uncertainty information is impacted by how the uncertainty is presented to them and which methods suffer the worst with changes in the environment.

### Presented with just uncertainty

-   Idea
    -   A lot of the discussions on not visualising uncertainty were centered on uncertainty being secondary to the mean estimate. I wonder what happens to decision making if you present an uncertainty estimate (or one with an implied central value such as a range).
    -   say we ask people to decide if they are going to do something based on a an estimated risk given as an average, given as a range, or given as an average with uncertainty.
    -   I can't think of a case where only the uncertainty with *no* central tendancy is appropriate. You can't exactly say "your bus is coming at some point in time give or take 10mins"

### How much uncertainty can people understand

-   Idea
    -   A lot of literature talks about how there is this mixed information about whether or not people can understand uncertainty.
    -   how much can uncertainty can individuals understand, vs how much can a group understand, I think it would be interesting to disentable the average vs the individual reaction to uncertainty.
    -   I wonder if the visualisations have the same issue as electoral systems. If an electoral system is too simple, highly educated people don't vote because they don't respect the process, if it is too complicated, poorly educated people don't vote because they don't understand it.
    -   There is something with communicating to a group vs individuals because of mass interpretation and then action. If you speak to a large enough number of people, the uncertainy disappears. It is not "you have a 10% chance of getting cancer" and becomes "10 of you will get cancer". Saying "it will happen" vs "it has a 10% chance of happening". This idea is not very well thought out.

### Forecast uncertainty implies path probability instead of pointwise.

-   Idea
    -   The confidence intervals of a forecast are calculated at each time step given the last observation (I think it's been a while since I did it honestly).
    -   Because these intervals are connected with shading, I think people would interpret the confidence interval as being for the entire path, not for each time period.
    -   I think HOPS or bootstrapped plots would do a better job of conveying possible paths.
-   Implication
    -   if a stakeholder is concerned about the likely outcomes of the path as a whole, rather than the forecast at one particular point in time, the visualisation could lead them to the wrong conclusion. The problem with this theory is I can't think of an example where this would be relevant.
-   How to test
    -   ...
