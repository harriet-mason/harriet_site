---
title: "Thesis Title"
author: Harriet Mason
bibliography: references.bib
date: last-modified
format: pdf
---

- Entire report needs to be 10-20 pages

# Motivation/Literature Review
- Explanation of several reasons uncertainty visualisation needs to be investigated:

## Importance of visualising uncertainty
A survey conducted by Dr Jessica Hullman found that majority of visualisation authors surveyed agree that expressing uncertainty is important and should be done more often than it currently is, some even agreed that failing to do so is tantamount to fraud [@Hullman2020a]. Despite this, only a quarter of respondents included uncertainty in 50% or more of their visualisations [@Hullman2020a]. This means people are convinced that visualising uncertainty is important from a moral standpoint, but they have still been able to provide self sufficient reasoning that allows them to avoid doing it. 

While some people may have a lower baseline than the general public, most people get better at understanding uncertainty plots the more they are exposed to them [@Kay2016].
Refusing to express uncertainty because people don't understand them, prevents people from improving in their ability to understand the plots, causing those that may not be able to understand uncertainty to continue to be bad at it.

## Why people dont visualise uncertainty
There is a large amount of literature providing new ways to visualise uncertainty and showing its effectiveness, but much less on why people don't do it. Hullmans study found that the most common reasons authors don't visualise uncertainty despite knowing the importance of it are: not wanting to overwhelm the audience; an inability to calculate the uncertainty; a lack of access to the uncertainty information; and not wanting to make their data seem questionable [@Hullman2020a]. It is hard to read that data visulisation authors have a general inability to calculate or access uncertainty information and remain unconcerned about general incompetence in the field, however this issue does not seem to be completely groundless. A strong definition of uncertainty, a consensus on which uncertainty should be incorporated and which should be ignored, as well as simple methods to visualise it seem to be largely absent in the literature. These gaps are worth discussing at length and form a large part of the motivation behind *Project 1*. Authors failling to visualise uncertainty due to a fear of overwhelming their audience, however, are motivations that can easily be discredited with the current literature.

Not wanting to overwhelm the audience typically links back to a general belief that audiences struggle to understand uncertainty. Rather than asking for a blanket yes/no conclusion on the general publics ability to understand uncertainty, it would be more helpful to focus on the intricacies behind understanding uncertainty. Some research suggests that laypeople cannot understand complicated statistical technicalities (such as the difference between Frequentist and Baysian thinking) [@Hoekstra2014], nor can they reliably translate an error bar plot to an equivalent hypothesis test [@Bella2005], but they can make accurate and efficient decisions factoring the uncertainty into their choices [@Kay2016] [@Fernandes2018]. A study that had account for uncertainty in bus arrival times showed laypeople have ability to make fast and accurate decisions that accounted for uncertainty that was displayed on a small screen [@Kay2016].

This might suggest that visualisation authors are unaware of the research indicating that laypeople are able incorperate uncertainty into their decisisons, or they are using that reasoning because they themselves don't uncerstand their true motivations to avoid visualising uncertainty. For example, at least one interviewee from Hullman's survey claimed that expertise implies that the signal being conveyed is significant, but also said they would omit uncertainty if it obfuscated the message they were trying to convey; while other authors who were capable of calculating and and representing uncertainty well did not do it, and were unable to provide a self-satisfying reason why [@Hullman2020a]. These conflicting motivations devloid of sound logic are acknowledged by Hullman herself in the paper.

> "It is worth noting that many authors seemed confident in stating rationales, as though they perceived them to be truths that do not require examples to demonstrate. It is possible that rationales for omission represent ingrained beliefs more than conclusions authors have drawn from concrete experiences attempting to convey uncertainty" [@Hullman2020a]. 

The gap between what authors desire to visualise uncertainty and the decision makers desire to have uncertainty communicated to them also comes through in the methods we use to express uncertainty. While authors typically prefer to express uncertainty in vague terms and will use reasons such as the one above to justify it, decision makers prefer uncertainty in precise quantitative terms [@Erev_1990], [@Olson_1997]. There does not only seem to be a disconnect between the authors work and the desires of decision makers, but also a second disconnect that exists between the researchers who develop uncertainty visualisations and those that use them.

Many of the reasons people don't visualise uncertainty have a common belief that uncertainty is secondary to estimations. The argument speaks to an unspoken belief of those that work with data, that the uncertainty associated with an estimate (the noise) only exists to hide the estimate itself (the signal). From this view, uncertainty is only seen as additional or hindering information, therefore despite its alleged importance, when simplifying a plot uncertainty the first thing to go. This belief is also reflected in the development of new uncertainty visualisations. 

- Cases where people want/need to visualise uncertainty but methods are lacking (such as spatial/temporal)

## Energy forecasts and importance of unertainty vis for clean energy (motivation)

# Thesis Overview 
- Replace projects with (phase/stage/chapter/problems/questions)
- brief detail of the proposed content of all chapters. 
- description of the theoretical and conceptual framework that underlies the thesis, and procedures to be used in addressing the research questions
- Project 1: the stuff detailled in PhD writings on blog. Basically a literature review of the current state of uncertainty visualisation
- Project 2: Maybe an experiment/ application of ideas tests (AEMO application maybe)
- Project 3: Technical outputs (translation of research) - possibly a package

# Project 1 Details
## The current state perspective on visualising uncertainty
Current research in visualising uncertainty seems to have a focus on designing new options for visualising uncertainty for specific types of data. Typical papers comparing visualisation methods seem to fit into one or both of the follow categories:

1) a paper that suggests a new visualisation method for a specific type of data  
2) a paper that compares two existing methods to idenfiy which is better.  

Generally the goal seems to be to increase the number of options we have in our "visualisation bag" so that we always have a plot at the ready if we want to visualise uncertainty. These new plots also tend to focus on a catch all plot that is "best for making decision" or "best for visualising spatial uncertainty". Data visualisation is commonly utilised as a tool in data exploration, so it is not uncommon for a data analyst to make a plot with only a vague goal and use it to pull out a large number of adjacent observations. For example, a data analyst might make a scatter plot to find out the relationship between two variables, but in doing do also finds out that one variable has discrete outcomes over a range of values. It is common knowledge that any single plot cannot reveal all information about a data set, all plot types obfuscate and uncover different types of information, however this aspect of plotting becomes more apparent in uncertainty visualisation. This difference stems from both the way we conceptualise and calculate uncertainty, as well as they way we think about visualising uncertainty.

This is seen best with an example. The study done by [@Hullman2015] is a great illustration in the importance of understanding your goal when visualising uncertainty. The study presented subjects with a HOPs, error bar and violin plot and asked them to report some numerical properties of the distribution. Participants were asked about the mean of the distribution, the probability of an outcome being above some threshold or between some threshold values. They measures which technique was better with absolute error between the subjects response and the true value. In simple plots there is just one variable but also presented plots with two or three variables. They specified that the error bars are not confidence intervals (they represent the underlying distribution, not the distribution of the mean)
- One variable tasks: what is the average, how often are the measurements above the red dot (visual version), how often will measurements lie between K2 and k3 ppm (text version)
- Two variables tasks: how often is measurement of solute B larger than the measurement of solute A
- They acknowledge that error bars are typically used to express statistical significance but don't ask questions directly related to this. They provide several reasons for this. First of all, they assume that people first make some probability estimate (they find P(A>B)) and then use that to assess significance. 
- Statistical significance is related to sampling distributions which they didn't care about
- Wanted to get a better understanding of peoples ability to assess if A>B, rather than asking a binary question about significance.
- "If there is special interest in knowing whether people make qualitative errors , we can set various qualitative errors such as 0.95 during the analysis phase and count the frequency of errors rather than their average magnitude" or just make an error bar plot.
- Three varaible task: how often is the measurement of solute B larger than both the measurement of solute A and the measurement of solute C?
- They argue that a normal distribution was the most favorable condition for the error bar and violin because it leads to a symmetric plot where the widest point is also the mean (so you acknowledge that violin plots are better at expressing the mode of a distribution???)
- perform worse at estimating mean when variance is high (large distance meant integrating oer large distances and imprecision with finite number of frames is higher when variance is larger) and worse at estimating the cumulative density when the variance was low.
- violin plots had a higher error when estimating the probability of a value above a threshold
- Error bars had higher error when estimating the probability of values between two thresholds
- I argue that the problem is not that people are incorrectly reading error bars, but that error bars are being incorrectly used.
- There is also some evidence in their results that a question so removed from the purpose of the plot just confuses people. "In light of the common usage of error bars for presenting multiple independent distributions, it is noteworthy how poorly subjects using these representations performed on tasks asking them to assess how reliably a draw from one variable was larger than the other", Many subjects reported values less than 50%, which are not plausible given that the mean of B was larger than the mean of A." Did not find any obvious patterns of errors. "We speculate that many subjects simply had no idea how to make a good guess"
Frequency based questions are easily answered with a HOPs plot. I suspect participants would have a much harder time using a HOPs plot to identify a range of statistically significant outcomes, or which outcome is the most likely with a non-normal distribution.




U the literature itself seems to be split down the middle on whether uncertainty is a secondary feature of our data, or its own object deserving of its own visualisation methods. It is wildly confusing to read papers on the topic because this clear difference in ideology is neither acknowledge or discussed. If uncertainty is merely a secondary aspect of the plot, the way we visualise uncertainty depends on the data we have. Temporal uncertainty, spatial uncertainty, normal (?) uncertainty should all have different methods specific to that type of data. If uncertainty is its own thing, we should consider the best way to visualse the specific distribution we are concerned with (and incorperate it into a visualisation of the data if the context is relevant). This distinction may seem pointless at a surface level, but when you actually consider how we estimate uncertainty and why we visualise it the distinction becomes important.

Lets first consider the scenario where uncertainty is specific to the type of data, basically the way we currently think about visualising uncertainty. In this context, what is temporal uncertainty? Something as abstract as "uncertainty related to time data" cannot be quantified or drawn. If I give you a data set and ask you to plot the "temporal uncertainty: with no other information you will be lost. So what is temporal uncertainty? Is it measurable? Is it something you can plot? You may try and describe to me what temporal uncertainty is, but you will likely end up describing uncertainty related to a forecast, or variance of a variable, or even model residuals. These are not "temporal uncertainty", they are the distribution of a prediction, a variable, or of a model. None of these things are specific to temporal data. This may just sound like me being pedantic about poor naming, but this incorrect framing of uncertainty leads to very inconsistent research.

Because we view uncertainty as a secondary feature of the data, and not as something we have constructed, we don't think about uncertainty visualisations with the "goal of the uncertainty" in mind. The problem is, we had a goal in mind when we estimated the uncertainty. The uncertainty related to whether or not two variables are significantly different from each other is different to the uncertainty related to whether or not each variable is significantly different from 0. Unlike making plots of data, where we can make a single plot and use it to get information on multiple hypothesis, in order to plot uncertainty we need to already have a hypothesis in mind and a plot made for one question cannot be used to answer another question. So not only is uncertainty not specific to the data, its not even specific to the random variable. Uncertainty is specific to some hypothesis (in an abstract sense, you don't literally need a hypothesis). If you make a forecast and want to know the confidence intervals associated with each point prediction, that would have a different confidence interval to the path of a 5 step ahead forecast. So not only does it not make sense to think about "temporal uncertainty" the concept of even visualising the uncertainty of some random variable without knowing what you want to know, is futile. If you ignore this clear distinction, you are going to misuse uncertainty techniques and beleive they are interchangable for every hypothesis, and thus draw incorrect conclusions. If you have ever used an error bar for a hypothesis that is not idenfiying a statistically significant range of values (using it to identify if two variables are statistically different from each other is also incorrect) then you are guilty of the exact issue I am talking about. 

While you may be sitting here, thinking "this is obvious and pointless to point out" I suspect you are not as aware of how much you interchangably use different uncertainty methods. As a matter of fact, I can't think of a single unit in my undergraduate degree that didn't make this mistake at least once. Therefore it is unsurprising that every paper on uncertainty visualisation research seems to ignore this issue entirely. Not only do they ignore this, but many are written with the perspective of trying to find a "best" or "universal" uncertainty method. Ignoring the hypothesis that generated the uncertainty then leads to the uncertainty questions asked in every paper being different. At no point is it considered if an uncertainty visualisation should depict statistical significance or significant difference between variables. If the plot should tell us which outcome is more probable or if I should be able to make accurate predictions. Maybe it should just give an idea of the distribution of each variable. This leads to the obvious question, which of these questions should the "best" uncertainty visualisation allow me to answer? There is an unspoken assumption prevailing in uncertainty visualisation that the uncertainty distribution associated with one hypothesis can just be swapped out for another. An assumption that the literature itself seems to disprove since there are many papers that are "contradictory" if you view them through the lense of finding a best plot, but strangely consistent when you reframe the conclusions through the lense of the "best plot for this hypothesis". Unfortunately this evidence is almost always used to highlight a failing in the plot, rather than a failing in the question.

My solution to this problem is to design uncertainty visualisation rules that are irrelevant to the data, but instead focus on the question being asked. I would review the research in uncertainty visualisation and reframe the conclusions with the goal of finding the best uncertainty method for each question (rather than for the type of data). Similar to how we have simple rules for effective data visualisation (continuous + continuous = scatter plot, discrete + continuous = box plot, etc) I would like to define rules for uncertainty visualisation (variable significance = error bar, probability comparison = HOPS plot, etc). Using this framework, the concept of designing a method for visualising temporal uncertainty becomes pointless.

Additionally (for possible future work) this process would change the way we view problems such as visualising spatial uncertainty. The question of how to constuct uncertainty plots for complicated data types (such as spatial data) is no longer driven by desiging an abstract uncertainty visualisation method for spatial data. The goal would be to find the best way to combine the most appropriate uncertainty visualisation for the relevant question and the best plot for the type of data. These methods may be able to be combined onto a single plot, however there may be cases where two different plots (one for the data and one for the uncertainty) is more appropriate and effective. Under this

This is a long winded overview of the general motivation for the change in perspective in visualising uncertainty. The five posts below go into more detail providing citations and evidence for the suggestions I make in the overview above. Directly below is a short summary of what each of the five parts cover. 

This section discusses why, despite many viable techniques for visualizing uncertainty and a moral consensus that it is the correct thing to do, most authors still ignore uncertainty in their visualizations. A large problem seems to be a a general inability to quantify and articulate what exactly uncertainty is and a view that it is of secondary imporance to estimates.

## Definition of uncertainty 
- (not sure if the taxonomy should go here)

## Rethinking uncertainty visualisation
What would be ideal is a general application of methods to visualise uncertainty that does not require complete knowledge of all plots that exist. This framework would guide authors towards effective visualisations and allow them to build up knowledge of underlying statistical ideas.
- rethinking uncertainty visualisation in terms of the motivation of the plot (the research question) instead of "best" uncertainty visualisation.
- Show that we can find a best distribution visualisation and reintroduce context to get a good uncertainty plot for that question and context.
- Classification of current plots and research questions that have been shown to be effective for those types of plots
- Reintroduce context

# Timeline
- A timetable for completing the thesis and a statement of progress to date
- ASC later in year (submitted an abstract for a poster)
- IEEE VIS (June 22 2023 poster abstract) - Conference October 2023 
- Aim for something international in a year (or in last year)
- Also could do UseR! & talk about some of the software developed (technical output)
- Could have an energy specific one (various applied energy)
- IEEE VIS (march 31st paper due in 2025) - Conference October 2025 (Vienna)
- Thesis due December 2025

# Bibliography

```{r, include=FALSE, eval=FALSE}
library(spelling)
qmd <- "index.qmd"
ignore <- readLines("WORDLIST")
check_spelling <- spell_check_files(
  qmd,
  ignore = ignore,
  lang = "en_GB"
)
if (nrow(check_spelling) > 0) {
  print(check_spelling)
  stop("Check spelling in Qmd files!")
}
```