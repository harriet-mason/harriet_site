---
title: "Plotting Apples, Oranges, and Distributions"
subtitle: "An alternative taxonomy to prevent information inequality in uncertainty visualisations"
author: Harriet Mason
abstract: "Supervisors: Di Cook, Sarah Goodwin, Emi Tanaka, and Ursula Laa"
bibliography: references.bib
date: last-modified
format: pdf
editor_options: 
  chunk_output_type: console
---

```{r}
#| echo: false
#| warning: false
#| message: false

# Load libraries
library(tidyverse)
library(tsibbledata)
library(Vizumap)
library(fable)
```

# The structure of this report
This report starts with a literature review that discusses the importance of uncertainty visualisations, however, uncertainty visualisations alone are not the focus of this report. Chapter 1 is uses the motivation as a jumping off point, but exists as its own literature review that identifies a problem in the way we think about data visualisation and provides a taxonomy that allows us to rectify these issues. The thesis overview explains where this taxonomy fits in with the larger picture of my PhD, and the timeline maps out this big picture with several milestones. The report finishes with a comment on additional work I have completed over the past year that are not directly related to my thesis.

# Motivation
Think back to the last time you made some sort of data visualisation. What was the purpose of that visualisation? Was it to better understand your data? Was it to help you make a decision? Was it to to communicate that decision to someone else? Now think about the last time you expressed some form of uncertainty. Was it a set of numerical confidence intervals? Maybe as a set of values in a table. Did you consider visualising your uncertainty instead? There are many stages in our analysis that benefit from the power of data visualisation, however this does not mean it is always done with success. Visualization is an important step in exploratory data analysis and it is often utilised to **learn** what is important about a data set. The importance of data driven discovery is highlighted by data sets such as Anscombe’s quartet [@anscombe] or the Datasaurus Dozen [@datasaurpkg]. Each of the pairwise plots in these data sets have the same summary statistics but strikingly different information when visualised. Anscombe quartet is shown in @fig-anscombe, because explaining the data is never the same as seeing it. Instead of having to repeatedly check endless hypothesis to find interesting numerical features, visualisations **tell** us what is important about our data. This powerful aspect of data visualisation is poorly or seldom used in later stages when we are communicating our findings, specifically with respect to uncertainty. 

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: fig-anscombe
#| fig-cap: "The four scatter plots that make up the Anscombe quartet. The four scatter plots are visually distinct but have the same mean, standard deviation, and correlation. The visualisation highlights the importants of plotting your data to identify underlying structure."
tibble(x = c(anscombe$x1, anscombe$x2,
             anscombe$x3, anscombe$x4),
       y = c(anscombe$y1, anscombe$y2,
             anscombe$y3, anscombe$y4),
       Plot = c(rep("Plot 1",11), rep("Plot 2",11), 
                rep("Plot 3",11), rep("Plot 4",11))) %>%
  ggplot(aes(x,y)) +
  geom_point(aes(fill=Plot), colour="black", 
             size=3, pch=21, alpha=0.75) +
  facet_wrap(~Plot) +
  theme_classic() +
  theme(aspect.ratio = 1,
        legend.position = "none") +
  scale_fill_brewer(type = "qual", palette = 4)
```

Visualisation can give people a more complete understanding of risks that numerical summaries alone. Graphics can give us a more complete view of the data. Even something as simple as sketching a distribution before recalling statistics or making predictions can greatly increase the accuracy of those measures [@Hullman2018; @Goldstein2014]. While there is some evidence that confidence intervals provided in text form only are less likely to be misinterpreted than graphics [@Savelli2013], text is insufficient to express more complicated aspects of a distribution, such as mass. Expressing uncertainty verbally decreases the perceived reliability and trustworthiness of the source [@VanderBles2020]. Any confusion caused by expressing uncertainty as a visualisation could also be due to a lack of expose, since @Kay2016 found people repeatedly exposed to the same uncertainty visualisations quickly get better at making judgements. Additionally, visualisation allow for interactive graphics that provide a more in depth understanding of probability [@Potter2009a; @Ancker2009] and infographics that make uncertainty more accessible for people with poor numeracy skills [@Ancker2009]. 

Despite these benefits, there is evidence that we don't visualise uncertainty as often as we should. A survey conducted by @Hullman2020a found that majority of visualisation authors agreed that expressing uncertainty is important and should be done more often than it currently is, some even agreed that failing to do so is tantamount to fraud. Despite this, only a quarter of respondents included uncertainty in 50% or more of their visualisations [@Hullman2020a]. Meaning participants were convinced that visualising uncertainty is morally important but were able to provide self sufficient reasoning that allows them to avoid doing it. Some economists suggest that visualisation authors are responding to incentives that make it tempting to avoid visualising uncertainty, even if those incentives are based more in perception than reality [@Manski2020]. The study by @Hullman2020a found that the most common reasons authors don't visualise uncertainty despite knowing it's moral importance are: not wanting to overwhelm the audience; an inability to calculate the uncertainty; a lack of access to the uncertainty information; and not wanting to make their data seem questionable [@Hullman2020a].

If decision markers are not presented with the uncertainty about an estimate the data analysts have, for all intents and purposes, made the decision for the decision maker. Upon further interviews @Hullman2020a found that authors believed uncertainty would overwhelm the audience and make their data seem questionable because decision makers are unable to understand uncertainty. This belief, while pervasive, is not true. There is some research that suggests laypeople cannot understand complicated concepts in statistical thinking (such as trick questions on hypothesis tests or the difference between Frequentist and Bayesian thinking) [@Hoekstra2014; @Bella2005] but there is a large amount of research suggesting that presenting uncertainty information improves decision making, both experimentally [@Joslyn2012; @Savelli2013; @Kay2016; @Fernandes2018] and in practice [@Al-Kassab2014]. As a matter of fact, doing what many authors currently do (providing only a deterministic outcome with no uncertainty) causes decision makers to be *less* decisive and have completely unbounded expectations on an outcome [@Savelli2013]. This reality cannot be avoided by providing secondary or non-specific information such as explaining calculations [@Joslyn2012], explaining the advantages of a recommendation [@Joslyn2012], or expressing uncertainty in vague terms [@Erev_1990; @Olson_1997], all of which are undesirable for decision makers and lead to measurably worse decisions [@Joslyn2012; @Erev_1990; @Olson_1997]. One of the most popular depictions of uncertainty for decision making is a quantile dotplot, shown in @fig-quantdot.

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: fig-quantdot
#| fig-cap: "This plot depicts an example of a a quantile dotplot that expresses the uncertainty associated with a daily maximum temperature. The probability associated with each temperature is expressed with discrete countable bins and the predicted temperature is experessed with a line. Discretised depictions such as this, make decision making in the face of uncertainty easier for the viewer."
set.seed(1)
dotplot_data <- tibble(temp = 10 + round(rnorm(40, mean=10, sd=2))) 
mid <- round(mean(dotplot_data$temp), 2)
dotplot_data %>%
  ggplot(aes(x=temp)) +
  geom_dotplot(binwidth = 0.75, fill="grey") +
  theme_classic() +
  geom_segment(x=round(mid), xend=round(mid), 
               y=-1, yend=0.7, size=2) +
  geom_label(x=round(mid), y=0.7, label=paste0(mid, "°C"),
             fill = "black", fontface = "bold",
             colour="white") +
  scale_x_continuous(breaks = seq(10, 30, 5), 
                     limits = c(10, 30)) +
  theme(axis.line.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.title.y = element_blank(),
        axis.text.y = element_blank()) +
  labs(x = "Tomorrow's Daily Max Temperature")
```

Not only does communicating uncertainty improve decisions but the mistrust created by communicating certainty in uncertain situations can be exploited. A 6-month survey of anti-mask groups on Facebook during the COVID-19 pandemic showed that the anti-maskers thought carefully about their grammar of graphics and made persuasive visualisations using the same data as pro-mask groups. They did this by exploiting information ignored by the pro-maskers [@Lee2021]. It is understood that deceptive plots can lead viewers to come to incorrect conclusions or significantly overstate effects or risks [@Pandey2015; @Padilla2022] but these incorrect takeaways cannot be mitigated with instructions in how to correctly understand the plot [@Boone2018]. This evidence indicates we are more likely than not to hurt our message when we ignore uncertainty information and trying to raise the general public's plot literacy is an insufficient strategy to curb conspiracy theories and misguided scientific communication.  In direct contrast to this, displaying numerical estimates of uncertainty information has shown to lead to greater trust in predictions [@Joslyn2012; @VanderBles2020]. While @Han2009 found people have more worry when presented with uncertainty regarding health outcomes, this worry is not a bad thing if the concern is warranted given the ambiguous situation.

The disconnect between the research supporting uncertainty and the consensus against may not be entirely driven by a lack of understanding of the literature. For example, at least one interviewee from the study by @Hullman2020a claimed that expertise implies that the signal being conveyed is significant, but also said they would omit uncertainty if it obfuscated the message they were trying to convey.  Other authors who were capable of calculating and and representing uncertainty well did not do it, and were unable to provide a self-satisfying reason why [@Hullman2020a]. These conflicting motivations are acknowledged in the paper itself where @Hullman2020a says:

> "It is worth noting that many authors seemed confident in stating rationales, as though they perceived them to be truths that do not require examples to demonstrate. It is possible that rationales for omission represent ingrained beliefs more than conclusions authors have drawn from concrete experiences attempting to convey uncertainty". 

An overwhelming consensus among visualisation authors seems to be that uncertainty is secondary to estimations. There is a belief help by those that work with data that the uncertainty associated with an estimate (the noise) only exists to hide the estimate itself (the signal). From this view, uncertainty is only seen as additional or hindering information, therefore despite its alleged importance, when simplifying a plot uncertainty the first thing to go. This belief is also reflected in the development of new uncertainty visualisations. Often when trying to visualise multi-dimensional data, uncertainty is relegated to unimportant aesthetics in the plot, often of lower importance than the estimate where it is easily ignored [@Correll2018; @Lucchesi2017]. Cases where uncertainty is not relegated to an undesirable aesthetic instead incorporate interactivity to allow users to explore the complicated space themselves [@Potter2009; @Potter2009a]. Even the literature about uncertainty communication expresses an implicit belief that it is of secondary importance to the estimates or context of the data.

This issues surrounding uncertainty visualisation is not helped by the fact that the term "uncertainty" lacks a commonly accepted definition in the literature. @Lipshitz1997 even commented that “there are almost as many definitions of uncertainty as there are treatments of the subject” . This mishmash of terminology leads to a large body of work, all claiming to finding the best visualisation or expression of of "uncertainty" but most don't even seem to agree on what uncertainty is. The most encompassing definition of uncertainty I have seen comes from @utypo who define uncertainty as **"any deviation from the unachievable ideal of completely deterministic knowledge of the relevant system"**. I am going to use uncertainty to refer to this broad concept that encapsulates many broad concepts such as probability mass distributions (PMF), estimate error, and any data that is not a set of deterministic outcomes.

More commonly uncertainty is defined using a taxonomy rather than a strict definition. There are a few taxonomies for uncertainty, but, just like the definition, most of them are a subset of the one laid out by @utypo. To complete the definition I will include that definition here. @fig-taxonomy is an illustration of the taxonomy presented by @utypo. In this taxonomy, there are three things we need to consider for each "uncertainty" we encounter through the modelling process. First, we need to consider the source of the uncertainty. Is this uncertainty coming from inaccurate measurements or a poorly defined model? This is the *location* of the uncertainty. Second, consider how well you can quantify this uncertainty. Do you know exactly how much measurement error there is in each observation or are you not even aware if there is or isn't measurement error? This is the *level* of your uncertainty, and it ranges from discrete to total ignorance. Finally, consider how this uncertainty came into existence. Is it a result of a naturally random process (epistemic) or is it due to imperfect information and could be improved (aleatory). This is the *nature* of your uncertainty. @utypo then go on to describe mapping our uncertainty in a 3D space that is defined by its location, level, and nature, but I think the taxonomy is more easily understood as a series of questions we need to consider when trying to quantify uncertainty for our audience.

![Depicts an ilustration of the taxonomy described in @utypo. From right to left the drawing shows the location, level and nature of uncertainty with examples of that category underneath. A specific source of uncertainty from the location can be mapped to a specific level. The ability for the uncertainty to increase or decrease (i.e. moving up or down the green line) depends on the nature of the uncertainty.](taxonomyvis.jpeg){#fig-taxonomy}

While information about the sources of our uncertainty and the type of uncertainty may seem like an unimportant secondary step in uncertainty visualisation, communicating these features of uncertainty helps decision makers make more informed choices. @Padilla2021 found that low forecaster confidence or high model uncertainty both contribute to more conservative judgements by decision makers. Failing to communicate the nature of your uncertainty can result in underestimation or overestimation of failure probabilities [@Kiureghian2009]. Additionally @Gustafson2019 found that the framing of our uncertainty, (i.e. informing the reading if the uncertainty came from a lack of knowledge, approximations, unknown unknowns, or disagreement among parties) does not have a significant effect on the belief in the estimates, perceived credibility, or behavioural intentions of the decision makers. This means communicating secondary information about your uncertainty can provide additional benefits to decision makers.

The use of uncertainty in high dimensional environments is especially important in energy data. In a meeting with people in the Systems Design and Transformation team at the Australian Energy Market Operator (AEMO), Dean Sharafi explained the difficulties caused by poor uncertainty communication. Large models that incorporate spatial-temporal data from many sources and systems are used to predict energy uses in the short and long term. Understanding how to improve and make better decisions in these models is imperative in both the daily operation of the energy sector as well as the transition from fossil fuels to clean energy. The energy sector needs better heuristics to make energy supply analysis less costly to conduct [@Stenclik2021], therefore it is an incredibly relevant application of uncertainty visualisation techniques. AEMO has been working with me and funding my PhD scholarship because there is a need in the energy sector to improve uncertainty visualisation techniques. Therefore we will be able to see an immediate use to the ideas discussed in this report and get feedback from people using uncertainty visualisation to make important and impactful decisions.'

{{< pagebreak >}}

# Thesis Overview 
The overarching theme of my thesis is a change in the way we understand uncertainty visualisations, specifically in the case of communication. This work will be divided into three chapters.

## Chapter 1: A new theoretical framework
Chapter 1 discuss the current state of the research surrounding uncertainty visualisation and describes a new taxonomy that can be used to better understand these mistakes. A very prevalent mistake in uncertainty research is comparing two distributions that contain different pieces of information. The issue with this method is that it makes it difficult to untangle whether the better performing plot did so due to a difference in information or a difference in presentation. Frequently these information differences are mistakenly reported as visualisation issues in the literature. These conclusions ignore the way we assess the information contained in a plot and disconnects visualisation from the rest of the statistics workflow which pays more attention to the specific information required to draw a conclusion. 

The taxonomy organises the information contained in a plot by relating the visual features to statistical concepts. The first step in assessing the information conveyed by a visualisation is to ask what distribution it displays. The second step is identifying which features of that distribution it displays. The final step is identfying which of the features are depicted with high priority in the plot vs low priority. This framework allows us to not only understand the information provided in a plot, but it also allows us to adjust plots to make sure they convey a sufficient level of information to answer a question. In discussing the hierarchy, I also touch on a belief that uncertainty visualisation will improve while uncertainty is seen as of low importance relative to other features of the data. Finally, I suggest that there is no overarching best uncertainty visualisation, but rather uncertainty visualisations should depend on a motivating question. 

This in depth literature review and suggested taxonomy form the first chapter of my thesis and set up the theoretical framework for the rest of the work.

## Chapter 2: Applications of the framework
The goal of chapter 2 is to test  this framework through experiments and investigate areas where new visualisation are needed through discussions with our partner team at AEMO. 

The first part of chapter 2 is to work closely with the AEMO team to understand gaps in the current uncertainty visualisation methods. We first met with the AEMO team in May and plan to have monthly meetings to discuss the uncertainty visualisation research. Our partners at AEMO will be able to provide detailed and open ended descriptions of the benefits and difficulties faced by improvements to their uncertainty visualisation techniques, which will allow us to direct the research in chapter 2 towards something with an immediate benefit in industry. 

The second part of chapter 2 is to test the effectiveness of this taxonomy compared to the current ways we think of information in graphics using a series of experiments. The purpose of data visualisation is insight, but due to time limits or other constraints, most visualisation studies use multiple benchmark tests as a substitute for measuring the complicated phenomena of insight [@North2006]. Unfortunately the validity of these results hinge on the large insights we gain from graphics being the sum total of these small incremental insights which may not always be true [@North2006]. Specifically in uncertainty visualisation, there is a focus on performance and accuracy based measures that assume more predictive behaviour from people than what research on human decision making suggests [@Hullman2019]. The experiments in chapter 2 should work to avoid these pitfalls (as well as the information pitfall highlighted in chapter 1) by focusing on large structural identification tasks rather than small accuracy tasks. Alternatively we could ask a large range of questions, with a large range of underlying distributions, and trying to match up the plots to their ideal question or case scenario. The work in chapter 2 will also be guided by discussions with the AEMO team which will allow us to get more open ended feedback on the uncertainty visualisations. This allow us to directly see the improvements (or lack thereof) in insight due to the suggested framework.

## Chapter 3: Translation
Chapter 3 will focus on applying the work established in chapter 1 and 2 to real world outputs. The goal of this work will be to make this research more accessible and allow others to easily implement it.

This chapter of the work could go in a number of ways, depending on how the work in chapter 2 pans out. If the research done in chapter 2 is predominantly focused on results in the energy industry, this chapter will be a discussion on the visualisations used by AEMO that were directed by this research, and the real world impacts of those improvements. Another research output could be software that is specific for visualising uncertainty information in energy data.

Alternatively, if the research output is applicable more broadly, we could introduce software or tools that makes it easier to implement these principals into the data analysis workflow. 

{{< pagebreak >}}

# Chapter 1: a taxonomy of visual information
This chapter starts with a discussion on the current landscape of uncertatiny visualisation and explains the need for a taxonomy. This taxonomy give us a language to discuss the information depicted in a visualisation by asking three simple questions.
1) Which distributions are depicted in this graphic?
2) Which features of those distributions are being visualised?
3) What is the implied order of importance of those features?
After the motivation for the taxonomy is established, I will explain each of these questions in detail. The discussion of each part of the taxonomy (the distribution, features, and hierarchy) starts with an illustrating example and then finishes with an explanation of the general theory.

## The current landscape of uncertainty visualisation and the need for a visualisation taxonomy
Research in uncertainty visualisation seems to have a considerable problem with poor definitions which results in research across the field that is largely incomparable. Part of this may come from the fact that uncertainty itself has a history of being poorly defined, but part of it also comes from the entire existence of the sub-field. Uncertainty visualisations are hard to define because, in the field of statistics, all visualisation is uncertainty visualisation.

You may wonder what the point of this alternative taxonomy is considering there many other visualisation taxonomies, some may even feel more natural than the one I present here. @Grewal2021 presented a taxonomy that categorized uncertainty visualisations based on discreteness of the distribution (a sub-feature of mass) and the domain expertise. @Hofmann2012 classified uncertainty distributions by the number of data points required to construct the graphic. @wilke2019fundamentals organised all graphics, into the 7 categories: amounts, distributions, proportions, x-y relationships, geospatial data, and uncertainty. There are countless other categorisations or taxonomies that organises visualisations into alternative groups to the ones presented here. The reason I suggest this taxonomy despite the alternatives is because none of these methods assess the **information** a visualisation **conveys**. The information expressed by a plot is not decided by how complicated it is, or how many data points were used to make it, or the type of data it conveys, it comes from the distributional features displayed and how those features are depicted in the visualisation.

Almost every estimate or piece of data is assumed to come from some random process or distribution, having a visualisation category explicitly for "uncertainty" seems antithetical to the field. Despite this, graphic are categorised according to rules that would seem almost imaginary from a statistical point of view. This results in a *large* number of visualisations that have different names but are functionally identical in what they communicate. A parallel co-ordinate plot, a line plot, a slope graph, and a parallel sets plot are all the same when you break them down into their components. These visualisation are only seen as distinct because of the data focused way we view visualisation. This is not a concrete basis for a visualisation framework considering the ways in which we adjust data using the tidy data framework make it obvious that the distinction between variable and observation is flexible depending on how we want to use the data. 

Multiple visualisations that are functionally the same is not the only issue with the current way we categorise visualisations. Visualisations that contain fundamentally different information are often compared in experiments due to belonging to the same "category". This distinction deciding what visualisations should be compared is not only arbitrarily followed, it is also frequently ignored. Graphics depicting mass (such as density plots) are compared to visualisations of significant values (such as an error bar plot) because they are both expressions of "uncertainty", but they are also compared to a visualisation of a sample (a hypothetical outcome plots (HOPs)) which is actually just an animated strip chart, which itself is just a scatter plot with a little noise. This is all performed through the lens of calling the animated scatter plot a *new* uncertainty visualisation which allows it to be compared to plots that contain distinctly different information [@Hullman2015]. If we are trying to test the benefits of using one plot over another, and the differences between the information conveyed by the two plots are **numerous**, then we are no closer to understanding better visual encoding of information. It is not a new idea that two plots need to convey the same information in order for the visual features to be compared [@Cleveland1984], but it seems what we define as "the same information" has been largely driven by the "category" the visualisation belongs to, rather than the actual information it contains.

The final problem with the visualisation landscape is that if the field starts considering the information displayed in a visualisation the experimental method currently used to assess graphics will fall apart. The most common way visualisations are compared is by using a series of small benchmark tests that identify the accuracy with which statistics can be identified from a plot [@Hullman2019; @North2006]. This results in visualisation experiments that don't visualise the parameter of highest interest (for example by plotting the mean and then asking what the mean is) but they instead plot a series of visualisations that have "adjacent" information and compare those. However, due to the largely absent understanding of the information contained in a graphic, the information in one plot will typically be more relevant to the question than the information in another plot, and the conclusion of the paper becomes self evident. If experimenters wanted they could ensure they compare two plots that have the same information but ask accuracy measures on a statistic that neither or both plots contain, but this is a rather pointless exercise when you consider how visualisations are used in practice. If both visualisations contain the parameter of interest and we are just reading information off a chart, a table would a better method of communicating the information [@Cleveland1984]. If neither visualisation contains the parameter of interest, the experiment is mimicking a scenario that would never occur in reality. If a visualisation author wanted their audience to know what the mean of their data was, they would include the mean in their plot.

This leaves the future of visualisation experiments a few options moving forward. The field could take a more "exploratory" approach to experimentation and ask multiple accuracy questions for multiple visualisations with different visual encodings and information, and try to identify the *trade-off* in the information a particular plot conveys. Data visualisation is commonly utilised as a tool in data exploration, so it is not uncommon for a data analyst to make a plot with only a vague goal and pull out a large number of adjacent observations. This experimental framework could replicate this process. Alternatively visualisation research could shift away from the accuracy concept all together and either allow for open ended responses, or ask questions that are more abstract to try and better replicate the large structural understanding that comes with the insights gained from visualisation. Some experiments have tried to ask questions about the general structure before [@Hofmann2012], but the question did not have a clear interpretation and assigning an accuracy score in these cases led to confusing results. Required changes to the experimental design of visualisation comparisons is, however, not the focus of this report. Instead parts 1, 2, and 3 of this chapter discuss a new taxonomy in assessing the information in a graphical display and uses this taxonomy to present alternative encodings of visual information.

This taxonomy is it provides a simple set of questions any visualisation author can ask themselves to identify if their plot conveys the message they are trying to communicate. Every plot conveys information and if you want your audience to answer a question about the plot, it needs to convey a *sufficient* level of relevant information for the audience to draw their conclusions. This taxonomy allows us to do that.

## Part 1: Distribution
### Example: Comparing HOPs, error bars, and violin plots
The first, and most important element of our taxonomy is correctly identifying the relevant distribution. Every visualisation is depicting *some* underlying distribution, and if that distribution is irrelevant to your motivating question the graphic will be a poor expression of the information you are interested in. @Hullman2015 is a great illustration in the importance of understanding the distribution depicted by different graphics. It is important to keep in mind that while I am focusing on a single paper to illustrate a point, this is a standard issue in the visualisation literature. 

The study by @Hullman2015 asked participants to provide some numerical properties of a distribution using a hypothetical outcome plot (HOPs), an error bar plot or a violin plot. When shown a single distribution participants were asked about the mean of the distribution, the probability of an outcome being above some threshold (indicated on the plot with a red dot), and the probability of an outcome being between two given values. When shown two distributions  the participants were asked "How often is measurement of solute B larger than the measurement of solute A?", and when shown three distributions they were asked "How often is measurement of solute B larger than the measurement of solute A and solute C?". @fig-examples shows an example error bar plot, violin plot, and some example frames from a HOPs for the two distribution case. There was also a high and low variance case for every plot and question combination. The study decided which technique was better using absolute error between the subjects response and the true value. They specified that the error bars are not confidence intervals as they represent the true underlying distribution, not the sampling distribution of the mean. They found that the HOPs reliably outperformed the violin and error bar plots for all the two and three distribution questions, but were no better than the violin or error bar plot in the other univariate cases (and in some cases worse). 

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: fig-examples
#| fig-subcap: 
#|   - "Error bar plot"
#|   - "Violin plot"
#|   - "HOPs frames 1 to 4"
#| fig-cap: "An example of the plots shown in the study by @Hullman2015 that were used to answer the quesiton 'In what percentage of vials is there more of solute B than A (Probability(B > A)?'. The error bar plot and violin plot depict the marginal distribution of Solutions A and B while the HOPs depicts the joint distribution. The questions is easier to answer using the HOPs since it highlights the correct distribution."
#| layout: "[[45,-5, 45], [100]]"


# Make error bar data
ymin <- c(30, 40)
ymax <- c(70, 80)
smean <- c(50, 60)
solute <- c("Solute A", "Solute B")
error_data <- tibble(ymin, ymax, smean, solute)

# Make error bar plot
ticks <- seq(0,100, 10)
error_data %>%
  ggplot(aes(x=solute)) + 
  geom_errorbar(aes(ymin=ymin, ymax=ymax), width=0.1, 
                linetype="dashed", size=1.4) +
  geom_boxplot(aes(y=smean), colour="dodgerblue") +
  labs(y="Parts Per Million (PPM)")+
  scale_y_continuous(breaks=ticks, limits=c(0,100)) +
  theme_classic() +
  theme(axis.title.x=element_blank(), 
        axis.text.x=element_text(size=15),
        aspect.ratio=1)

# Make violin plot data
n <- 100
soluteA <- rnorm(n, 50, 10)
soluteB <- rnorm(n, 60, 10)
solute <- c("Solute A", "Solute B")
violin_data <- tibble(ppm = c(soluteA, soluteB), 
                      solute =c(rep("Solute A", n), rep("Solute B", n)))

# Make violin plot
violin_data %>%
  ggplot(aes(x=solute, y=ppm)) + 
  geom_violin(fill= "dodgerblue", colour="dodgerblue") +
  labs(y="Parts Per Million (PPM)")+
  scale_y_continuous(breaks=ticks, limits=c(0,100)) +
  theme_classic() +
  theme(axis.title.x=element_blank(), 
        axis.text.x=element_text(size=15),
        aspect.ratio=1)  

# HOPs Data
frame <- rep(seq(4), 2)
y <- c(rnorm(4, 50, 10), rnorm(4, 60, 10))
solute <- c(rep("Solute A", 4), rep("Solute B", 4))
hops_data <- tibble(frame, y, solute)

# HOPS plot frame
hops_data %>%
  ggplot(aes(x=solute)) + 
  geom_boxplot(aes(y=y), colour="dodgerblue") +
  labs(y="Parts Per Million (PPM)")+
  facet_wrap(~frame, nrow=1) +
  scale_y_continuous(breaks=ticks, limits=c(0,100)) +
  theme_classic() +
  theme(axis.title.x=element_blank(), 
        axis.text.x=element_text(size=6),
        axis.text.y=element_text(size=6),
        aspect.ratio=1)

```

My first issue with the plots in @Hullman2015 study is that the violin plot and error bars are visualising a different distribution to the HOPs. The error bar plot and the violin plot in @fig-examples visualise the marginal distributions of solutions A and B and provides no information on the joint distribution. The HOPs is similar to a slope graph except instead of connecting the outcomes from A and B with a line, they are connected through frames of an animation. The HOPs, just like a slope graph, depicts a relationship between two variables, i.e. the *joint* distribution. This key feature can be used to improve upon the HOPs and direct our attention to the distribution that is even more useful for answering this question.

Two alternative graphics that could be used to answer the question "In what percentage of vials is there more of solute B than A (Probability(B > A)?" are provided in @fig-alternatives. The scatter plot depicts the joint distribution of the two solutions and uses colour to highlight the Bernoulli distribution that is more closely aligned with the question. The stacked bar exclusively visualises the Bernoulli distribution that describes the event $B>A$ and ignores the joint distribution highlighted by the scatter plot. Through this process of moving from the marginal distributions, to the joint distribution to the Bernoulli distribution we moved the information needed to answer the question "What is $P(B>A)$?" from something you needed to calculate in your head (when looking at the error bar plots) to something you can **see** in the bar chart. While this process is illuminating, it is important to avoid whittling down the problem **too** much. Providing a categorical decision alone is somewhat useless [@Joslyn2012], and visualising a single estimate is akin to providing a decision or expressing no uncertainty at all. We want to simplify a graphic to make the important distribution relevant but not so much that we remove the information that gives the graphic context. 

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: fig-alternatives
#| fig-subcap: 
#|   - "Scatter Plot"
#|   - "Stacked Bar Chart"
#| fig-cap: "Two plots that could be used as alternatives to answer to question 'In what percentage of vials is there more of solute B than A (Probability(B > A)?'. The scatter plot in (a) illustrates the joint distribution while the stacked bar chart in (b) is an expression of the Bernouli distribution of the event B > A. The closer the distribution displayed is to the distribution of interest, the easier it is to answer the question."
#| layout: "[[48, -4, 48]]"

# Scatter plot data
scatter_data <- tibble(soluteA = soluteA,
                       soluteB = soluteB) %>%
  mutate(biggerb = ifelse(soluteB>soluteA, "Yes", "No"))

# Scatter plot
scatter_data %>%
  ggplot(aes(x=soluteA, y=soluteB)) +
  geom_point(aes(colour=biggerb)) +
  labs(x="Solute A Concentration (PPM)", 
       y="Solute B Concentration (PPM)") +
  guides(colour=guide_legend(title="Is B>A?")) +
  scale_y_continuous(breaks=ticks, limits=c(0,100)) +
  scale_x_continuous(breaks=ticks, limits=c(0,100)) +
  theme_classic() +
  theme(aspect.ratio=1) +
  scale_colour_brewer(type = "qual", palette = 6)

# Make bar plot data
bar_data <- scatter_data %>%
  mutate(biggersolute = ifelse(biggerb=="Yes", "Solute B", "Solute A")) %>%
  select(-biggerb)

# Stacked bar chart
bar_data %>%
  mutate(dummy = "d") %>%
  ggplot(aes(x=dummy, fill=biggersolute)) +
  geom_bar() +
  labs(y = "Proportion") +
  guides(fill=guide_legend(title="% of draws that will have more of...")) + 
  scale_y_continuous(breaks=ticks, limits=c(0,100)) +
  theme_classic() +
  theme(axis.title.x=element_blank(), 
        axis.text.x=element_blank(),
        axis.ticks.x = element_blank(),
        aspect.ratio=4/2)  +
  scale_fill_brewer(type = "qual", palette = 6)
  
```

### Theory: Selecting the correct distribution
In order to fairly compare two visualisations, they need to provide the same information. The important distinction between the distribution displayed and the distribution required to answer a question is often ignored in our discussions of good or bad visualisations. This means that studies identifying some graphics as better than others may only do so because the two plots displayed different distributions and one was more relevant to the question. @fig-distdraw illustrates how we think about distributions when performing statistical tests compared to when we create visualisations. When we compute statistics or perform a hypothesis test, we typically put a lot of thought into selecting the correct distribution for our question, however, when we perform visualisation we typically plot a collection of normal marginal distributions and ignore the actual distribution required. This extends to our perception of visualisation in general as most of the visualisations we consider to be for "distributions" are actually just tools for visualising marginal distributions. 

![This is an illustration of the differences in the distributions we infer from our data when we are doing a hypothesis test versus when we are constructing a visualisation. The complete joint distribution of all variables gives us the most complete picture, a sub category of that are all the distributions we use to answer questions about our data, and a sub category of that are specific marginal distributions. When we visualised data we only consider expressing the marginal or conditional distributions of our variables, even when they don't have the information we desire. ](incorrectdistributions.jpeg){#fig-distdraw}

An example that might be more familiar to the average statistician is something I like to refer to as "the error bar problem". If you have ever looked at two overlapping error bars and said "oh these variables are not statistically significantly different" you have used error bars incorrectly. While it is true that error bars that do not overlap implies statistical significance, overlapping error bars do not imply the converse. The same is true for a lot of the other ad-hoc statistical tests we use error bars for. A study done by @Bella2005 asked participants to adjust two error bars until the means were "just" statistically significantly different, and most people adjusted the error bars until they were just touching. In the case of independent confidence intervals, the p-value of the two-tailed t-test for error bars that just touch is about 0.006 [@Schenker2001]. @Bella2005 also found that few people could incorporate changing information about independence that arises from repeated measure design and most participants were ignorant to the fact that error bars are used for both confidence intervals and standard error bars, two wildly different indicators of precision. 

This is a classic example of expecting readers to draw conclusions about a distribution that was not visualised. Error bars typically represent the 95% confidence interval of a sampling distribution, most commonly the significant values of a t-distribution. This means that each error bar provides the range of significant values through the two end points, and a vague indicator of variance with the length of the bar, *of each variable independently*. What an error bar does **not** depict is the t-distribution associated with a difference of two means, the equivalent statistical test we utilise error bars for. The visualisation itself is not the problem, trying to draw conclusions that requires information that was not visualised, is.

The papers that go on to cite these misunderstandings about error bars discuss the work as though the problems are caused by error bars themselves. They suggest that error bars should be avoided as a visualisation tool, ignoring the fact that these fundamental misunderstandings in uncertainty will likely follow other visual encodings of t-distributions. Maybe some visual aspect of error bars encourages these extrapolations, but the HOPs example I drew on at the start of this chapter illustrates that this problem of expecting people to answer questions that are not directly related to the visualised distribution is larger than questions about significance. 

This distribution framework does not mean you actually need some assumed distribution to apply the taxonomy. Data made with resampling methods from some unspecified null are fine too. What is important is recognising whether or not the distribution is appropriate for the question. Thinking about how to visualise specific distributions is not alien to this specific framework.

This view of understanding uncertainty visualisations opens up a new gap in the literature. For example, if we accept that visualizing a collection of t-distributions is not a substitute for a collection of F-tests or paired t-tests, how *should* we visualise our data if we want to draw those conclusions? This distribution based perception to visualisation highlights areas of improvement in our current work. @Wickham2011 discusses how in different displays of product plots result in depictions of different marginal, conditional, and joint distributions of data, which also does not require an assumed distribution.

## Part 2: Features
### Example: returning to the HOPs, error bars, and violin plots
If we have two visualisations that depict the same distribution, does that mean they contain the same information? No. Let us return to the study done by @Hullman2015 to answer this question. Not only are the distributions depicted in the three visualisation methods different, but the features of each distribution are also different. In discussing the concept of a *best* visualisation, we rarely discuss which *features* of the distribution are being displayed in the graphic. The way we currently look at visualisation would classify the error bar plot and the violin plot as visualisations of a "distribution", the scatter plot is a visualisation of a "relationship" while the bar plot is a visualisation of "amounts" (@wilke2019fundamentals), but this categorisation hides a lot of important details about drawing information from a graph. In this example the violin plot and the scatter plot both showed that each solution had an independent marginal normal distribution, the error bar (although technically a plot for visualising distributions) gives no concept of mass and would not give you the ability to identify even a simple distribution. Not only is it important to select a distribution that is appropriate for our question, it is also important to *show* the aspect of that distribution that holds the relevant information. The example asked about the *frequency* of a particular outcome which translates to visualising a hypothetical sample from a joint distribution **or** the *parameter* of the Bernoulli distribution. These are the aspects of each distribution that best highlight the information we are looking for. If we visualised features of the distribution that don't hold the information we are looking for, the information is difficult to ascertain and is more likely to be found through potentially faulty heuristics.

@fig-bad depicts two graphics that have the of the same distributions as those in @fig-alternatives, but each plot visualises an aspect of the distribution that is not as relevant to the question as those in @fig-alternatives. The 2D error circle highlights the mean and the values that are within the 95% confidence range of the joint distribution. Since neither of those parameters are directly related to the question at hand, visualising the mean and significance instead of the outcomes made it harder to answer "What is $P(B>A)$?". Since the parameter of the Bernoulli distribution *is* directly related to the question, visualising a sample makes it harder to answer the question.

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: fig-bad
#| fig-subcap: 
#|   - "Error circle. The point represents the mean and the circle represents the 95% CI"
#|   - "A scatter plot depicting the samples of the Bernoulli distribution"
#| fig-cap: "Two plots that are somewhat useful in answering the question 'In what percentage of vials is there more of solute B than A (Probability(B > A)?'. While the distributions visualised are correct, each plot expresses an inadequate feature of the respective distributions sicne plot (a) visualises a mean and significance valeu when it should visualise a set of outcomes and (b) visualises a set of outcomes when it should express a mean. We can see that visualising the correct feature of the distribution is just as important as highiting the correct distribution."
#| layout: [[46,-4, 40]]
#| layout-valign: "bottom"

# CIRCLE PLOT
library(ggforce)
ticks <- seq(0,100, 10)
tibble(x=50,
       y=60) %>%
  ggplot(aes(x,y)) + 
  geom_circle(aes(x0=x, y0=y, r = 20))+
  geom_point() +
  geom_abline() + 
  scale_y_continuous(breaks=seq(0,100, 10), limits=c(0,100)) +
  scale_x_continuous(breaks=seq(0,100, 10), limits=c(0,100)) +
  labs(y = "Solution B Parts Per Million (PPM)",
       x = "Solution A Parts Per Million (PPM)")+
  theme_classic() +
  theme(aspect.ratio=1)

# Sample Bernoulli Plot
scatter_data %>%
  ggplot(aes(x=biggerb, y=0, colour=biggerb)) +
  geom_jitter(width=0.4, height=0.4) + 
  scale_y_continuous(limits=c(-1,1)) +
  labs(x = "Is B>A?")+
  theme_classic() +
  theme(aspect.ratio=1,
        axis.title.y = element_blank(), 
        axis.text.y=element_blank(),
        legend.position = "none",
        axis.ticks.y = element_blank()) +
  scale_colour_brewer(type = "qual", palette = 6)

```

Not only does visualising the incorrect distribution and incorrect features make it difficult to answer our questions, it might also completely disorientate the viewer. In the discussion of the HOPs, error bar, and violin plot study @Hullman2015 notes: 
> "In light of the common usage of error bars for presenting multiple independent distributions, it is noteworthy how poorly subjects using these representations performed on tasks asking them to assess how reliably a draw from one variable was larger than the other...Many subjects reported values less than 50%, which are not plausible given that the mean of B was larger than the mean of A."

### Theory: the four features of a distribution
Different features of a distribution have different questions they are mode adept adept at answering. The aspect of a distribution that are typically depicted in graphics can be organised into four key features. @fig-features depicts these four features along with an example of how they are typically depicted in graphics. Each of these four features have a set of questions they are better equip to answer. The four features are:

1) **Mass** describes the PMF/PDF or CMF/CDF of the functions. Depictions of mass can inform us of the mode, likely or impossible values, and whether or not we have an identifiable distribution.

2) **Samples** are a set of actual or simulated outcomes of a distribution. Our data falls into this category as it can be seen as an outcome of some "underlying" distribution. Samples can also be simulated through techniques such as bootstrapping or random sampling. Samples are a often a good way to present an impressions of a distribution "as it is" because little to no processing is required to show it. This feature is useful in answering questions about frequency or probability. 

3) **Parameters** are the statistics that are related to our distribution. They can be the sufficient statistics of the distribution, such as the mean, variance, minimum and maximum, or they can be other statistics such as the or correlation, median and mode. Visualising a specific parameter of our distribution typically gives us freedom because it allows us to express any aspect of a plot in terms of a single value. Unfortunately this flexibility means that the set of questions any single parameter plot can answer are limited. Questions about the mean, median, maximum, minimum, correlation, values of significance, etc. would usually need multiple plots to answer.

4) **Exchangeability** illustrates whether or not elements from a sequence of random variables in this distribution can be swapped. Rather than wanting to answer questions about exchangeability, it is usually something we want to highlight in our data. Exchangeability is often expressed by having the graphical elements touch (if the data is inexchangeable) or not touch (if the data is exchangeable). A time series' inechangeability is expressed using a line plot, spatial data's inechangeability is expressed using a map and the exchangeability of randomly sampled data is often expressed using points. Exchangeability is adjacent to continuity because visually connected features in a graphic are also used to express discretised outcomes of a continuous process (such as a ridgeline plot or a histogram).

![An illustration of the four main features of a distribution that graphics are typically used to represent. The four expressions of a distribution are mass, sample, parameters and exchangeability. Every commonly used visualisation can be categorised using these four features.](distfeatures.jpeg){#fig-features width=60%}

This is not an exhaustive list of every possible feature of a distribution, but rather the four most commonly visualised aspects of a distribution. Almost every currently used visualisation can be categorised based on which distributions and related features it depicts. Additionally, these features are not entirely distinct, particular visualisations of parameters or samples may also double as a depiction of mass. Therefore, a graphic that is designed to depict one feature of a distribution may depict multiple features of multiple distributions. A boxplot is a good example of a graphic that depicts both mass (at a very low resolution) and a handful of selected parameters. Additionally different distributions that appear in the same plot can be expressed with different features. @fig-errortest visualises a simple hypothesis test by expressing one distribution with significant values, and the other with a sample. This visualisation shows the probability of a type two error expressed by the number of points in the circle.

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: fig-errortest
#| fig-cap: "A visualisation of a hypothesis test where the black circle represents the significant values of the null distribution and the pink points represent a sample from the true distribution. We are concerned with the threshold values of the black distribution, so it is expressed as a parameter, but we are interested in the frequency of an outcome in the pink distribution, so it is expressed as a sample. The number of points (out of 100) from the true distribution that fall in the black circle show the probability of a type two error."

# Generate Data
distA <- tibble(x = rnorm(100, 30, 10),
                y = rnorm(100, 30, 10))
# CIRCLE PLOT
library(ggforce)
ticks <- seq(0,100, 10)
tibble(x=50,
       y=60) %>%
  ggplot(aes(x,y)) + 
  geom_circle(aes(x0=x, y0=y, r = 20))+
  geom_label(label="Null") +
  geom_point(data=distA, colour="violet") +
  geom_label(x=30, y=30, label="True", colour="violet") +
  scale_y_continuous(breaks=seq(0,100, 10), limits=c(0,100)) +
  scale_x_continuous(breaks=seq(0,100, 10), limits=c(0,100)) +
  labs(y = "Random Variable B",
       x = "Random Variable A")+
  theme_classic() +
  theme(aspect.ratio=1)
```

Identifying plots by these four features instead of the data itself can change the way we conceive of plot similarities and differences. If we think of of the features or plots themselves being data we can apply these frameworks recursively. An animated plot is the same as a line plot except response values are different (a line plot has a single response value and an animated plot has a static plot as a response value) and the inexchangeability is expressed through animation instead of physical connection. The line-up protocol is an application on this idea and considers each visualisation to be a single outcome of some larger distribution [@Buja2009; @Wickham2010; @Chowdhury; @Hofmann2012]. By generating a sample of visualisations from a hypothetical distribution visualisation authors can check if perceived patterns are real or merely the result of chance.

There are other distributional features, such as discreteness, that are important to consider in uncertainty visualisation. There is a reasonable amount of evidence that cumulative displays or discrete displays (such as a quantile dot-plots or histograms) are the best ways to express mass for decision making and probability estimates [@Fernandes2018; @Hofmann2012; Kay2016; @Hullman2018; @kale2019decision]. Despite this, I don't provide a feature of discreteness. This is because features, such as discreetness do not provide additional information when we include them, it is simply an adjustment in the way we express mass. Considering adjustments of the features to be features in of themselves will make the list overwhelming and defeat its purpose.

## Part 3: Hierarchy
### Example: Looking to spatial uncertainty
Knowing the relevant distributions and the best features to use to express these distributions are not of much help if we have too many features to visualise. Often, our goal in visualisation is not to perform a statistical test or to make a decision based on a single estimate, but to identify some underlying structure in our data or make a decision based on a large number of connected systems. We also can't just visualise each aspect of these complicated systems separately. Elements from a single distribution should be displayed using a single plot, since displaying the features of one distribution across multiple plots makes the information hard to combine and results in some details (such as the estimate error) being completely ignored [@moritz2017trust; Correll2018]. An example of a case where following these rules becomes difficult is when we want to visualise the spatial context of an estimate along with it's error.

Trying to depict the uncertainty of an estimates with a spatial aspect is incredibly difficult. A common complaint about choropleth maps is that they display the total of the estimated parameter as a function of **land size** even if land size is irrelevant to the estimate (e.g. a visualisation of red vs blue states in an American election). Visualisations that correct for this, such as hex maps, do so by colouring and plotting hexagonal tiles that each represent a portion of the dependent location [@Kobakian]. This means an irrelevant feature, such as land size is not depicted as important in the map but the location dependency is maintained. Trying to highlight the uncertainty associated with these estimates makes the process even more difficult.

There are four proposed methods of visualising spatial uncertainty that can be made with the `Vizumap` R package [@lucchesi2021vizumap]. These four plots are shown in @fig-maps.

@fig-bivariate depicts a bivariate map which uses a bivariate colour palette that is created by blending two single hue colour palettes. One colour represents the variable of interest while the other represents the sampling error of that variable. There are two immediate problems with this method. First of all, uncertainty is being expressed with hue and saturation which @Maceachren2012 found to be the worst aesthetics to map to uncertainty to as they don't have an intuitive interpretation. Value has a natural connection to uncertainty (lighter values equate to higher uncertainty and darker values equate to more certainty) so it is a much more appropriate choice. While the `Vizumap` data does depict areas of light and darkness, they are largely irrelevant to the uncertainty measure causing our heuristics to lead us to the incorrect conclusions. The Value-Suppressing Uncertainty Palettes (VSUP) shown in @fig-vsup maps estimates to the hue and error to the value thereby creating a more intuitive plot [@Correll2018]. Additionally, at high levels of uncertainty VSUP only has one output colour, which prevents viewers from decrypting any particular value and also avoids enforcing a binary encoding of significance [@Correll2018]. Unfortunately VSUP are not easy to combine with packages like `Vizumap` which leaves it still somewhat difficult to express this encoding in practice, however the combination of a bivariate map with VSUP has shown to improve decisions in the face of uncertainty [@Correll2018]. 

@fig-pixel depicts a pixel map. Pixel maps are similar to HOPs since they present a sample of possible values for the estimate, rather than a single value and an uncertainty visualisation. Currently the effectiveness of a pixel map is yet to be shown in any experiments, and it may turn out to be a poor encoding of uncertainty information, however it is a promising visualisation. Unfortunately, mapping a distribution of colours to numerical values is currently required to extract any numerical estimates from the plot which is a relatively difficult mental task. Therefore the pixel map might be more effective if values were written explicitly on top of the pixels. The pixel map is also quite computationally expensive and hard to interpret when the relevant geographical areas are small relative to a large map, so it is best used on simpler smaller geographical areas.

@fig-exceed is a exceedance probability map that shows the probability of the estimate being over a certain value. This plot was developed specifically for decision making so it is a simple visualisation of a single parameter [@Kuhnert2018]. An exceedance probability map map is actually very similar to a choropleth map, but instead of expressing an estimate, it shows a probability. Therefore this plot is only able to express probability through a change in information rather than an improvement in visualisation techniques.

@fig-glyph is a glyph map that uses colour of a glyph to express an estimate and the rotation of the glyph to express uncertainty. Orientation has no intuitive link to uncertainty and should be avoided at all costs [@Maceachren2012]. Additionally, by mapping the estimate and its error to distinctly different features, this plot makes it easier to ignore the uncertainty associated with an estimate.

```{r}
#| eval: false
#| echo: false
data(us_data)
data(us_geo)
# Data stuff
us_data <- us_data %>% mutate(Estimate = pov_rate,
                              Error = pov_moe)
poverty <- read.uv(data = us_data, estimate = "Estimate", error = "Error")

# Bivariate plot
# make pal 
customBivPal3 <- build_palette(name = "usr", colrange = list(colour = c("chartreuse4", "darkblue"), difC = c(3, 4)))
customBivPal1 <- build_palette(name = "usr", colrange = list(colour = c("tan2", "lightskyblue"), difC = c(1, 1)))
# Make map
usBivMap <- build_bmap(data = poverty, geoData = us_geo, id = "GEO_ID", terciles = TRUE, , palette = customBivPal3)
# make key
usBivKey <- build_bkey(data = poverty, palette = customBivPal3, terciles = TRUE)
# attach key (+ visualise)
attach_key(usBivMap, usBivKey) 
ggsave("ideas/confirmation/bivariatemap.jpeg", width = 10, height = 5)

# Pixel Map
us_data$GEO.id2 <- as.numeric(us_data$GEO.id2)
ca_data <- subset(us_data, us_data$GEO.id2 > 6000 & us_data$GEO.id2 < 7000)
ca_data <- read.uv(data = ca_data, estimate = "Estimate", error = "Error")
row.names(ca_data) <- seq(1, nrow(ca_data), 1)
ca_geo <- subset(us_geo, us_geo@data$STATE == "06")
pix <- pixelate(ca_geo, id = "region")
df <- data.frame(region = sapply(slot(ca_geo, "polygons"), function(x) slot(x, "ID")), name = unique(ca_geo@data$GEO_ID))
ca_data$region <- df[match(ca_data$GEO_ID, df$name), 1]
ca_data$region <- as.character(ca_data$region)
unifPixMap <- build_pmap(data = ca_data, distribution = "uniform", pixelGeo = pix, id = "region", border = ca_geo)
view(unifPixMap)
ggsave("ideas/confirmation/pixelmap.jpeg", width = 7, height = 7)

# Glyph Map
co_geo <- subset(us_geo, us_geo@data$STATE == "08")
us_data$GEO.id2 <- as.numeric(us_data$GEO.id2)
co_data <- subset(us_data, us_data$GEO.id2 > 8000 & us_data$GEO.id2 < 9000)
co_data <- read.uv(data = co_data, estimate = "Estimate", error = "Error")
usGlyphMap <- build_gmap(data = co_data, geoData = co_geo, id = "GEO_ID", size = 80, glyph = "icone", border = "county")
usGlyphKey <- build_gkey(data = co_data, glyph = "icone")
attach_key(usGlyphMap, usGlyphKey)
ggsave("ideas/confirmation/glyphmap.jpeg", width = 7, height = 5)

# Exceedance probability map
poverty <- read.uv(data = us_data, estimate = "pov_rate", error = "pov_moe")
quantile(us_data$pov_rate)
pd <- quote({ pexp(q, rate, lower.tail = FALSE) })
args <- quote({ list(rate = 1/estimate) })
pdflist <- list(dist = pd, args = args, th = 30)
usExcMap <- build_emap(data = poverty, pdflist = pdflist, geoData = us_geo, id = "GEO_ID", key_label = "Pr[Estimate > 30]")
view(usExcMap)
ggsave("ideas/confirmation/exceedmap.jpeg", width = 10, height = 5)
```


::: {#fig-maps layout="[[59,-2, 39], [59,-2, 39]]" layout-valign="bottom"}

![Bivariate Map](bivariatemap.jpeg){#fig-bivariate}

![Pixel map](pixelmap.jpeg){#fig-pixel}

![Exceedance probability map](exceedmap.jpeg){#fig-exceed}

![Glyph Map](glyphmap.jpeg){#fig-glyph}

Four spatial uncertainty visualisations that can be made using the `Vizumap` package. Each plots depicts a map with a combination of estimate and error expressed using (a) a bivariate colour palette, (b) a sample of outcomes, (c) a statistic that uses both the error and estimate in its calculation, and (d) using colour value for the estimate and rotation for the uncertainty. The pixel chart (b) gives a good sense of uncertainty, the exceedance probability map (c) is easy to read, and the bivariate map (a) and glyph map (d) are hard to interpret because the estimate and the uncertainty are not well integrated with each other.
:::


![An example of the Value-Suppressing Uncertainty Palette designed by @Correll2018. The change in hue shows a change in the estimated value, while the change in value highlights a change in uncertainty. The estimates merge together as the uncertainty increases to prevent viewers identifying insignificant differences between values.](vsup.png){#fig-vsup width=35%}



The four plots depicted in @fig-maps are merely some suggested solutions for a particular case of uncertainty visualisation. They are great when we want to highlight the error associated with a particular estimate, or visualise a sample, but if we need an idea of a more complicated feature of the distribution (such as mass) they are not so useful. What is interesting about this spatial uncertainty example is the implicit hierarchy we put on the error and the estimate. @fig-bivariate set the estimate and error at the same level of importance; @fig-glyph established the estimate to be of higher importance than the error; and @fig-pixel and @fig-exceed both visualised a feature that combined the error and estimate (by expressing a sample and a parameter that combined the error and estimate respectively) placing both at the same level of importance. What may have been less obvious is that all four plots place spatial information at the highest level of importance. If the only thing you want from your plot is a sense of estimates with respect to their position in space, these plots work well, but sometimes other pieces of information are more important. By always assuming features such as the spatial context are always the most important aspect of a plot, we kneecap research and don't consider the wide array of ways we can express complicated concepts such as exchangeability and location.

### Theory: Mapping the features to the appropriate hierarchy
When there are a large number of features competing for our interest, we don't much flexibility in how we choose to visualise them. It is common knowledge that any single plot cannot reveal all information about a data set, different plots obfuscate and uncover different types of information. Leveraging that trade off is what makes a good visualisation and we do that by using the hierarchy of visual perception.

A hierarchy of elementary perception tasks is not a new idea in visualisation. @Cleveland1984 found a natural ordering of 10 elementary perception tasks in term of how accurately participants could extract information that was mapped to that feature. The hierarchy established was:
1) Position along a common scale
2) Positions along non-aligned scales
3) Length, direction (slope), angle (starting from the same origin)
4) Area
5) Volume, curvature
6) Shading, colour saturation
While @Cleveland1984 notes that these tasks are not exhaustive nor mutually exclusive, and accuracy is not the only metric that should decide if a graphic is worthwhile, this hierarchy does provide a useful rule of thumb in understanding the importance of information in a graph. This paper is also quite old, so more modern aesthetics such as time (for an animation) or obscure aesthetics such as rotation which was used in the glyph map are not tested. 

This means considering the information in the graph alone is not sufficient. We also need to consider the implicit hierarchy of importance we assign to that information. It is one thing to have information in a plot, it is another for this information to be displayed in a way that allows for the information to be *efficiently* extracted. Previous discussions on hierarchy focused on mapping specific variables to elementary tasks, but I am combining the idea with the concept of relevant features and asking you to instead consider what features are mapped to each elementary ask.

It is also important to keep in mind that just because information is in a graphic, that does not mean it will be "seen". The phenomena of inattentional blindness shows that there is no perception without attention and it is powerful enough that participants can fail to be aware of random objects appearing on a screen or a gorilla walking through a basketball game [@simons1999gorillas; mack2003inattentional]. Therefore if information is mapped to graphical elements that are so low on hierarchy they can be ignored, they might as well not be there at all. The people who don't plot uncertainty because they think its unimportant and the people who relegate uncertainty to the lowest ranking aesthetic on the list are both saying the same thing, "I think the uncertainty is unimportant". Including uncertainty is worth very little if no attention is left to see it. This does not mean we cannot put a *large* amount of information in a graphic. Glyph maps can be used to depict the multi-dimensional information in spatial-temporal data, but we still need to decide what information is important. Different scales can be used depending on whether you want to identify trends in global variance or local variance [@Wickham2012], and smaller details are almost impossible to convey. No matter how much information we try and put in a plot, there will always be only a handful of key takeaways.

Not only should we considered the hierarchy in the information we depict, but we also need to consider the heuristics that connect some pieces of information to specific elementary tasks. For example @Hofmann2012 showed that polar co-ordinates are more effective than cartesian co-ordinates when considering data that depicts a 360 degree direction (a case where polar co-ordinates has a natural interpretation). Uncertainty also has a natural mapping that should be considered when we express it in a plot. Error is best mapped to fuzziness, location, and colour value; arrangement, size and transparency are an OK second choice; but saturation, hue, orientation and shape are unacceptable and have no intuitive connection to variance [@Maceachren2012]. No only do the graphical elements we map our features to matter, but the direction matters too. Graphical elements that are more fuzzy (fuzziness), further from centre (location), lighter (colour value), poorly arranged (arrangement), smaller (size), more transparent (transparency) are perceived to be more uncertain [@Maceachren2012]. This idea also extends to interval estimation, where gradient expressions are best for questions about probability, but ambiguation is best for start and end time estimation [@Gschwandtnei2016]. Heuristics can work against us just as much as they can work for us. The sine illusion can cause the confidence interval of a smoothed sine curve to seem wider at the peaks than the troughs, causing us to underestimate uncertainty associated with changing values [@Vanderplas2015]. Therefore we should not only keep the hierarchy of information in mind when we map features of our distribution, but also take advantage of these intuitive mappings when we can.

The final consideration when deciding what information to depict is whether or not additional information will clutter the graph. Our visualisation should aim to show enough information to solve a task while avoiding irrelevant distracting information [@kosslyn2006graph]. While including additional features can increase the accuracy of some conclusions, it can also bias or discount others. Including means estimates on depictions of mass can cause people to discount the uncertainty information and use the difference between means as a proxy for the probability distribution [@Kale2021]. @Nathonours found that a scatter plot is better than a line plot if you want to convey the correlation between two time series but we cannot be sure if this was influenced by swapping the distribution depicted or by dropping the irrelevant feature of inexchangeability. @fig-timescatter depicts four different visualisations of the same time series data. Each plot depicts a different combination of distribution (joint or conditional on time) and exchangeability (the inexchangeability of a time series is either depicted with connected values or ignored with points). This plot highlights that the information we choose to discard in a plot is just as important as the information we choose to keep.

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: fig-timescatter
#| fig-subcap: 
#|   - "Conditional distribution on time with inexchangeability"
#|   - "Conditional distribution on time"
#|   - "Joint distribution of series"
#|   - "Joint distribution of series with inexchangeability"
#| fig-cap: "Four figures that illustrate the strikingly different visualisations we get when we make one change in the distribution and feature depicted. Plots (a) and (b) depict two conditional distributions of a time series while plots (c) and (d) show the joint distribution of those two series and express time with opacity such that recent dates are darker. Plots (a) and (d) express the inechangeability of time series data with connected points while plots (b) and (c) ignore that feature of the data. By ordering these plots by how well they express correlation, we can untangle how much improvement is contributed by the distribution verus how much is caused by the change in features."
#| layout: [[48,-4, 48], [43, -14, 43]]

# Data for conditional distribution
series_data <- aus_retail %>%
  filter(State %in% c("Victoria", "Queensland"),
         Industry == "Food retailing",
         year(Month)>2010) 

# Data for joint distribution
series_data2 <- series_data %>%
  select(-c(Industry, `Series ID`)) %>%
  pivot_wider(id_cols = Month,
              values_from = Turnover,
              names_from = State)%>%
  mutate(id = row_number()/max(row_number()))

# Data for joint distribution w segments
series_data3 <- series_data2 %>%
  mutate(x1 = lag(Queensland),
         y1 = lag(Victoria),
         x2 = Queensland,
         y2 = Victoria) %>%
  select(-c(Queensland, Victoria))

# Line plot
ggplot(series_data, aes(x=as.Date(Month), y=Turnover, colour=State)) +
  geom_line() +
  labs(y="Food retailing turnover (AUD)",
       x = "Month") + 
  theme_classic() +
  theme(aspect.ratio=1)  +
  scale_colour_brewer(type = "qual", palette = 7)

# Point plot
ggplot(series_data, aes(x=as.Date(Month), y=Turnover)) +
  geom_point(aes(colour=State)) +
  labs(y="Food retailing turnover (AUD)",
       x = "Month") + 
  #geom_segment(data=series_data2,
  #             aes(y=Queensland, xend=as.Date(Month),
  #                 yend=Victoria)) +
  theme_classic() +
  theme(aspect.ratio=1) +
  scale_colour_brewer(type = "qual", palette = 7)

# Scatter plot
ggplot(series_data2, aes(x=Queensland, y=Victoria, alpha=id)) +
  geom_point() +
  labs(y="Victoria food retailing turnover (AUD)",
       x= "Queensland food retailing turnover (AUD)") +
  theme_classic() +
  theme(aspect.ratio=1,
        legend.position = "none")  

# Scatter plot with exchangeability
ggplot(series_data3, aes(alpha=id)) +
  geom_segment(aes(x=x1, y=y1, xend=x2, yend=y2))  +
  labs(y="Victoria food retailing turnover (AUD)",
       x= "Queensland food retailing turnover (AUD)") +
  theme_classic() +
  theme(aspect.ratio=1,
        legend.position = "none") 
```

In the introduction, when I displayed the power of visualisation using Ancombe, I implied that visualisations have a miraculous power to tell us what is important about our data. That is not entirely true. While Anscombe's quartet shows the importance of using visualisation to find hidden details, it also highlights the importance of a visualisation author who knows how to express what is important. If I provided a visualisation of Anscombe's quartet but used a different combinations of variables, presented an expression of mass, or provided a series of error bar plots (which would have all been identical), the interesting features of the data would have remained hidden. We are often not so lucky to already *know* the best way to visualise our data, so having rules that allow us to understand *what* we are visualising is the first step in finding that "best perspective".  


{{< pagebreak >}}

# Timeline

|Date|Description|
|:-|:---|
|May 2022| Changed topic to uncertainty visualisation with new scholarship funded by AEMO |
|August-November 20232|Leave from PhD|
|May 2023| Met with Dean Sharafi, Alireza Fereidouni, and Toby Price from the AEMO Systems Design and Transformation team to discuss current issues they are facing in uncertainty visualisation|
|June 2023| Meeting with AEMO team to map out future monthly meetings over the next year and build up applications of uncertainty visualisations|
|June 2023|Confirmation milestone|
|June 2023| Submit poster abstract on aspects of the literature review for IEEE VIS 2023|
|July 2023| Complete chapter 1, a detailed literature review paper that captures the key aspects of the infoviz community and submit to a journal|
|August 2023| Start initial experiments for Chapter 2 of the thesis|
|October 2023| Attend IEEE VIS 2023|
|December 2023| Attend ASC and present poster on taxonomy of uncertainty visualisations with results from preliminary experiments|
|April 2024| Submit paper summarising key findings from experiments |
|May 2024| Progress review milestone |
|September 2024| Attend POSIT::CONF(2024)|
|October 2024| Attend IEEE VIS 2024|
|April 2024| Submit paper for Chapter 3 |
|May 2025| Pre-submission milestone|
|October 2025| Attend IEEE VIS 2025 in Vienna |
|November 2025| Attend UseR! conference to discuss software developed for translation of uncertainty visualisation work|
|December 2025| Submit Thesis |

{{< pagebreak >}}

# Additional Work

For the duration of the PhD I have also been working on improving `cassowaryr`, an R package that finds interesting scatter plots in high dimensional data sets. Due to a dependency on the package `interp`, some computations take a long time unless noise is added to the data. We want to add binning to the `cassowaryr` package, so we need an alternative solution. To fix this issue my supervisor Di Cook and I did a Google Summer of Code project and supervised a recent high school graduate Umang Majumder, in making the `tRiad` R package. This package replaced some low level functions from `interp` that were causing issues. Since March 2023 I have been in discussions with the author of `interp` to work through this problem.

We want this package to work on large data scatter plots so that they can provide insights into the high dimensional energy case study we are working on. The package can also help inform improvements in uncertainty visualisations by investigating all the interesting bivariate joint distributions.  Therefore, binning needs to be implemented into `cassowaryr`.

{{< pagebreak >}}

# Bibliography

```{r, include=FALSE, eval=FALSE}
library(spelling)
qmd <- "ideas/confirmation/confirmationreport.qmd"
ignore <- readLines("WORDLIST")
check_spelling <- spell_check_files(
  qmd,
  ignore = ignore,
  lang = "en_GB"
)
if (nrow(check_spelling) > 0) {
  print(check_spelling)
  stop("Check spelling in Qmd files!")
}
```