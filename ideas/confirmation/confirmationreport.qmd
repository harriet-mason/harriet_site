---
title: "Plotting Apples, Oranges, and Distributions"
subtitle: "Two prevailing issues in the uncertainty visualisation literature"
author: Harriet Mason
bibliography: references.bib
date: last-modified
format: pdf
---

# Motivation
Think back to the last time you made some sort of data visualisation. What was the purpose of that visualisation? Was it to better understand your data? Was it to help you make a decision? Was it to to communicate that decision to someone else? There are many stages in our analysis that benefit from the power of data visualisation, however this does not mean it is always done with success. Visualization is an important step in exploratory data analysis and it is often utilised to **learn** what is important about a data set. The importance of data driven discovery is highlighted by data sets such as Anscombe’s quartet [@anscombe] or the Datasaurus Dozen [@datasaurpkg]. Each of the pairwise plots in these data sets have the same summary statistics but strikingly different information when visualised. Instead of having to repeatedly check endless hypothesis to find interesting numerical features, visualisations **tell** us what is important about the data set. This powerful aspect of data visualization is poorly or seldom used in later stages when we are communicating our findings, specifically with respect to uncertainty. 

Utilising visualisation can give people a more complete understanding of risks. Studies asking participants to sketch a distribution allowed them to better compute statistics about that distribution and improve predictions [@Hullman2018; @Goldstein2014]. While there is some evidence that confidence provided in text form only are less likely to be misinterpretated than grahics [@Savelli2013], text is insufficient to express more complicated aspects of a distribution, such as mass. The confusion caused by visualisation could also be due to a lack of expose, since @Kay2016 found that people exposed to the same uncertainty visualisation get better at making judgements the more they are exposed to them. Additionally, visualisation allows for interactive graphics that provide a more in depth understanding of probability [@Potter2009a; @Ancker2009] and infographics that make uncertainty more accessible for people with poor numeracy skills [@Ancker2009]. 

Despite these benefits, there is a reasonable amount of annecdotal and survey evidence that we don't visualise uncertainty as often as we should. Some economists suggest that visualisation authors are responding to incentives that make it tempting to avoid visualising uncertainty, even if those incentives are based more in perception than reality [@Manski2020]. A survey conducted by @Hullman2020a found that majority of visualisation authors agreed that expressing uncertainty is important and should be done more often than it currently is, some even agreed that failing to do so is tantamount to fraud. Despite this, only a quarter of respondents included uncertainty in 50% or more of their visualisations [@Hullman2020a]. Meaning participants were convinced that visualising uncertainty is morally important but were able to provide self sufficient reasoning that allows them to avoid doing it. The study by @Hullman2020a found that the most common reasons authors don't visualise uncertainty in practice despite knowing it's moral importance are: not wanting to overwhelm the audience; an inability to calculate the uncertainty; a lack of access to the uncertainty information; and not wanting to make their data seem questionable [@Hullman2020a].

If decision markers are not presented with the uncertainty about an estimate the data analysts have, for all intents and purposes, made the decision for the decision maker. Upon further interviews @Hullman2020a found that authors believed uncertainty would overwhelm the audience and make their data seem questionable because decision makers are unable to understand uncertainty. This belief, while pervasive, is not true. While some research suggests that laypeople cannot understand complicated concepts in statistical thinking (such as trick questions on hypothesis tests or the difference between Frequentist and Baysian thinking) [@Hoekstra2014; @Bella2005] there is a large amount of research suggesting that presenting uncertainty information improves decision making, both experimentally [@Joslyn2012; @Savelli2013; @Kay2016; @Fernandes2018] and in practice [@Al-Kassab2014]. As a matter of fact, doing what many authors currently do (providing only a deterministic outcome with no uncertainty) causes decision makers to be *less* decisive and have completely unbounded expectations on an outcome [@Savelli2013]. This reality cannot be avoided by providing secondary or non-specific information such as explaining calculations [@Joslyn2012], explaining the advantages of a recommendation [@Joslyn2012], or expressing uncertainty in vague terms [@Erev_1990; @Olson_1997], all of which are undesirable for decision makers and lead to measurably worse decisions [@Joslyn2012; @Erev_1990; @Olson_1997]. Expressing uncertainty verbally additionally decreases the percieved reliability and trustworthiness of the source [@VanderBles2020].

Not only does communicating uncertainty improve decisions but the mistrust created by communicating certainty in uncertain situations can be exploited. A 6-month survey of anti-mask groups on Facebook during to COVID-19 pandemic showed that the anti-maskers thought carefully about their grammer of graphics and made pursuasive visualisations using the same data as pro-mask groups by exploiting information ignored by the pro-maskers [@Lee2021]. It is understood that deceptive plots can lead viewers to come to incorrect conclusions or significantly overstate effects [@Pandey2015] but these incorrect takeaways cannot be mitigated with instructions in how to correctly understand the plot [@Boone2018]. This evidence indicates we are more likely than not to hurt our message when we ignore uncertainty information and trying to raise the general publics plot literacy is an insufficient strategy to curb conspiracy theories and misguided scientific communication.  In direct contrast to this, displaying numerical estimates of uncertainty information has shown to lead to greater trust in predicitions [@Joslyn2012; @VanderBles2020]. While @Han2009 found people have more worry when presented with uncertainty reguarding health outcomes, this worry is not a bad thing if the concern is warranted given the ambiguous situation.

The disconnect between the research supporting uncertainty and the consensus aganinst may not be entirely driven by a lack of understanding of the literature. For example, at least one interviewee from the study by @Hullman2020a claimed that expertise implies that the signal being conveyed is significant, but also said they would omit uncertainty if it obfuscated the message they were trying to convey [@Hullman2020a].  Other authors who were capable of calculating and and representing uncertainty well did not do it, and were unable to provide a self-satisfying reason why [@Hullman2020a]. These conflicting motivations are acknowledged in the paper itself where @Hullman2020a says:

> "It is worth noting that many authors seemed confident in stating rationales, as though they perceived them to be truths that do not require examples to demonstrate. It is possible that rationales for omission represent ingrained beliefs more than conclusions authors have drawn from concrete experiences attempting to convey uncertainty". 

An overwhelming consensus among visualisation authors seems to be that uncertainty is secondary to estimations. There is a belief help by those that work with data that the uncertainty associated with an estimate (the noise) only exists to hide the estimate itself (the signal). From this view, uncertainty is only seen as additional or hindering information, therefore despite its alleged importance, when simplifying a plot uncertainty the first thing to go. This belief is also reflected in the development of new uncertainty visualisations. Often when trying to visualise a high problem, uncertainty is relegated to unimportant aesthetics in the plot, often of lower importance than the estimate [@Correll2018; @Lucchesi2017]. This is not uncommon with high dimensional data considering spatial temporal data often Cases where uncertainty is not relegated to an undesirable aesthetic instead incorporate interactivity to allow users to explore the complicated space themselves [@Potter2009; @Potter2009a]. Even the literature about uncertainty communication expresses an implicit belief that it is of secondary importance to the estimates or context of the data.

The use of uncertainty in high dimensional environments is especially important in energy data. Large models that incorporate spatial-temporal data from many sources and systems are used to predict energy uses in the short and long term. Understanding how to improve and make better decisions in these models is imperative in both the daily operation of the energy sector as well as the transition from fossile fuels to clean energy. Therefore the energy sector is an incredibly relevant application of research in uncertainty visualisation techniques.

{{< pagebreak >}}

# Thesis Overview 
The overarching theme of my thesis is a change in the way we understand uncertainty visualisations, specifically in the case of communication. This work will be divided into three chapters.

## Chapter 1: A new theoretical framework
Chapter 1 discuss the current state of the research surrounding uncertainty visualisation and describe two fundamental mistakes in the way researchers conceptualise uncertainty visualisations. The first mistake is in the role of distributions in the visualisation framework, where irrelevant distributions are used to answer questions and relevant distributions are ignored. These distribution issues are mistakenly reported as visualisation issues in the literature, ignoring a fundamental statistical problem in the way we visualise uncertainty. The second mistake is a belief that uncertainty visualisation will improve while uncertainty is seen as of low importance. I highlight how a lot of research in imporoving uncertainty visualisation reflects the belief that uncertainty is inherrently unimportant. I suggest that there is no overarching best uncertainty visualisation, but rather uncertainty visualisations should depend on a motivating question. Finally I provide a framework for visualising uncertainty that mitigates these issues by ensuring the visualisation author decides on the motivations and of their graphic, can correctly identify the relevant distribution and the aspects of the distribution that are needed for their motivation; and correctly assigns priority to these apects by using the correct aesthetics.

## Chapter 2: Applications of the framework
Chapter 2 applies this framework and investigates its practical usefullness through experiemnts and discussions with AEMO. The purpose of data visualisation is insight, but due to time limits or other constraints, most visualisation studies use multiple benchmark tests as a substitute for measuring the complicated phenomena of insight [@North2006]. Unfortunately the validity of these results hinge on the large insights we gain from graphics being the sum total of these small incremental insights which may not always be true [@North2006]. Specifically in uncertainty visualisation, there is a focus on performance and accuracy based measures that assume more predictabe behaviour from people than what research on human decision making suggests. The work in Chapter 2 should avoid these common pitfalls that arrise from experimental plot evaluations by working closely with people at AEMO. Our partners at AEMO will be able to provide detailled and open ended descriptions of the beenfits and struggles of our uncertainty visualisation techniques. This allow us to directly see the improvements (or lack thereof) in insight due to the suggested framework.

## Chapter 3: Translation
Chapter 3 will be a translation of chapter 1 and 2 into an R package. This will make this research more accessible and allow others to easily implement this visualisation framework in their own work.

{{< pagebreak >}}

# Chapter 1: A new theortical framework
## Two problems in uncertainty visualisation research
### An overview of the current ideas
Current research in visualising uncertainty seems to have a focus on designing new options for visualising uncertainty for specific types of data. Typical papers comparing visualisation methods seem to fit into one or both of the follow categories:

1) a paper that suggests a new visualisation method for a specific type of data  
2) a paper that compares two existing methods to idenfiy which is better.  

Generally the goal seems to be to increase the number of options we have in our "visualisation bag" so that we always have a plot at the ready if we want to visualise uncertainty. These new plots also tend to focus on a catch all plot that is "best for making decision" or "best for visualising spatial uncertainty". Data visualisation is commonly utilised as a tool in data exploration, so it is not uncommon for a data analyst to make a plot with only a vague goal and use it to pull out a large number of adjacent observations. For example, a data analyst might make a scatter plot to find out the relationship between two variables, but in doing do also finds out that one variable has discrete outcomes over a range of values. It is common knowledge that any single plot cannot reveal all information about a data set, all plot types obfuscate and uncover different types of information, however this aspect of plotting becomes more apparent in uncertainty visualisation. Often when discussing "uncertainty" information, we expect readers to be able to draw information from a plot that was not estimated, prioritised, or visualised. Communicating uncertainty can be boilled down into two simple steps, first we need to quantify the uncertainty, and second, we need to communicate it [@Webster2003]. The two issues I have noticed in uncertainty visualisation literature each come from one of these steps. The first issue is failing to identify the correct distribution (quantifying) and the second is failling to select a visualisation that highlights the important aspects of the distribution (communicating). These issues run deep in the literature, but they are easiest to understand with an example that I will repeatedly return to as we develop this idea.

### The example: comparing HOPs, error barss, and violin plots
The issues that are rampant in the uncertainty visualisation literature are easily seen if we zoom in on one example. The study done by @Hullman2015 is a great illustration in the importance of having a clear motivation when you design a graphic. It is important to keep in mind that while I am primarily discussing one paper to illustrate a point, the issues I am brining to light are the standard in the uncertainty visualisation literature. This paper is no outlier.

The study by @Hullman2015 asked participants to provide some numerical properties of a distribution using a hypothetical outcome plot (HOPs), an error bar plot or violin plot. Participants were given questions relating to individual and multiple distributions. When shown a single distribution participants were asked about the mean of the distribution, the probability of an outcome being above some threshold (indicated on the plot with a red dot), or the probability of an outcome being between two given values. When shown two distributions the participants were asked "How often is measurement of solute B larger than the measurement of solute A?", and when shown three distributions "How often is measurement of solute B larger than the measurement of solute A and solute C?". @fig-examples shows an example error bar plot and violin plot for the two distribution case. There was also a high and low variance case for every plot and question combination. The study decided which technique was better using absolute error between the subjects response and the true value. They specified that the error bars are not confidence intervals as they represent the true underlying distribution, not the sampling distribution of the mean. They found that the HOPs reliably outperformed the violin and error bar plots for all the two and three distribution questions, but were no better than the violin or error bar plot in the other univariate cases (and in some cases worse). 

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: fig-examples
#| fig-subcap: 
#|   - "Error bar plot"
#|   - "Violin plot"
#| fig-cap: "An example of the plots shown in the two variable tasks given in [@Hullman2015]. The example question provided with this plot was 'In what percentage of vials is there more of solute B than A (Probability(B > A)?'"
#| layout-ncol: 2

# Load Libries
library(tidyverse)
# Make error bar data
ymin <- c(30, 40)
ymax <- c(70, 80)
smean <- c(50, 60)
solute <- c("Solute A", "Solute B")
error_data <- tibble(ymin, ymax, smean, solute)

# Make error bar plot
ticks <- seq(0,100, 10)
error_data %>%
  ggplot(aes(x=solute)) + 
  geom_errorbar(aes(ymin=ymin, ymax=ymax), width=0.1, 
                linetype="dashed", size=1.4) +
  geom_boxplot(aes(y=smean), colour="dodgerblue") +
  labs(y="Parts Per Million (PPM)")+
  scale_y_continuous(breaks=ticks, limits=c(0,100)) +
  theme_classic() +
  theme(axis.title.x=element_blank(), 
        axis.text.x=element_text(size=15),
        aspect.ratio=1)

# Make violin plot data
n <- 100
soluteA <- rnorm(n, 50, 10)
soluteB <- rnorm(n, 60, 10)
solute <- c("Solute A", "Solute B")
violin_data <- tibble(ppm = c(soluteA, soluteB), 
                      solute =c(rep("Solute A", n), rep("Solute B", n)))

# Make violin plot
violin_data %>%
  ggplot(aes(x=solute, y=ppm)) + 
  geom_violin(fill= "dodgerblue", colour="dodgerblue") +
  labs(y="Parts Per Million (PPM)")+
  scale_y_continuous(breaks=ticks, limits=c(0,100)) +
  theme_classic() +
  theme(axis.title.x=element_blank(), 
        axis.text.x=element_text(size=15),
        aspect.ratio=1)  
```

## Issue 1: visualising the wrong distribution
### Looking at the example
My first issue with the plots in @Hullman2015 study is that the violin plot and error bars are visualising a different distribution to the HOPs plot. The error bar plot and the violin plot in @fig-examples visualise the marginal distributions of solutions A and B and provides no information on the joint distribution. The HOPs plot is similar to a slope graph except instead of connecting the outcomes from A and B with a line, they are connected through frames of an animation. The HOPs, just like a slope graph, depicts a relationship between two varibles, i.e. the *joint* distribution. This key feature can be used to improve upon the HOPs and direct our attention to the distribution that is even more useful for answering this question.

Two alternative graphics that could be used to answer the question "In what percentage of vials is there more of solute B than A (Probability(B > A)?" are provided in @fig-alternatives. The scatter plot depicts the joint distribution of the two solutions and uses colour to highlight the Bernoulli distribution that is more closely aligned with the question. The stacked bar exclusively visualises the Bernouli distribution that describes the event $B>A$ and ignores the joint distribution highlighted by the scatter plot. Through this process of moving from the marginal distributions, to the joint distribution to the bernouli distributionm we moved the information needed to answer the question "What is $P(B>A)$?" from something you needed to calculate in your head (when looking at the error bar plots) to something you can **see** in the bar chart. While this process is illuminating, it is important to avoid whittling down the problem **too** much. Providing a categorical decision alone is somewhat useless, it is important to ballance advice with uncertainty estimates as a ballance of the two results in the most accurate decisions [@Joslyn2012]. 

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: fig-alternatives
#| fig-subcap: 
#|   - "Scatter Plot"
#|   - "Stacked Bar Chart"
#| fig-cap: "Two plots that could be used as alternatives to answer to question 'In what percentage of vials is there more of solute B than A (Probability(B > A)?'. The scatter plot in (a) focuses on the relationship between the concentration of solute A and solute B, while the stacked bar chart in (b) highlights the frequency with which each solute is greater than the other."
#| layout-ncol: 2

# Scatter plot data
scatter_data <- tibble(soluteA = soluteA,
                       soluteB = soluteB) %>%
  mutate(biggerb = ifelse(soluteB>soluteA, "Yes", "No"))

# Scatter plot
scatter_data %>%
  ggplot(aes(x=soluteA, y=soluteB)) +
  geom_point(aes(colour=biggerb)) +
  labs(x="Solute A Concentration (PPM)", 
       y="Solute B Concentration (PPM)") +
  guides(colour=guide_legend(title="Is B>A?")) +
  scale_y_continuous(breaks=ticks, limits=c(0,100)) +
  scale_x_continuous(breaks=ticks, limits=c(0,100)) +
  theme_classic() +
  theme(aspect.ratio=1)  

# Make bar plot data
bar_data <- scatter_data %>%
  mutate(biggersolute = ifelse(biggerb=="Yes", "Solute B", "Solute A")) %>%
  select(-biggerb)

# Stacked bar chart
bar_data %>%
  mutate(dummy = "d") %>%
  ggplot(aes(x=dummy, fill=biggersolute)) +
  geom_bar() +
  labs(y = "Proportion") +
  guides(fill=guide_legend(title="% of draws that will have more of...")) + 
  scale_y_continuous(breaks=ticks, limits=c(0,100)) +
  theme_classic() +
  theme(axis.title.x=element_blank(), 
        axis.text.x=element_blank(),
        axis.ticks.x = element_blank(),
        aspect.ratio=4/2)  
  
```

### The issue more broadly
This important distinction between the distribution displayed and the distribution required to answer a question is often ignored in our discussions good or bad uncertainty visualizations. This means that studies identifying some graphics as better than others may only do so because the two plots displayed different distributions. @fig-distdraw illustrates how we think about distributions when performing statsitical tests compared to when we create visualisations. When we compute statistics or perform a hypothesis test, we typically put a lot of thought into what the correct distribution is for our question, however, when we perform visualisation we typically plot a collection of normal marginal distributions and ignore the actual distribution required. This extends to our perception of visualisation in general as most of the visualisation we consider to be for "distributions" are actually just tools for visualising marginal distributions. The relationship between plots and the distributions they present are not entirely alien to the field, @Wickham2011 discusses product plots and how different displays result in depictions of different marginal, conditional, and joint distributions. Unfortunately discussions like this are sometimes lost in later research comparing the use of the graphic.

![Illustration depicting the difference between the distributions we use for testing vs the visualisation we visualise](incorrectdistributions.jpeg){#fig-distdraw}

An example that might be more familiar to the average statistician is something I like to refer to as "the error bar problem". If you have ever looked at two overlapping error bars and said "oh these variables are not statistically significantly different" you have used error bars incorrectly. While it is true that error bars that do not overlap implies statistical significance, overlapping error bars do not imply the converse in true. The same is true for a lot of the other ad-hoc statistical tests we use error bars for. A study done by @Bella2005 asked participants to adjust two error bars until the means were "just" statistically significantly different, and most people adjusted the error bars until they were just touching. In the case of independent confidence intervals that just touch, the p-value of the associated two-tailled t-test is about 0.006 [@Schenker2001]. @Bella2005 also found that few people could incorporate changing information about independence that arrises from repeated measure design and most participants were ignorant to the fact that error bars are used for both confidence intervals and standard error bars, two wildly different indicators of precision. 

This is a classic example of expecting readers to draw conclusions about a distribution that was not visualised. Error bars typically represent the 95% confidence interval of a sampling distribution, most commonly the significant values of a t-distribution. This means that each error bar provides the range of significant values through the two end points, and a vague indicator of variance with the length of the bar, *of each variable independently*. What an error bar does **not** depict is the t-distribution associated with a difference of two means, the equivalent statistical test we utilise error bars for. The visualisation itself is not the problem, trying to draw conclusions that requires information that was not visualised is.

The papers that go on to cite these misunderstandings about error bars discuss the work as though the problems are caused by error bars themselves. They suggest that error bars should be avoided as a visualisation tool, ignoring the fact that these fundamental misunderstandings in uncertainty will likely follow other visual encodings of t-distributions. Maybe some visual aspect of error bars encourages these extrapolations, but the HOPs example I drew on at the start of this chapter illustrates that this problem of expecting people to answer questions that are not directly related to the visualised distribution is larger than questions about significance.

This view of understanding uncertainty visualizations opens up a new gap in the literature. For example, if we accept that visualizing a collection of t-distributions is not a substitute for a collection of F-tests or paired t-tests, how *should* we visualise uncertainty if we want to draw those conclusions? Rather than shutting down the discussion on the usefulness of visualisation, I beleive it opens it and highlights areas of improvement in our current work.

### The scope of the problem
Expecting readers to draw information about a distribution that was not is not the only problem with quantifying uncertainty, but it is the apsect of the issue with a clearer solution. An adjacent issue is *how much* uncertainty we should include when trying to quantify our associated distribution. Obviously the correct answer is somewhere between "every possible outcome" which would result in an unbounded uncertainty and "a simple confidence interval based on a set of strict and unrealistic assumptions" that would result in an interval that is far too narrow. Unfortunately between those two extremes the solution is largely a judgement decision which can sometimes be overwhelming. This is why software that provides *some* uncertainty visualisation as a default, such as forecasts in the `fable` package are useful [@fable]. It prevents authors omitting uncertainty through inaction. There is the possibility, however, that default uncertainty visualisations facilitate the poor understanding of which conclusions are relevant to the uncertainty visualised.

This issue is not helped by the fact that the term "uncertainty" lacks a commonly accepted definition in the literature. @Lipshitz1997 even commented that “there are almost as many definitions of uncertainty as there are treatments of the subject” . This mishmash of terminology leads to a large body of work, all claiming to finding the best visualisation or expression of of "uncertainty" but most don't even seem to agree on what uncertainty is. The most encompassing definition of uncertainty I have seen comes from @utypo who define uncertainty as **"any deviation from the unachievable ideal of completely deterministic knowledge of the relevant system"**. This definition does not completely align with the distribution conceptualisation I discussed earlier in this chapter, but the cases where they diverge are rare and can still be handled by focusing on the relevant information. More commonly, uncertainty is defined using a taxonomy rather than a strict definition. There are a few of taxonomies for uncertainty, but, just like the definition, most of them are a subset of the one laid out  by @utypo.

@fig-taxonomy is an illustration of the taxonomy presented by @utypo. In this taxonomy, there are three things we need to consider for each "uncertainty" we encounter through the modelling process. First, consider the source of the uncertainty. Is this uncertainty coming from inaccurate measurements or a poorly defined model? This is the *location* of the uncertainty. Second, consider how well you can quantify this uncertainty. Do you know exactly how much measurement error there is in each observation or are you not even aware if there is or isn't measurement error? This is the *level* of your uncertainty, and it ranges from discrete to total ignorance. Finally, consider how this uncertainty came into existence. Is it a result of a naturally random process (epistemic) or is it due to imperfect information and could be improved (aleatory). This is the *nature* of your uncertainty. @utypo then go on to describe mapping our uncertainty in a 3D space that is defined by its location, level, and nature, but I think the taxonomy is more easily understood as a series of questions we need to consider when asking how much unertainty we should express to our audience.

![Illustration of the taxonomy described in @utypo](taxonomyvis.jpeg){#fig-taxonomy}

While information about the sources of our uncertainty and the type of uncertainty may seem like an unimportant secondary step in uncertainty visualisation, communicating these features of uncertainty helps decision makers make more informed choices. @Padilla2021 found that low forecaster confidence or high model uncertainty both contribute to more conservative judgements by decision makers. Failling to communicate the nature of your uncertainty can result in underestimation or overestimation of failure probabilities [@Kiureghian2009]. Additionally @Gustafson2019 found that the framing of our uncertainty, (i.e. informing the reading if the uncertainty came from a lack of knowledge, approximations, unknown unknowns, or disagreement among parties) does not have a significant effect on the belief in the estimates, perceived credibility, or behavioral intentions of the decision makers. This means communicating secondary information about your uncertainty is unlikely to negatively affect your communication and is important in understanding the scope of your uncertainty.

## Issue 2: visualising the correct features
### Returning to our example
Let us return to the study by @Hullman2015 to discuss the second issue with the comparison. In discussing the concept of a "best" visualisation, we rarely discuss which *features* of the distribution are being displayed in the graphic. The way we currently look at visualisation would classify the error bar plot and the violin plot as visualisations of a "distribution", the scatter plot is a visualisation of a "relationship" while the bar plot is a visualisation of "amounts" (@wilke2019fundamentals), but this categorisation hides a lot of important details about drawing information from a graph. In this example the violin plot and the scatter plot both showed that each solution had an independent marginal normal distribution, the error bar (although technically a plot for visualising distributions) gives no concept of mass and would not give you the ability to identify even a simple distribution. Not only is it important to select a distribution that is appropriate for our question, it is also important to *show* the aspect of that distribution that holds the relevant information. The example asked about the *frequency* of a particular outcome which translates to visualising the *outcomes* of the joint distribution **or** the *parameter* of the Bernoulli distribution since these are the aspects of each distribution that hold the information we are looking for. If we visualised features of the distribution that don't hold the information we are looking for, the information is difficult to assertain and is more likely to be found through potentially faulty heuristics.

@fig-bad depicts two graphics that have the of the same distributions as those in @fig-alternative, but each plot visualises an aspect of the distribution that is less relevant to the question. The 2D error bar plot (or circle?) highlights the mean and the values that are within the 95% confidence range. Since none of the parameters of the joint distribution are directly related to the question at hand, visualising the mean and significance instead of the outcomes made it harder to answer "What is $P(B>A)$?". Since the parameter of the Bernouli distribution *is* directly related to the question, visualising outcomes makes it harder to answer the question.

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: fig-bad
#| fig-subcap: 
#|   - "2D error bar plot"
#|   - "Scatter plot of bernouli"
#|   - "alternative to 2d error bar plot"
#| fig-cap: "Two plots that are not useful in answering the question 'In what percentage of vials is there more of solute B than A (Probability(B > A)?'. While the distributions visualised are correct, we have visualised the incorrect feature of the repsective distributions. Plot (a) visualises mean and significance thresholds of for values (b) the."
#| layout-ncol: 2

# 2D error bar plot
ticks <- seq(0,100, 10)
tibble(x=50,
       y=60) %>%
  ggplot(aes(x,y)) + 
  geom_errorbarh(xmin=30, xmax=70) +
  geom_errorbar(ymin=40, ymax=80) +
  geom_point() +
  geom_abline() + 
  scale_y_continuous(breaks=ticks, limits=c(0,100)) +
  scale_x_continuous(breaks=ticks, limits=c(0,100)) +
  labs(y = "Solution B Parts Per Million (PPM)",
       x = "Solution A Parts Per Million (PPM)")+
  theme_classic() +
  theme(aspect.ratio=1)

# Outome Bernouli Plot
scatter_data %>%
  ggplot(aes(x=biggerb, y=0, colour=biggerb)) +
  geom_jitter(width=0.4, height=0.4) + 
  scale_y_continuous(limits=c(-1,1)) +
  labs(x = "Is B>A?")+
  theme_classic() +
  theme(aspect.ratio=1,
        axis.title.y = element_blank(), 
        axis.text.y=element_blank(),
        legend.position = "none",
        axis.ticks.y = element_blank())

# CIRCLE PLOT
library(ggforce)
ticks <- seq(0,100, 10)
tibble(x=50,
       y=60) %>%
  ggplot(aes(x,y)) + 
  geom_circle(aes(x0=x, y0=y, r = 20))+
  geom_point() +
  geom_abline() + 
  scale_y_continuous(breaks=seq(0,100, 10), limits=c(0,100)) +
  scale_x_continuous(breaks=seq(0,100, 10), limits=c(0,100)) +
  labs(y = "Solution B Parts Per Million (PPM)",
       x = "Solution A Parts Per Million (PPM)")+
  theme_classic() +
  theme(aspect.ratio=1)
```


### The four features of a distribution
Outcomes and Parameters are obviously not the only features of our distribution. The apsect of a distribution that are typically depicted in graphics can be organised into four key features. @fig-features depicts these four features along with an example of how they are typically depicted in graphics. Each of these four features have a set of questions they are better equip to answer. The four features are:

1) **Mass** describes the PMF/PDF or CMF/CDF of the functions. Depictions of mass can inform us of the mode, likely or impossible values, and whether or not we have an identifiable distribution. There is a connection between outcomes and mass because if you show enough outcomes and introduce opacity a plot out outcomes will start to look like a plot depicting the mass.

*Should I merge outcomes with mass???*

2) **Outcomes** are a set of actual or simulated outcomes of a distribution. Our data falls into this category as it can be seen as an outcome of some "underlying" distribution. Outcomes can also be simulated through techniques such as bootstrapping or random sampling. Outcomes are useful to answer questions about frequency, however since outcomes have a connection to mass (depending on the structure of the graphic, a mass plot may just be a smoothed plot of a large number of outcomes) they can sometimes be used to answer the same questions.

3) **Parameters** are the statistics that are related to our distribution. They can be the sufficient statistics of the distribution, such as the mean, variance, minimum and maximum, or they can be other statistics such as the or correlation, median and mode. Visualising a specific parameter of our distribution typically gives us freedom because it allows us to express any aspect of a plot in terms of a single value. Unfortunately this flexibility means that the set of question questions a parameter plot can answer are limited. Questions about the mean, median, maximum, minimum, correlation, values of significance, etc would usually need multiple plots to answer.

4) **Exchangability** illustrates whether or not a elements in a sequence of random variables from this distribution can be swapped. Rather than answering questions about exchangability, it is something we typically want to highlight in our data. Exchangability is often expressed by having features touch in a graphic. A time series inechangability is expressed using a line plot, spatial data's inechangability is expressed using a map and the exchangability of randomly sampled data is often expressed using points. Exchangability is adjacent to continuity because visually connected features in a graphic are also used to express discretised outcomes of a continuous process (such as a ridgeline plot).

![The four main features of a distribution graphics are typically used to represent](distfeatures.jpeg){#fig-features}

It is important to note that this list if not exhaustive but rather the four most commonly visualised aspects of a distribution. Each of these features have can be ignored, set as a high priorty, or set as a low priority in any particular graphic. Any particular hierarchy of importance for aspects of a distribution can be mapped to the well researched hierarchy of aesthetics of a plot. 

### Mapping the features to the hierarchy
*Hierarchy*
*Natrual Mappings*
There are several 

- Frequency based questions are easily answered with a HOPs plot OR ANY PLOT OF HYPOTHETICAL OUTCOMES. 

- High dimensional visualisation techniques also can be viewed through this frameoworok and is the reason I find the tour to be such a valuable tool in visualisation. The tour using because it tries to give a view of the overarching joint distribution. A parallel co-ordinate plot and a SPLOM both provide a look at all of the bivariate joint distributions, or a subset of them.

- [@Buja2009] [@Wickham2010] suggests the lineup protocol to test if what we see in a plot is actually there. Esentailly makes an uncertainty visualisation of plots themselves using hypothetical outcomes.
- [@Chowdhury] shows use of infoviz to show distribution of high dimensional hard to conceptualise situation of plots of low dimensional projections of data.
- [@Hofmann2012] Infoviz is directly comparable to statistical tests and the ways we use graphical elements can have an impact on the power of the test. This paper tries to estimate the power of a statistical test performed using infoviz. It compares polar and cartesian co-ordinates and finds that polar co-ordinates are a higher powered test. It also compares multiple ways of showing distribution to see which has more power.
- [Grewal2021] looks categorising uncertainty visualisations based on at how current visualisation methods compare based on x=discreetness of uncertainty and y=how field specific the audience is.



- parameters (location, other variables, etc)



### High dimensional uncertainty visualisation




*(Hierarchy)*
- INATTENTIVE BLINDNESS
- the visualisation we use to find information out for ourselves do not need to be the same visualisations we use to communicate those interesting findings. using one graphic to get a lot of information makes sense in the exploratory stage, but less so in the communiating stage. Our goal is not to present a lot of information so that we might find the information we want, it is to give the relevant information for a deicison or for understanding. Instead of making visualisations that tell us interesting features, we need to pick visualsiatin that highlight the interesting features we have found.
- when you use plots for EDA you often will make hundreds to try and find interesting features, when you present a plot for communication or decisions, you can only show a small handful.
There are only so many aesthetics we can map information to, and those aesthetics have a hierarchy of how easy they are the process. Additional secondary information can simultaneously give important context and clutter the visualisation, so it is important to understand our motivations when creating a visualisation. 



What should become apparent comparing these plots is that there is a constant trade off between information relevant to the our question and secondary information that provides context. 


The primary issue I have with this study (and by extension most visualisation studies) is the philosophy driving the motivation. An error bar plot is good at displaying a range of statistically significant values. A violin plot is good at depicting the frequency of outcomes of a distribution. Both can be used to get a general idea of the variance or central value of a distribution, but the significant values, and the frequency of individual values is not all the information we might need from a distribution. While an animated HOPs plot was better at depicting probability, or frequency of an outcome, than an error bar plot, that does not mean it is the "best" at answering the question "In what percentage of vials is there more of solute B than A (Probability(B > A)?". @fig-alternatives depicts several plots that I actually think would do a better job than the HOPs plot used in the experiment.


*HIERARCHY DISCUSSION HERE* Lets take a look at the accuracy rank of an error bar.
- placebo rule, studys compare to placebo is bad, should compare to next best alternative (should compare plots that have the feature of interest at a similar importance ranking)
- paper Wanted to get a better understanding of peoples ability to assess if A>B, rather than asking a binary question about significance.

![Explicit highlighting of what information is contained in the error bar plot and high important the plot communicates that information to be](feature.jpeg)

*ERROR BAR HIERARCHY EXAMPLE HERE* This is why the conversation and research around error bars is very frustrating to read. 
- I think this issue is largely why error bar plots get a lot of flack as being a bad plot, I think they are just being misused. 
 - In the paper, Hullman does acknowledge that error bars are typically used to identify significance, but
- people couldn't answer that question with ANY DISTRIBUTION PLOT
- Ask yourself what questions can be answered with each aesthetic of the plot. Is this the hierarchy of information you want to express.
- mapping the most important feature to the highest aspect
- So after zooming in on the uses (and misuses) of error bar plots, it should be clear to see why error bar plots *are* valid visualisations of significance
- First of all, the study ignores the questions that would be ideal for an error bar or violin plot.
- they assume that people first make some probability estimate (they find P(A>B)) and then use that to assess significance. 
- Statistical significance is related to sampling distributions which they didn't care about
- "If there is special interest in knowing whether people make qualitative errors , we can set various qualitative errors such as 0.95 during the analysis phase and count the frequency of errors rather than their average magnitude" or just make an error bar plot.
- They argue that a normal distribution was the most favorable condition for the error bar and violin because it leads to a symmetric plot where the widest point is also the mean (so you acknowledge that violin plots are better at expressing the mode of a distribution???)
- There is also some evidence in their results that a question so removed from the purpose of the plot just confuses people. "In light of the common usage of error bars for presenting multiple independent distributions, it is noteworthy how poorly subjects using these representations performed on tasks asking them to assess how reliably a draw from one variable was larger than the other", Many subjects reported values less than 50%, which are not plausible given that the mean of B was larger than the mean of A." Did not find any obvious patterns of errors. "We speculate that many subjects simply had no idea how to make a good guess"


*INCORPERATING CONTEXT HERE*
- It is important to avoid highlighting irrelevant aspects of the plot as that will confuse viewers and make it harder for them to draw conclusions. 
- [@Kale2021]. Adding means to uncertainty visualizations has small biasing effects on both magnitude estimation and decision-making, consistent with users relying on visual distance between distributions as a proxy for effect size visualization designs that support the least biased effect size estimation do not support the best decision-making, suggesting that a chart user’s sense of effect size may not necessarily be identical when they use the same information for different tasks.
- [@Nathonours] finds that a scatter plot is better than a line plot if you want to convey the correlation between two time series. Considering the correlation is a feature of the joint distribution, it makes sense that a depiction of the joint distribution (a scatter plot) outperformed between the two time series, and a line plot is the marginal distribution of each time ser (its also like a flipped slope graph?)
- With this example in mind, we now understand both the distribution we visualise and the method we use should be directly related to the uncertainty we want to ask questions about. These are the key points we need to keep in mind every time we visualise uncertainty, however even the things that this paper got right (i.e. in order to answer questions about an uncertainty distribution we need to show the uncertainty distribution) get lost when researchers try to reintroduce the context of the data. Papers discussing new ways to visualise "temporal uncertainty" or "spatial uncertainty" seem to forget the uncertainty is related to a distribution, not a type of data. If I ask you to explain to me what "temporal uncertainty" is, you may try and describe uncertainty related to a forecast, or variance of a variable, or even model residuals. These are not "temporal uncertainty", they are the distribution of a prediction, a variable, or of a model. None of these things are specific to temporal data. This may just sound like me being pedantic about poor naming, but this incorrect framing of uncertainty leads to very inconsistent research. Because we view uncertainty as a secondary feature of the data, and not as something we have constructed, we don't think about uncertainty visualisations with the "goal of the uncertainty" in mind.
- The people who don't plot uncertainty because they think its unimportant and the people who relegate uncertainty to the lowest ranking aesthetic on the list are both saying the same thing, "I think the uncertainty is unimportant". Including uncertainty is worth very little if its impossible to interpret and the last thing I notice in the plot. 
- You can chose to include the temporal aspect of the plot in some capacity, however it is not by accident that there is a general struggle to visualise uncertainty when there are so many competing ideas. This changes the way we view problems such as visualising spatial uncertainty. The question of how to constuct uncertainty plots for complicated data types (such as spatial data) is no longer driven by desiging an abstract uncertainty visualisation method for spatial data. The goal would be to find the best way to combine the most appropriate uncertainty visualisation for the relevant question and the best plot for the type of data. These methods may be able to be combined onto a single plot, however there may be cases where two different plots (one for the data and one for the uncertainty) is more appropriate and effective. 
- There is an unspoken assumption prevailing in uncertainty visualisation that the uncertainty distribution associated with one hypothesis can just be swapped out for another. An assumption that the literature itself seems to disprove since there are many papers that are "contradictory" if you view them through the lense of finding a best plot, but strangely consistent when you reframe the conclusions through the lense of the "best plot for this hypothesis". Unfortunately this evidence is almost always used to highlight a failing in the plot, rather than a failing in the question.
- people being able to identify uncertainty is not what is important because 1) all these perform worse than two side by side plots, 2)
- check what kind of question they can answer

*SPATIAL UNCERTAINTY EXAMPLE*
- [@Wickham2012] Glyph maps are a good way to visualize spatial temporal data (read: not uncertainty). The ordering is determined by space, and the glyph is a line plot. Different scales allow us to look at trends in global variance, local variance. Aggregation allows investigations of trends.
- [@Correll2018] They make value-suppressing uncertainty palettes. Suggest using it to endocde the uncertainty in the colour apect of the graph (should be directly integrated in a shared chart). Found that Juxtaposed maps, by introducing a second search task to the identification task (searching for the proper values in two, rather than one map), would have poorer performance than other conditions. (read the paper to find out details)
- [@Lucchesi2017] This paper is discusses different ways to present uncertainty in a map. A lot of them made it easy to SEE that there was uncertainty, but it was still difficult to compare across locations and tell if areas are statistically the same (which I would argue is a large part of uncertainty). Rotation is pretty useless, the pixelation was interesting but had above issue, the grid colour wasnt a bad idea, but I wouldnt use a second colour in the scale (if i did use diverging palet, it would be on the statistic).

## (Other examples) Tying the reasons together
This is a large reason why the current literature on uncertainty distributions (specifically with respect to error bar plots) is so misguided. The outcome of many visualisation papers can be reconsided when you think about the underlying distributions selected and the hierarchy of information provided.
- I argue that the problem is not that people are incorrectly reading error bars, but that error bars are being incorrectly used.

Two themes in these papers
- Relevant features should be of primary importance
- Irrelevant features distract and lead to bias

*Collection of Visualisaion papers*
- [@Kobakian]hexagon tile map was significantly more effective for spotting a real population related data trend model hidden in a lineup.
- [@Gschwandtnei2016] Could have to do with making sure you match your question with an intuitive plot feature. The paper looks at different types of temporal uncertainty and discusses the differences and how to best visualise each one. Has some interesting methods to visualise intervals. Gradients were best for probability and ambiguation was best for start and end time estimation
- [@Fernandes2018] This paper has several visualisation specific findings. We found that cumulative distribution function (CDF) plots and low-density quantile dotplots pro- duce more accurate and consistent decisions compared to other uncertainty visualizations, textual displays, and displays with no uncertainty. The types of displays that we have found to be the best for supporting decision-making in the transit have also been shown to be more accurate at estimating probability intervals more generally. Alternatively their question was best answered with the CDF (what is the probability I will miss the bus if I leave now). Reflective with real costs and benefits which are not always quantifiable, they have to weigh those up on their own.
- [@Padilla2022] visualizing the same data in different ways had differential effects on the changes we observed in risk perceptions. Cumulative y-axis will most reliably lead to people perceiving greater pandemic risk because cumulative y-axis can show only an upward or flat trend. In contrast, incident y-axis can be highly variable and even reduce viewers’ perceived pandemic risk, if the charts exhibit a downward trend. Basically aspects on the plot other than the the distribution and hierarchy also have an effect on the way the plot is percieved (and what information is highlighted)
- [@Vanderplas2015] plotting features can work for you or against you. The sine illusion causes bars on a sine curve to seem longer at the peaks and troughs (because people view the perpendicular area). This means increases in variance in a time series plot are harder to identify if they are paired with an increase in slope. The illustion is very pervasive and the you have to go to extreme measures to remove its effect.
- [@Maceachren2012] The first paper organises different types of uncertainty and then matches each type of uncertatiny to a visual aspect of a graph. Experiment 1 tested the extent to which symbols have intuitive applications. Participants response times were the same across symbols, etc. Found fuzziness, location, (and sightly less value) were the best for visualising uncertainty. Arrangement, size and transpacency slightly less so. Saturation, hue, orientation and shape are unacceptable. Only one direction was deemed intuitive by participants (fuzziness: more fuzzy=less certain; location: further from center=less certain; value: lighter=less certain; arrangement: poorer arrangement=less certain; size: smaller=less certain; transparency: more obscured=less certain). While iconic sign-vehicles can be more intuitive and more accurately judged when aggregated (than are abstract sign-vehicles), the abstract sign-vehicles can lead to quicker judgments.  Experiment 2 assessed task performance when multiple symbols appeared.
- [@Hullman2018] Visualising uncertainty as a set of discrete outcomes improves recall of sampling distribution.

### Sorting the questions and experiments


*Final Paragraph*
Why do we need a plot that can "see" everything? Plots are similar to people in the sense that expecting one person to have a complete and flawless view of something as complicated as society ignores that one of the things that has allowed humanity to flourish is that there are so many of us. It allows each person to specialise and be good at something important to them rather than trying to be mediocre at everything. Similarly, no plot can see every possible thing you might want to know about your data. Leaning into and uncovering the strengths of each plot is the best way to research them because it reflects the ways in which we actually incorperate plotting into our diagram. While general principals are helpful, fundamentally each plot for each question should be as unique as the person who made it.

{{< pagebreak >}}

# Timeline
A timetable for completing the thesis and a statement of progress to date (make sure to match everything to apsects of the thesis)
- (done) Literature review (have made progress in this year) have picked up main parts of the infoviz community - changed topic once got new scholarship (check off to say completed)
- can pencil in energy aspect now (met with them earlier in the year) - have a need for this work 
- next meeting in June (and have regular meetings monthly over the next year and hopefully build up applications)
- Have idea of when I think I will have a finished piece of work (literature review)
- Content for each of the pieces should be mapped out
- ASC later in year (submitted an abstract for a poster)
- getting design of experiment clear and question we want to answer clear can go round and round (good idea to have something simple to test) - dont have time for something too complicated in phd need to have it in small managable pieces
- ASC december 2023 - apsect of literature review + results from preliminary experiemnt
- IEEE VIS (June 22 2023 poster abstract) - Conference October 2023 (present aspect of literature review)
- The Rstudio conference (posit conference) - hard to get a spot (worth applying for)
- Aim for something international in a year (or in last year)
- Also could do UseR! 2024 (or 2025) & talk about some of the software developed (technical output) 
- Could have an energy specific one (various applied energy)
- IEEE VIS (march 31st paper due in 2025) - Conference October 2025 (Vienna)
- Thesis due December 2025

{{< pagebreak >}}

# Bibliography

```{r, include=FALSE, eval=FALSE}
library(spelling)
qmd <- "index.qmd"
ignore <- readLines("WORDLIST")
check_spelling <- spell_check_files(
  qmd,
  ignore = ignore,
  lang = "en_GB"
)
if (nrow(check_spelling) > 0) {
  print(check_spelling)
  stop("Check spelling in Qmd files!")
}
```