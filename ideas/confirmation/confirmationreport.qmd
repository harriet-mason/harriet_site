---
title: "Plotting Apples, Oranges, and Distributions"
subtitle: "A New Taxonomy to Prevent Information Inequalities In Uncertainty Visualisations"
author: Harriet Mason
affiliations:
      - name: "Supervisors: Di Cook, Sarah Goodwin, Emi Tanaka, and Ursula Laa"
bibliography: references.bib
date: last-modified
format: pdf
editor_options: 
  chunk_output_type: console
---

```{r}
#| echo: false
#| warning: false
#| message: false

# Load libraries
library(tidyverse)
library(tsibbledata)
library(Vizumap)
library(fable)
```

# Motivation
Think back to the last time you made some sort of data visualisation. What was the purpose of that visualisation? Was it to better understand your data? Was it to help you make a decision? Was it to to communicate that decision to someone else? Now think about the last time you expressed uncertainty. Was it in a table of estimates or was it in a visualisation? There are many stages in our analysis that benefit from the power of data visualisation, however this does not mean it is always done with success. Visualization is an important step in exploratory data analysis and it is often utilised to **learn** what is important about a data set. The importance of data driven discovery is highlighted by data sets such as Anscombe’s quartet [@anscombe] or the Datasaurus Dozen [@datasaurpkg]. Each of the pairwise plots in these data sets have the same summary statistics but strikingly different information when visualised. The anscombe quartet is shown in @fig-anscombe. Instead of having to repeatedly check endless hypothesis to find interesting numerical features, visualisations **tell** us what is important about the data set. This powerful aspect of data visualisation is poorly or seldom used in later stages when we are communicating our findings, specifically with respect to uncertainty. 

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: fig-anscombe
#| fig-cap: "The four scatter plots that make up the anscombe quartet. Each x and y variable has the same mean, standard deviation, and correlation"
tibble(x = c(anscombe$x1, anscombe$x2,
             anscombe$x3, anscombe$x4),
       y = c(anscombe$y1, anscombe$y2,
             anscombe$y3, anscombe$y4),
       Plot = c(rep("Plot 1",11), rep("Plot 2",11), 
                rep("Plot 3",11), rep("Plot 4",11))) %>%
  ggplot(aes(x,y)) +
  geom_point(aes(fill=Plot), colour="black", 
             size=3, pch=21, alpha=0.75) +
  facet_wrap(~Plot) +
  theme_classic() +
  theme(aspect.ratio = 1,
        legend.position = "none") +
  scale_fill_brewer(type = "qual", palette = 4)
```

Utilising visualisation can give people a more complete understanding of risks. Studies asking participants to sketch a distribution allowed them to better compute statistics about that distribution and improve predictions [@Hullman2018; @Goldstein2014]. While there is some evidence that confidence provided in text form only are less likely to be misinterpretated than grahics [@Savelli2013], text is insufficient to express more complicated aspects of a distribution, such as mass. The confusion caused by visualisation could also be due to a lack of expose, since @Kay2016 found that people exposed to the same uncertainty visualisation get better at making judgements the more they are exposed to them. Additionally, visualisation allows for interactive graphics that provide a more in depth understanding of probability [@Potter2009a; @Ancker2009] and infographics that make uncertainty more accessible for people with poor numeracy skills [@Ancker2009]. 

Despite these benefits, there is a reasonable amount of annecdotal and survey evidence that we don't visualise uncertainty as often as we should. Some economists suggest that visualisation authors are responding to incentives that make it tempting to avoid visualising uncertainty, even if those incentives are based more in perception than reality [@Manski2020]. A survey conducted by @Hullman2020a found that majority of visualisation authors agreed that expressing uncertainty is important and should be done more often than it currently is, some even agreed that failing to do so is tantamount to fraud. Despite this, only a quarter of respondents included uncertainty in 50% or more of their visualisations [@Hullman2020a]. Meaning participants were convinced that visualising uncertainty is morally important but were able to provide self sufficient reasoning that allows them to avoid doing it. The study by @Hullman2020a found that the most common reasons authors don't visualise uncertainty in practice despite knowing it's moral importance are: not wanting to overwhelm the audience; an inability to calculate the uncertainty; a lack of access to the uncertainty information; and not wanting to make their data seem questionable [@Hullman2020a].

If decision markers are not presented with the uncertainty about an estimate the data analysts have, for all intents and purposes, made the decision for the decision maker. Upon further interviews @Hullman2020a found that authors believed uncertainty would overwhelm the audience and make their data seem questionable because decision makers are unable to understand uncertainty. This belief, while pervasive, is not true. While some research suggests that laypeople cannot understand complicated concepts in statistical thinking (such as trick questions on hypothesis tests or the difference between Frequentist and Baysian thinking) [@Hoekstra2014; @Bella2005] there is a large amount of research suggesting that presenting uncertainty information improves decision making, both experimentally [@Joslyn2012; @Savelli2013; @Kay2016; @Fernandes2018] and in practice [@Al-Kassab2014]. As a matter of fact, doing what many authors currently do (providing only a deterministic outcome with no uncertainty) causes decision makers to be *less* decisive and have completely unbounded expectations on an outcome [@Savelli2013]. This reality cannot be avoided by providing secondary or non-specific information such as explaining calculations [@Joslyn2012], explaining the advantages of a recommendation [@Joslyn2012], or expressing uncertainty in vague terms [@Erev_1990; @Olson_1997], all of which are undesirable for decision makers and lead to measurably worse decisions [@Joslyn2012; @Erev_1990; @Olson_1997]. Expressing uncertainty verbally additionally decreases the percieved reliability and trustworthiness of the source [@VanderBles2020]. One of the most popular depictions of uncertainty for decision making is a quantile dotplot, shown in @fig-quantdot.

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: fig-quantdot
#| fig-cap: "A quantile dotplot is an uncertainty visualisation that provides a discrete display of uncertainty to aid decision making. This example plot provides an estimate for a daily maximum temperature."
set.seed(1)
dotplot_data <- tibble(temp = 10 + round(rnorm(40, mean=10, sd=2))) 
mid <- round(mean(dotplot_data$temp), 2)
dotplot_data %>%
  ggplot(aes(x=temp)) +
  geom_dotplot(binwidth = 0.75, fill="grey") +
  theme_classic() +
  geom_segment(x=round(mid), xend=round(mid), 
               y=-1, yend=0.5, size=2) +
  geom_label(x=round(mid), y=0.5, label=paste0(mid, "°C"),
             fill = "black", fontface = "bold",
             colour="white") +
  scale_x_continuous(breaks = seq(10, 30, 5), 
                     limits = c(10, 30)) +
  theme(axis.line.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.title.y = element_blank(),
        axis.text.y = element_blank()) +
  labs(x = "Tomorrow's Daily Max Temperature")
```

Not only does communicating uncertainty improve decisions but the mistrust created by communicating certainty in uncertain situations can be exploited. A 6-month survey of anti-mask groups on Facebook during to COVID-19 pandemic showed that the anti-maskers thought carefully about their grammer of graphics and made pursuasive visualisations using the same data as pro-mask groups by exploiting information ignored by the pro-maskers [@Lee2021]. It is understood that deceptive plots can lead viewers to come to incorrect conclusions or significantly overstate effects or risks [@Pandey2015; @Padilla2022] but these incorrect takeaways cannot be mitigated with instructions in how to correctly understand the plot [@Boone2018]. This evidence indicates we are more likely than not to hurt our message when we ignore uncertainty information and trying to raise the general publics plot literacy is an insufficient strategy to curb conspiracy theories and misguided scientific communication.  In direct contrast to this, displaying numerical estimates of uncertainty information has shown to lead to greater trust in predicitions [@Joslyn2012; @VanderBles2020]. While @Han2009 found people have more worry when presented with uncertainty reguarding health outcomes, this worry is not a bad thing if the concern is warranted given the ambiguous situation.

The disconnect between the research supporting uncertainty and the consensus aganinst may not be entirely driven by a lack of understanding of the literature. For example, at least one interviewee from the study by @Hullman2020a claimed that expertise implies that the signal being conveyed is significant, but also said they would omit uncertainty if it obfuscated the message they were trying to convey [@Hullman2020a].  Other authors who were capable of calculating and and representing uncertainty well did not do it, and were unable to provide a self-satisfying reason why [@Hullman2020a]. These conflicting motivations are acknowledged in the paper itself where @Hullman2020a says:

> "It is worth noting that many authors seemed confident in stating rationales, as though they perceived them to be truths that do not require examples to demonstrate. It is possible that rationales for omission represent ingrained beliefs more than conclusions authors have drawn from concrete experiences attempting to convey uncertainty". 

An overwhelming consensus among visualisation authors seems to be that uncertainty is secondary to estimations. There is a belief help by those that work with data that the uncertainty associated with an estimate (the noise) only exists to hide the estimate itself (the signal). From this view, uncertainty is only seen as additional or hindering information, therefore despite its alleged importance, when simplifying a plot uncertainty the first thing to go. This belief is also reflected in the development of new uncertainty visualisations. Often when trying to visualise a high problem, uncertainty is relegated to unimportant aesthetics in the plot, often of lower importance than the estimate [@Correll2018; @Lucchesi2017]. This is not uncommon with high dimensional data considering spatial temporal data often Cases where uncertainty is not relegated to an undesirable aesthetic instead incorporate interactivity to allow users to explore the complicated space themselves [@Potter2009; @Potter2009a]. Even the literature about uncertainty communication expresses an implicit belief that it is of secondary importance to the estimates or context of the data.

An adjacent issue is *how much* uncertainty we should include when trying to quantify our associated distribution. Obviously the correct answer is somewhere between "every possible outcome" which would result in an unbounded uncertainty and "a simple confidence interval based on a set of strict and unrealistic assumptions" that would result in an interval that is far too narrow. Unfortunately between those two extremes the solution is largely a judgement decision which can sometimes be overwhelming. This is why software that provides *some* uncertainty visualisation as a default, such as forecasts in the `fable` package are useful [@fable]. It prevents authors omitting uncertainty through inaction. There is the possibility, however, that default uncertainty visualisations facilitate the poor understanding of which conclusions are relevant to the uncertainty visualised.

This issue is not helped by the fact that the term "uncertainty" lacks a commonly accepted definition in the literature. @Lipshitz1997 even commented that “there are almost as many definitions of uncertainty as there are treatments of the subject” . This mishmash of terminology leads to a large body of work, all claiming to finding the best visualisation or expression of of "uncertainty" but most don't even seem to agree on what uncertainty is. The most encompassing definition of uncertainty I have seen comes from @utypo who define uncertainty as **"any deviation from the unachievable ideal of completely deterministic knowledge of the relevant system"**. This definition does not completely align with the distribution conceptualisation I discussed earlier in this chapter, but the cases where they diverge are rare and can still be handled by focusing on the relevant information. More commonly, uncertainty is defined using a taxonomy rather than a strict definition. There are a few of taxonomies for uncertainty, but, just like the definition, most of them are a subset of the one laid out  by @utypo.

@fig-taxonomy is an illustration of the taxonomy presented by @utypo. In this taxonomy, there are three things we need to consider for each "uncertainty" we encounter through the modelling process. First, consider the source of the uncertainty. Is this uncertainty coming from inaccurate measurements or a poorly defined model? This is the *location* of the uncertainty. Second, consider how well you can quantify this uncertainty. Do you know exactly how much measurement error there is in each observation or are you not even aware if there is or isn't measurement error? This is the *level* of your uncertainty, and it ranges from discrete to total ignorance. Finally, consider how this uncertainty came into existence. Is it a result of a naturally random process (epistemic) or is it due to imperfect information and could be improved (aleatory). This is the *nature* of your uncertainty. @utypo then go on to describe mapping our uncertainty in a 3D space that is defined by its location, level, and nature, but I think the taxonomy is more easily understood as a series of questions we need to consider when asking how much unertainty we should express to our audience.

![Illustration of the taxonomy described in @utypo](taxonomyvis.jpeg){#fig-taxonomy}

While information about the sources of our uncertainty and the type of uncertainty may seem like an unimportant secondary step in uncertainty visualisation, communicating these features of uncertainty helps decision makers make more informed choices. @Padilla2021 found that low forecaster confidence or high model uncertainty both contribute to more conservative judgements by decision makers. Failling to communicate the nature of your uncertainty can result in underestimation or overestimation of failure probabilities [@Kiureghian2009]. Additionally @Gustafson2019 found that the framing of our uncertainty, (i.e. informing the reading if the uncertainty came from a lack of knowledge, approximations, unknown unknowns, or disagreement among parties) does not have a significant effect on the belief in the estimates, perceived credibility, or behavioral intentions of the decision makers. This means communicating secondary information about your uncertainty is unlikely to negatively affect your communication and is important in understanding the scope of your uncertainty.

The use of uncertainty in high dimensional environments is especially important in energy data. Large models that incorporate spatial-temporal data from many sources and systems are used to predict energy uses in the short and long term. Understanding how to improve and make better decisions in these models is imperative in both the daily operation of the energy sector as well as the transition from fossile fuels to clean energy. Therefore the energy sector is an incredibly relevant application of research in uncertainty visualisation techniques.

{{< pagebreak >}}

# Thesis Overview 
The overarching theme of my thesis is a change in the way we understand uncertainty visualisations, specifically in the case of communication. This work will be divided into three chapters.

## Chapter 1: A new theoretical framework
Chapter 1 discuss the current state of the research surrounding uncertainty visualisation and describe two fundamental mistakes in the way researchers conceptualise uncertainty visualisations. The first mistake is in the role of distributions in the visualisation framework, where irrelevant distributions are used to answer questions and relevant distributions are ignored. These distribution issues are mistakenly reported as visualisation issues in the literature, ignoring a fundamental statistical problem in the way we visualise uncertainty. The second mistake is a belief that uncertainty visualisation will improve while uncertainty is seen as of low importance. I highlight how a lot of research in imporoving uncertainty visualisation reflects the belief that uncertainty is inherrently unimportant. I suggest that there is no overarching best uncertainty visualisation, but rather uncertainty visualisations should depend on a motivating question. Finally I provide a framework for visualising uncertainty that mitigates these issues by ensuring the visualisation author decides on the motivations and of their graphic, can correctly identify the relevant distribution and the aspects of the distribution that are needed for their motivation; and correctly assigns priority to these apects by using the correct aesthetics.

## Chapter 2: Applications of the framework
Chapter 2 applies this framework and investigates its practical usefullness through experiemnts and discussions with AEMO. The purpose of data visualisation is insight, but due to time limits or other constraints, most visualisation studies use multiple benchmark tests as a substitute for measuring the complicated phenomena of insight [@North2006]. Unfortunately the validity of these results hinge on the large insights we gain from graphics being the sum total of these small incremental insights which may not always be true [@North2006]. Specifically in uncertainty visualisation, there is a focus on performance and accuracy based measures that assume more predictabe behaviour from people than what research on human decision making suggests. The work in Chapter 2 should avoid these common pitfalls that arrise from experimental plot evaluations by working closely with people at AEMO. Our partners at AEMO will be able to provide detailled and open ended descriptions of the beenfits and struggles of our uncertainty visualisation techniques. This allow us to directly see the improvements (or lack thereof) in insight due to the suggested framework.

## Chapter 3: Translation
Chapter 3 will be a translation of chapter 1 and 2 into an R package. This will make this research more accessible and allow others to easily implement this visualisation framework in their own work.

{{< pagebreak >}}

# Chapter 1: A new theortical framework
## The current landscape of visualisation experiments
Current research in visualising uncertainty seems to have a focus on designing new options for visualising uncertainty for specific types of data. Typical papers comparing visualisation methods seem to fit into one or both of the follow categories:

1) a paper that suggests or compares uncertainty visualisations that depict a single distribution.

2) a paper that suggests or compares uncertainty visualisations for a specific type of data  

Generally the goal seems to be to increase the number of options we have in our "visualisation bag" so that we always have a plot at the ready if we want to visualise uncertainty. These new plots also tend to focus on a catch all plot that is "best for making decision" or "best for visualising spatial uncertainty". Data visualisation is commonly utilised as a tool in data exploration, so it is not uncommon for a data analyst to make a plot with only a vague goal and use it to pull out a large number of adjacent observations. For example, a data analyst might make a scatter plot to find out the relationship between two variables, but in doing do also finds out that one variable has discrete outcomes over a range of values. It is common knowledge that any single plot cannot reveal all information about a data set, all plot types obfuscate and uncover different types of information, however this aspect of plotting becomes more apparent in uncertainty visualisation. Often when discussing "uncertainty" information, we expect readers to be able to draw information from a plot that was not estimated, prioritised, or visualised. Communicating uncertainty can be boilled down into two simple steps, first we need to quantify the uncertainty, and second, we need to communicate it [@Webster2003]. The two issues I have noticed in uncertainty visualisation literature each come from one of these steps. The first issue is failing to identify the correct distribution (quantifying) and the second is failling to select a visualisation that highlights the important aspects of the distribution (communicating). These issues run deep in the literature, but they are easiest to understand with an example that I will repeatedly return to as we develop this idea.

## Part 1: Distribution
### Example: Comparing HOPs, error bars, and violin plots
The issues that are rampant in the uncertainty visualisation literature are easily seen if we zoom in on one example. The study done by @Hullman2015 is a great illustration in the importance of having a clear motivation when you design a graphic. It is important to keep in mind that while I am primarily discussing one paper to illustrate a point, the issues I am brining to light are the standard in the uncertainty visualisation literature. This paper is no outlier.

The study by @Hullman2015 asked participants to provide some numerical properties of a distribution using a hypothetical outcome plot (HOPs), an error bar plot or violin plot. Participants were given questions relating to individual and multiple distributions. When shown a single distribution participants were asked about the mean of the distribution, the probability of an outcome being above some threshold (indicated on the plot with a red dot), or the probability of an outcome being between two given values. When shown two distributions the participants were asked "How often is measurement of solute B larger than the measurement of solute A?", and when shown three distributions "How often is measurement of solute B larger than the measurement of solute A and solute C?". @fig-examples shows an example error bar plot and violin plot for the two distribution case. There was also a high and low variance case for every plot and question combination. The study decided which technique was better using absolute error between the subjects response and the true value. They specified that the error bars are not confidence intervals as they represent the true underlying distribution, not the sampling distribution of the mean. They found that the HOPs reliably outperformed the violin and error bar plots for all the two and three distribution questions, but were no better than the violin or error bar plot in the other univariate cases (and in some cases worse). 

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: fig-examples
#| fig-subcap: 
#|   - "Error bar plot"
#|   - "Violin plot"
#| fig-cap: "An example of the plots shown in the two variable tasks given in [@Hullman2015]. The example question provided with this plot was 'In what percentage of vials is there more of solute B than A (Probability(B > A)?'"
#| layout-ncol: 2


# Make error bar data
ymin <- c(30, 40)
ymax <- c(70, 80)
smean <- c(50, 60)
solute <- c("Solute A", "Solute B")
error_data <- tibble(ymin, ymax, smean, solute)

# Make error bar plot
ticks <- seq(0,100, 10)
error_data %>%
  ggplot(aes(x=solute)) + 
  geom_errorbar(aes(ymin=ymin, ymax=ymax), width=0.1, 
                linetype="dashed", size=1.4) +
  geom_boxplot(aes(y=smean), colour="dodgerblue") +
  labs(y="Parts Per Million (PPM)")+
  scale_y_continuous(breaks=ticks, limits=c(0,100)) +
  theme_classic() +
  theme(axis.title.x=element_blank(), 
        axis.text.x=element_text(size=15),
        aspect.ratio=1)

# Make violin plot data
n <- 100
soluteA <- rnorm(n, 50, 10)
soluteB <- rnorm(n, 60, 10)
solute <- c("Solute A", "Solute B")
violin_data <- tibble(ppm = c(soluteA, soluteB), 
                      solute =c(rep("Solute A", n), rep("Solute B", n)))

# Make violin plot
violin_data %>%
  ggplot(aes(x=solute, y=ppm)) + 
  geom_violin(fill= "dodgerblue", colour="dodgerblue") +
  labs(y="Parts Per Million (PPM)")+
  scale_y_continuous(breaks=ticks, limits=c(0,100)) +
  theme_classic() +
  theme(axis.title.x=element_blank(), 
        axis.text.x=element_text(size=15),
        aspect.ratio=1)  
```

My first issue with the plots in @Hullman2015 study is that the violin plot and error bars are visualising a different distribution to the HOPs plot. The error bar plot and the violin plot in @fig-examples visualise the marginal distributions of solutions A and B and provides no information on the joint distribution. The HOPs plot is similar to a slope graph except instead of connecting the outcomes from A and B with a line, they are connected through frames of an animation. The HOPs, just like a slope graph, depicts a relationship between two varibles, i.e. the *joint* distribution. This key feature can be used to improve upon the HOPs and direct our attention to the distribution that is even more useful for answering this question.

Two alternative graphics that could be used to answer the question "In what percentage of vials is there more of solute B than A (Probability(B > A)?" are provided in @fig-alternatives. The scatter plot depicts the joint distribution of the two solutions and uses colour to highlight the Bernoulli distribution that is more closely aligned with the question. The stacked bar exclusively visualises the Bernouli distribution that describes the event $B>A$ and ignores the joint distribution highlighted by the scatter plot. Through this process of moving from the marginal distributions, to the joint distribution to the bernouli distributionm we moved the information needed to answer the question "What is $P(B>A)$?" from something you needed to calculate in your head (when looking at the error bar plots) to something you can **see** in the bar chart. While this process is illuminating, it is important to avoid whittling down the problem **too** much. Providing a categorical decision alone is somewhat useless, it is important to ballance advice with uncertainty estimates as a ballance of the two results in the most accurate decisions [@Joslyn2012]. 

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: fig-alternatives
#| fig-subcap: 
#|   - "Scatter Plot"
#|   - "Stacked Bar Chart"
#| fig-cap: "Two plots that could be used as alternatives to answer to question 'In what percentage of vials is there more of solute B than A (Probability(B > A)?'. The scatter plot in (a) focuses on the relationship between the concentration of solute A and solute B, while the stacked bar chart in (b) highlights the frequency with which each solute is greater than the other."
#| layout-ncol: 2

# Scatter plot data
scatter_data <- tibble(soluteA = soluteA,
                       soluteB = soluteB) %>%
  mutate(biggerb = ifelse(soluteB>soluteA, "Yes", "No"))

# Scatter plot
scatter_data %>%
  ggplot(aes(x=soluteA, y=soluteB)) +
  geom_point(aes(colour=biggerb)) +
  labs(x="Solute A Concentration (PPM)", 
       y="Solute B Concentration (PPM)") +
  guides(colour=guide_legend(title="Is B>A?")) +
  scale_y_continuous(breaks=ticks, limits=c(0,100)) +
  scale_x_continuous(breaks=ticks, limits=c(0,100)) +
  theme_classic() +
  theme(aspect.ratio=1) +
  scale_colour_brewer(type = "qual", palette = 6)

# Make bar plot data
bar_data <- scatter_data %>%
  mutate(biggersolute = ifelse(biggerb=="Yes", "Solute B", "Solute A")) %>%
  select(-biggerb)

# Stacked bar chart
bar_data %>%
  mutate(dummy = "d") %>%
  ggplot(aes(x=dummy, fill=biggersolute)) +
  geom_bar() +
  labs(y = "Proportion") +
  guides(fill=guide_legend(title="% of draws that will have more of...")) + 
  scale_y_continuous(breaks=ticks, limits=c(0,100)) +
  theme_classic() +
  theme(axis.title.x=element_blank(), 
        axis.text.x=element_blank(),
        axis.ticks.x = element_blank(),
        aspect.ratio=4/2)  +
  scale_fill_brewer(type = "qual", palette = 6)
  
```


### Theory: Selecting the correct distribution
In order to fairly compare two uncertainty visualisations, they need to provide the same information. This is rarely done in plots comparing distribution visualisations. The important distinction between the distribution displayed and the distribution required to answer a question is often ignored in our discussions good or bad uncertainty visualizations. This means that studies identifying some graphics as better than others may only do so because the two plots displayed different distributions. @fig-distdraw illustrates how we think about distributions when performing statsitical tests compared to when we create visualisations. When we compute statistics or perform a hypothesis test, we typically put a lot of thought into what the correct distribution is for our question, however, when we perform visualisation we typically plot a collection of normal marginal distributions and ignore the actual distribution required. This extends to our perception of visualisation in general as most of the visualisation we consider to be for "distributions" are actually just tools for visualising marginal distributions. 

![Illustration depicting the difference between the distributions we use for testing vs the visualisation we visualise](incorrectdistributions.jpeg){#fig-distdraw}

An example that might be more familiar to the average statistician is something I like to refer to as "the error bar problem". If you have ever looked at two overlapping error bars and said "oh these variables are not statistically significantly different" you have used error bars incorrectly. While it is true that error bars that do not overlap implies statistical significance, overlapping error bars do not imply the converse in true. The same is true for a lot of the other ad-hoc statistical tests we use error bars for. A study done by @Bella2005 asked participants to adjust two error bars until the means were "just" statistically significantly different, and most people adjusted the error bars until they were just touching. In the case of independent confidence intervals that just touch, the p-value of the associated two-tailled t-test is about 0.006 [@Schenker2001]. @Bella2005 also found that few people could incorporate changing information about independence that arrises from repeated measure design and most participants were ignorant to the fact that error bars are used for both confidence intervals and standard error bars, two wildly different indicators of precision. 

This is a classic example of expecting readers to draw conclusions about a distribution that was not visualised. Error bars typically represent the 95% confidence interval of a sampling distribution, most commonly the significant values of a t-distribution. This means that each error bar provides the range of significant values through the two end points, and a vague indicator of variance with the length of the bar, *of each variable independently*. What an error bar does **not** depict is the t-distribution associated with a difference of two means, the equivalent statistical test we utilise error bars for. The visualisation itself is not the problem, trying to draw conclusions that requires information that was not visualised is.

The papers that go on to cite these misunderstandings about error bars discuss the work as though the problems are caused by error bars themselves. They suggest that error bars should be avoided as a visualisation tool, ignoring the fact that these fundamental misunderstandings in uncertainty will likely follow other visual encodings of t-distributions. Maybe some visual aspect of error bars encourages these extrapolations, but the HOPs example I drew on at the start of this chapter illustrates that this problem of expecting people to answer questions that are not directly related to the visualised distribution is larger than questions about significance. 

This view of understanding uncertainty visualizations opens up a new gap in the literature. For example, if we accept that visualizing a collection of t-distributions is not a substitute for a collection of F-tests or paired t-tests, how *should* we visualise uncertainty if we want to draw those conclusions? Rather than shutting down the discussion on the usefulness of visualisation, I beleive it opens it and highlights areas of improvement in our current work.

## Part 2: Features
### Example: more 
Let us return to the study done by @Hullman2015. Not only is the distribution depicted in the visualisation different, but the features of the distribution depicted are also different. In discussing the concept of a "best" visualisation, we rarely discuss which *features* of the distribution are being displayed in the graphic. The way we currently look at visualisation would classify the error bar plot and the violin plot as visualisations of a "distribution", the scatter plot is a visualisation of a "relationship" while the bar plot is a visualisation of "amounts" (@wilke2019fundamentals), but this categorisation hides a lot of important details about drawing information from a graph. In this example the violin plot and the scatter plot both showed that each solution had an independent marginal normal distribution, the error bar (although technically a plot for visualising distributions) gives no concept of mass and would not give you the ability to identify even a simple distribution. Not only is it important to select a distribution that is appropriate for our question, it is also important to *show* the aspect of that distribution that holds the relevant information. The example asked about the *frequency* of a particular outcome which translates to visualising the *outcomes* of the joint distribution **or** the *parameter* of the Bernoulli distribution since these are the aspects of each distribution that hold the information we are looking for. If we visualised features of the distribution that don't hold the information we are looking for, the information is difficult to assertain and is more likely to be found through potentially faulty heuristics.

@fig-bad depicts two graphics that have the of the same distributions as those in @fig-alternatives, but each plot visualises an aspect of the distribution that is less relevant to the question. The 2D error bar plot (or circle?) highlights the mean and the values that are within the 95% confidence range. Since none of the parameters of the joint distribution are directly related to the question at hand, visualising the mean and significance instead of the outcomes made it harder to answer "What is $P(B>A)$?". Since the parameter of the Bernouli distribution *is* directly related to the question, visualising outcomes makes it harder to answer the question.

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: fig-bad
#| fig-subcap: 
#|   - "Error cirlce. The point in the middle represents the mean and the circle represents the area 95% of values will fall into."
#|   - "A scatter plot depicting the samples of the bernouli distribution"
#| fig-cap: "Two plots that are not useful in answering the question 'In what percentage of vials is there more of solute B than A (Probability(B > A)?'. While the distributions visualised are correct, we have visualised the incorrect feature of the repsective distributions. Plot (a) visualises mean and significance thresholds of for values (b) the."
#| layout-ncol: 2

# 2D error bar plot
#ticks <- seq(0,100, 10)
#tibble(x=50,
#       y=60) %>%
#  ggplot(aes(x,y)) + 
#  geom_errorbarh(xmin=30, xmax=70) +
#  geom_errorbar(ymin=40, ymax=80) +
#  geom_point() +
#  geom_abline() + 
#  scale_y_continuous(breaks=ticks, limits=c(0,100)) +
#  scale_x_continuous(breaks=ticks, limits=c(0,100)) +
#  labs(y = "Solution B Parts Per Million (PPM)",
#       x = "Solution A Parts Per Million (PPM)")+
#  theme_classic() +
#  theme(aspect.ratio=1)

# CIRCLE PLOT
library(ggforce)
ticks <- seq(0,100, 10)
tibble(x=50,
       y=60) %>%
  ggplot(aes(x,y)) + 
  geom_circle(aes(x0=x, y0=y, r = 20))+
  geom_point() +
  geom_abline() + 
  scale_y_continuous(breaks=seq(0,100, 10), limits=c(0,100)) +
  scale_x_continuous(breaks=seq(0,100, 10), limits=c(0,100)) +
  labs(y = "Solution B Parts Per Million (PPM)",
       x = "Solution A Parts Per Million (PPM)")+
  theme_classic() +
  theme(aspect.ratio=1)

# Outome Bernouli Plot
scatter_data %>%
  ggplot(aes(x=biggerb, y=0, colour=biggerb)) +
  geom_jitter(width=0.4, height=0.4) + 
  scale_y_continuous(limits=c(-1,1)) +
  labs(x = "Is B>A?")+
  theme_classic() +
  theme(aspect.ratio=1,
        axis.title.y = element_blank(), 
        axis.text.y=element_blank(),
        legend.position = "none",
        axis.ticks.y = element_blank()) +
  scale_colour_brewer(type = "qual", palette = 6)

```

Not only does visualising the incorrect distribution that highlights incorrect features related to a question make it difficult to answer correctly, it might also completely disorientate the viewer. In the discussion of the study @Hullman2015 notes: 
> "In light of the common usage of error bars for presenting multiple independent distributions, it is noteworthy how poorly subjects using these representations performed on tasks asking them to assess how reliably a draw from one variable was larger than the other...Many subjects reported values less than 50%, which are not plausible given that the mean of B was larger than the mean of A."

### Theory: the four features of a distribution
Different features of a distribution have different questions they are mode adept adept at answering. The apsect of a distribution that are typically depicted in graphics can be organised into four key features. @fig-features depicts these four features along with an example of how they are typically depicted in graphics. Each of these four features have a set of questions they are better equip to answer. The four features are:

1) **Mass** describes the PMF/PDF or CMF/CDF of the functions. Depictions of mass can inform us of the mode, likely or impossible values, and whether or not we have an identifiable distribution. 

2) **Samples** are a set of actual or simulated outcomes of a distribution. Our data falls into this category as it can be seen as an outcome of some "underlying" distribution. Outcomes can also be simulated through techniques such as bootstrapping or random sampling. Outcomes are useful to answer questions about frequency, however since outcomes have a connection to mass (depending on the structure of the graphic, a mass plot may just be a smoothed plot of a visualisation of a sample) they can sometimes be used to answer the same questions.

3) **Parameters** are the statistics that are related to our distribution. They can be the sufficient statistics of the distribution, such as the mean, variance, minimum and maximum, or they can be other statistics such as the or correlation, median and mode. Visualising a specific parameter of our distribution typically gives us freedom because it allows us to express any aspect of a plot in terms of a single value. Unfortunately this flexibility means that the set of question questions a parameter plot can answer are limited. Questions about the mean, median, maximum, minimum, correlation, values of significance, etc would usually need multiple plots to answer.

4) **Exchangability** illustrates whether or not a elements in a sequence of random variables from this distribution can be swapped. Rather than answering questions about exchangability, it is something we typically want to highlight in our data. Exchangability is often expressed by having features touch in a graphic. A time series inechangability is expressed using a line plot, spatial data's inechangability is expressed using a map and the exchangability of randomly sampled data is often expressed using points. Exchangability is adjacent to continuity because visually connected features in a graphic are also used to express discretised outcomes of a continuous process (such as a ridgeline plot).

![The four main features of a distribution graphics are typically used to represent](distfeatures.jpeg){#fig-features}

This is not an exhaustive list of every possible feature of a distribution, but rather the four most commonly visualised aspects of a distribution. Additionally, these features are not entirely distinct, particular visualsiations of parameters or samples may also double as a depiction of mass. Therefore, a graphic that is designed to depict one feature of a distribution may depict multiple features of multiple distributions. It also compares multiple ways of showing distribution to see which has more power.

## Part 3: Hierarchy
### Example: Looking to spatial uncertainty
This chapter so far has focused on the first way in which we talk about "visualising uncertainty" which is to depict some aspect of a distribution, however this does not cover the case of visualising uncertainty to express the impreciseness of estimates. Often our goal in visualisation is not to perform a statistical test or to make a decision based on a single estimate, but to identify some underlying structure in our data or make a decision based on a large number of connected systems. Visualising estimates and the uncertainty associated with those estimates separately can lead to the uncertainty being ignored [@moritz2017trust]. This is typically the case when we try and visualise spatial uncertainty. 

Trying to depict the uncertainty of an estimates with a spatial aspect is incredibly difficult. Even displaying an estimate in a spatial context can have problems. Improvements in spatial plots typically focus on ways to map a distribution that is dependent on something other than land size while still depicting the inexchangability and location that is relevant to spatial data. A common complaint about choropleth maps is that they depict the estimated parameter as a function of land size instead of location, even if land size is irrelevant to the estimate (such as in the case of voting). Visualisations that correct for this, such as hex maps, do so by colouring and plotting hexagonal tiles that each represent a portion of the dependent location [@Kobakian]. This means an irrelevant feature, such as land size is not depicted as important in the map. Trying to highlight the uncertainty associated with these estimates makes the process even more difficult.

There are four proposed methods of visualising spatial uncertainty that can be made with the `Vizumap` R package [@lucchesi2021vizumap]. These four plots each present uncertainty with varying degrees of success and failure.

The two visualiations that do a reasonably good job of expressing error are the bivariate map and the pixel map. @fig-spatialuncert1 depicts these two types of visualisation. Plot (a) depicts a bivariate map which uses a bivariate colour palette which is created by blending two single hue colour palettes. One colour represents the variable of interest while the other represents uncertainty. There are two immediate problems with this method. First of all, uncertainty is being mapped with hue and saturation. @fig-huesatcol illustrates the differences between hue, saturation, and value when looking at a colour palette. @Maceachren2012 found hue and saturation to be two of the worst aesthetics to map to uncertainty as they don't have an intuitive connection to uncertainty and so participants were worse. Value does have a natural connection to uncertainty (lighter values equate to higher uncertainty and darker values equate to more certainty) so it is a much more appropriate choice. While the `Vizumap` data does depict areas of light and darkenss, they are largely irrlevant to the uncertainty measure causing our heuristics to lead us to the incorrect conclusions. The Value-Suppressing Uncertainty Palettes (VSUP) shown in @fig-vsup maps estimates to the hue and uncertainty to the value thereby creating a more intuitive plot [@Correll2018 ]. Additionally, at high levels of uncertainty VSUP only has one output colour, which explicitly prevents any decoding of one output colour without enforcing a binary encoding of significance [@Correll2018]. Unfortunately VSUP are not easy to combine with packages like `Vizumap` which leaves it still somewhat difficult to express this encoding in practice, however the combination of a bivariate map with VSUP has shown to improve decisions in the face of uncertainty [@Correll2018]. Plot (b) from @fig-spatialuncert1 depicts a pixel map. Pixel maps are similar to HOPs plots since they present a sample of possible values for the estimate, rather than a single value and an uncertainty visualsiation. Some depictions of a pixel map are quite intuitive, such as those shown in @Bauer2015, however these cases link "uncertainty" to sparseness rather than a wide array of potential outcomes. Currently the effectivness of a pixel map is yet to be shown in any experients, and it may turn out to be a poor encoding of uncertainty information. Mapping a distribution of colours to numerical values is currently required to extract actual values from the plot which is a relatively difficult mental task Therefore the pixel map might be more effective if values were written explicitly on top of the pixels. The pixel map is also quite computationally expensive and hard to interpret when the relevant geographical areas are small relative to a large map, so it is likely best used on simpler smaller geographical areas. 

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: fig-spatialuncert1
#| fig-subcap: 
#|   - "Bivariate map"
#|   - "Pixel map"
#| fig-cap: "The two more inventive visualisations of spatial uncertainty that can be made using the `Vizumap` package. These are the example visualisations of the bivariate and pixel map that are provided in the package vingette."

# Data stuff
data(us_data)
data(us_geo)
us_data <- us_data %>% mutate(Estimate = pov_rate,
                              Error = pov_moe)
poverty <- read.uv(data = us_data, estimate = "Estimate", error = "Error")

# Bivariate plot
# make pal 
customBivPal3 <- build_palette(name = "usr", colrange = list(colour = c("chartreuse4", "darkblue"), difC = c(3, 4)))
customBivPal1 <- build_palette(name = "usr", colrange = list(colour = c("tan2", "lightskyblue"), difC = c(1, 1)))
# Make map
usBivMap <- build_bmap(data = poverty, geoData = us_geo, id = "GEO_ID", terciles = TRUE, , palette = customBivPal3)
# make key
usBivKey <- build_bkey(data = poverty, palette = customBivPal3, terciles = TRUE)
# attach key (+ visualise)
#attach_key(usBivMap, usBivKey) 

# Pixel Map
us_data$GEO.id2 <- as.numeric(us_data$GEO.id2)
ca_data <- subset(us_data, us_data$GEO.id2 > 6000 & us_data$GEO.id2 < 7000)
ca_data <- read.uv(data = ca_data, estimate = "Estimate", error = "Error")
row.names(ca_data) <- seq(1, nrow(ca_data), 1)
ca_geo <- subset(us_geo, us_geo@data$STATE == "06")
pix <- pixelate(ca_geo, id = "region")
df <- data.frame(region = sapply(slot(ca_geo, "polygons"), function(x) slot(x, "ID")), name = unique(ca_geo@data$GEO_ID))
ca_data$region <- df[match(ca_data$GEO_ID, df$name), 1]
ca_data$region <- as.character(ca_data$region)
unifPixMap <- build_pmap(data = ca_data, distribution = "uniform", pixelGeo = pix, id = "region", border = ca_geo)
#view(unifPixMap)
```

![Visual depiction of hue, saturation and value of a colour palette from @huesatval](huesatval.png){#fig-huesatcol}

![An example of a Value-Suppressing Uncertainty Palette with axis aids](vsup.png){#fig-vsup}

@fig-spatialuncert2 depicts the two plots that are either inappropriate for depicting spatial error, or are functionally the same as maps that already exist. Plot (c) is a glyph map that uses colour of a glyph to express the estimate and the rotation of the glyph to express uncertainty. Orientation has no intuitive link to uncertaitny and should be avoided at all costs [@Maceachren2012]. This plot also makes it easier to ignore the uncertainty all together since it is mapped to a feature that is different to the estimate. Plot (d) is a exceedance probability map that shows the probability of the estimate being over a certain value. This plot was developed specifically for decision making relative to a single value in mind, and can be considered a visualisation of a single parameter [@Kuhnert2018]. An exceedance probability map map is actually very similar to a chloropleth map, but instead of expressing an estimate, it shows a probability. Therefore this plot is only able to express probability through a change in information rather than an improvement in visualisation techniques.

```{r}
#| eval: false
#| echo: false
#| warning: false
#| message: false
#| label: fig-spatialuncert2
#| fig-subcap: 
#|   - "Glyph map"
#|   - "Exceedance probability map"
#| fig-cap: "The two visualisations of spatial uncertainty that can be made using the `Vizumap` package that are less impressive. The glyph map is already used in high dimensional data and rotation is a poor mapping for error. An exceedance probability map is useful for uncertainty visualisaitons, but it is functionally the same as a chloropleth map. "

# Glyph Map
co_geo <- subset(us_geo, us_geo@data$STATE == "08")
us_data$GEO.id2 <- as.numeric(us_data$GEO.id2)
co_data <- subset(us_data, us_data$GEO.id2 > 8000 & us_data$GEO.id2 < 9000)
co_data <- read.uv(data = co_data, estimate = "Estimate", error = "Error")
usGlyphMap <- build_gmap(data = co_data, geoData = co_geo, id = "GEO_ID", size = 80, glyph = "icone", border = "county")
usGlyphKey <- build_gkey(data = co_data, glyph = "icone")
attach_key(usGlyphMap, usGlyphKey)

# Exceedance probability map
poverty <- read.uv(data = us_data, estimate = "pov_rate", error = "pov_moe")
quantile(us_data$pov_rate)
pd <- quote({ pexp(q, rate, lower.tail = FALSE) })
args <- quote({ list(rate = 1/estimate) })
pdflist <- list(dist = pd, args = args, th = 30)
usExcMap <- build_emap(data = poverty, pdflist = pdflist, geoData = us_geo, id = "GEO_ID", key_label = "Pr[Estimate > 30]")
view(usExcMap)
```


The four plots depicted in @fig-spatialuncert1 and @fig-spatialuncert2 are not the be all end and all of visualisation of spatial uncertainty. They are a suggested solution for a particular case of uncertainty visualisation, when we want to highlight the certainty or uncertainty associated with a particular estimate. They may not even always work in that case depending on the complexity of the underlying distribution. The plots that can be expressed using `Visumap` sets a particular order of importance to the information we are conveying (typically geographical information > estimate > sample uncertainty) and maps that information to aesthetics that are of relative importance in the plot.

### Theory: Mapping the features to the appropriate hierarchy
The methods I discussed in parts 1 and 2 of this chapter and the method I discussed in the starting example of this section are not as distinct as I am making them out to be. We can think of this new problem using the framing I have established so far in this chapter. This question boils down to "how do I visualise the sampling distriubtions of large number of estimates and also visualise some features of a larger distribution that THOSE estimates came from". When there are many features of the data that are competing for our interest, we often don't have as much flexibility in how we choose to visualise uncertainty. Frequently we abandon complicated features of our distributions, such as mass, to make way for for complicated features of the data such as spatial dependence. Obviously every possible piece of information you might want to depict in a plot cannot be extrated with maximum efficiency, so we need to establish some kind of hierarchy.

A hierarchy of elementary perception tasks is not a new idea in visualisation. @Cleveland1984 found a natural ordering of 10 elementary perception tasks in term of how accurately participants could extract information that was mapped to that feature. The hierarchy established was:
1) Position along a common scale
2) Positions along nonaligned scales
3) Length, direction (slope), angle
4) Area
5) Volume, curvature
6) Shading, color saturation
While @Cleveland1984 notes that these tasks are not exhaustive nor mutually exclusive, and accuracy is not the only metric that should decide if a graphic is worthwhile, this hierarchy does provide a useful rule of thumb in understanding the importance of information we are illustrating in a graph. Therefore, not only should we consider what information we are depicting in our graph, we should also consider the implicit information hierarchy our graphic displays. It is one thing to have information in a plot, it is another for this information to be displayed in a way that allows for the information to be *efficiently* extracted. Previous discussions on hierarchy focused on mapping specific variables to elementary tasks, but I am combining the idea with the concept of relevant features and asking what features of each distribution do you think are important.

Additionally, just because information is in a graphic, that does not mean it will be "seen". The phenomena of inattentional blindness shows that there is no perception without attention and it is powerful enough that participants can fail to be aware of random objects appearing on a screen or a gorilla walking through a basketball game [@simons1999gorillas; mack2003inattentional]. Therefore if information is mapped to graphical elements that are so low on hierarchy they can be ignored, they might as well not be there at all. Therefore the people who don't plot uncertainty because they think its unimportant and the people who relegate uncertainty to the lowest ranking aesthetic on the list are both saying the same thing, "I think the uncertainty is unimportant". Including uncertainty is worth very little if no attention is left to see it. That is not to say we cannot put a *large* amount of information in a graphic, glyph maps can be used to depict the multi-dimensional information in spatial-temporal data, but we still need to decide what information is important. Differenct scales can be used depending on whether you want to identify trends in global variance or local variance [@Wickham2012], and smaller details are almost impossible to convey. No matter how much information we try and put in a plot, there will always be only a hanful of key take aways.

Not only should we considered the hierarchy in the information we depict, but we also need to consider the heuristics that connect some pieces of information to specific elementary tasks. For example @Hofmann2012 showed that polar co-ordinates are more effective than catesian co-ordinates when considering data that depicts directions which have a naturally polar interpretation. Uncertainty is naturally mapped to Fuziness, location, colour value best, then arrangement, size and transparency are an OK second choice, while saturation, hue, orientation and shape are unacceptable and have no intuitive connection to variance [@Maceachren2012]. No only do the graphical elements we map our features to matter, but the direction matters too. Graphical elements that are more fuzzy (fuziness), further from center (location), lighter (colour value), poorly arranged (arrangement), smaller (size), more transparent (transparency) are percieved to be more uncertain [@Maceachren2012]. This idea also extends to interval estimation, since gradient expressions are best for questions about probaility, but ambiguation is best for start and end time estimation [@Gschwandtnei2016]. Heuristics can work against us just as much as they can work for us. The sine illusion can cause the confidence interval of a smoothed sine curve to seem wider at the peaks than the troughs, causing us to underestimate uncertainty associated with changing values [@Vanderplas2015]. 

There is an assumption in visualsiation that we need to show enough information to solve a task while avoiding irrelevant distracting information [@kosslyn2006graph]. While including additional features can increase the accuracies of some conclusions, it can also bias or discount others. Including means estimates on depictions of mass can cause people to discount the uncertainty information and use the difference between means as a proxy [@Kale2021]. @Nathonours found that a scatter plot is better than a line plot if you want to convey the correlation between two time series. Unfortunately we cannot be sure if this was influenced by swapping the conditional distribution for the joint distribution on the high priority position axis, or by dropping the visualisation of the time series exchangeability  feature. @fig-timescatter depicts four different visualisations of the same time series data. Each plot depicts a different combination of distribution (joint or conditional on time) and exchangability (the inechangability is either depicted with connected values or ignored with points). 

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: fig-timescatter
#| fig-subcap: 
#|   - "Conditional distribution on time with inexchangability"
#|   - "Conditional distribution on time"
#|   - "Joint distribution of series"
#|   - "Joint distribution of series with inexchangability"
#| fig-cap: "Four figures to illustrate the impact of changing the priority of the distribution displayed and presence of exchangeability . Time is depicted with opacity in plots (c) and (d) to maintain the same information. More recent dates are darker. Could you order these four plots by how well they convey correlation?"
#| layout-ncol: 2

# Data for conditional distribution
series_data <- aus_retail %>%
  filter(State %in% c("Victoria", "Queensland"),
         Industry == "Food retailing",
         year(Month)>2010) 

# Data for joint distribution
series_data2 <- series_data %>%
  select(-c(Industry, `Series ID`)) %>%
  pivot_wider(id_cols = Month,
              values_from = Turnover,
              names_from = State)%>%
  mutate(id = row_number()/max(row_number()))

# Data for joint distribution w segments
series_data3 <- series_data2 %>%
  mutate(x1 = lag(Queensland),
         y1 = lag(Victoria),
         x2 = Queensland,
         y2 = Victoria) %>%
  select(-c(Queensland, Victoria))

# Line plot
ggplot(series_data, aes(x=as.Date(Month), y=Turnover, colour=State)) +
  geom_line() +
  labs(y="Food retailing turnover (AUD)",
       x = "Month") + 
  theme_classic() +
  theme(aspect.ratio=1)  +
  scale_colour_brewer(type = "qual", palette = 7)

# Point plot
ggplot(series_data, aes(x=as.Date(Month), y=Turnover)) +
  geom_point(aes(colour=State)) +
  labs(y="Food retailing turnover (AUD)",
       x = "Month") + 
  #geom_segment(data=series_data2,
  #             aes(y=Queensland, xend=as.Date(Month),
  #                 yend=Victoria)) +
  theme_classic() +
  theme(aspect.ratio=1) +
  scale_colour_brewer(type = "qual", palette = 7)

# Scatter plot
ggplot(series_data2, aes(x=Queensland, y=Victoria, alpha=id)) +
  geom_point() +
  labs(y="Victoria food retailing turnover (AUD)",
       x= "Queensland food retailing turnover (AUD)") +
  theme_classic() +
  theme(aspect.ratio=1,
        legend.position = "none")  

# Scatter plot with exchangability
ggplot(series_data3, aes(alpha=id)) +
  geom_segment(aes(x=x1, y=y1, xend=x2, yend=y2))  +
  labs(y="Victoria food retailing turnover (AUD)",
       x= "Queensland food retailing turnover (AUD)") +
  theme_classic() +
  theme(aspect.ratio=1,
        legend.position = "none") 
```


With this in mind lets assess the hierarchy established in the figures of @fig-spatialuncert1 and @fig-spatialuncert2. Despite being against the categorisation of "uncertainty", "estimate", and "data" I will use it here because it is how it is referred to in the literature. Plot (a) in set the estimate and uncertainty at the same level of importance, plot (b) visualised the uncertainty over the estimate. Roation does not exist in the hierarchy (direction refers to identifying slopes and angles are for vectors starting at the same origin) and I found it easy to ignore, therefore plot (c) sets "uncertainty" is at a lower level than the estimate. Plot (d) displays a parameter related to both the estimate and its uncertainty. All four plots place spatial information at the highest level of importance. If the only thing you want from your plot is a sense of estimates with respect to their position in space, these plots are fine, but what if there is a feature of your distribution that is of higher importance than the order/inexchangeability of your data? Playing around with this hierarchy and asking if the same features of our distribution are being conveyed can lead to some interesting plots, such as those depicted in @fig-timescatter. 

What should become apparent comparing these plots is that there is a constant trade off between information relevant to the our question and secondary information that provides context. By always assuming features such as the spatial context are always the most important apsect of a plot, we kneecap research and don't consider the wide array of ways we can express complicated concepts such as inexchangability and perception heavy information such as location.

## Why this taxonomy?
You may wonder what the point of this alternative taxonomy is. There many other visualisation taxonomies that may feel more natural than the one I present here. For example @Grewal2021 categorized uncertainty visualizations based on discreteness of the distribution (a sub feature of mass) and the domain expertise (something I don't cover). @Hofmann2012 classified uncertainty distributions by the level of data summation, a measure that was adjacent to the number of data points used to construct the graphic, but the plots made were also all depictions of different distributions features. @wilke2019fundamentals organised all graphics, not just uncertainty visualisations, into 7 categories (with large overlaps). These groups were amounts, distributions, proportions, x-y relationships, geospatial data, and uncertainty. There is also evidence that the way we express the mass of a distribution is important even though it is something my taxonomy does not cover. There is a reasonable amount of evidence that cumulative displays or discrete displays (such as a quantile dot-plots or histograms) are the best ways to express mass for decision making and probability estimates [@Fernandes2018; @Hofmann2012; Kay2016; @Hullman2018].

Despite these alternatives I still believe this taxonomy is filling a gap in the literature for two reasons. First of all this taxonomy allows us to properly evaluate the results of any visualisation paper (not just uncertainty specific ones). By reading the questions asked of participants, identifying the relevant distribution and feature, and then identifying which plot maps that feature to the highest ranking elementary perception task, you can often guess the conclusion of a visualisation paper with a high degree of accuracy. There are a handful of questions the taxonomy does not resolve (e.g. comparing a different marginal densities or comparing catesian vs polar co-ordinates), but a large number of experiments that compare two plots that supply different information become self evident. The second benefit of the taxonomy is it provides a simple set of questions any visualisation author can ask themselves to identiy if their plot conveys the message they are trying to communicate. Every plot conveys information and if you want your audience to answer a question about the plot, it needs to convey a *sufficient* amount of relevant information for the audience to draw their conclusions. This taxonomy allows not only allows us to understand what may be wrong with some visualisations, but it also provides suggestions to improve a visualisation once you understand the motivation.

Unfortunately visualisation papers that properly consider the underlying distributional information depicted in a plot are rare, and papers that don't conflate issues created by an incorrect distribution, incorrect features, or poor use of aesthetics even more so. 

{{< pagebreak >}}

# Timeline
A timetable for completing the thesis and a statement of progress to date (make sure to match everything to apsects of the thesis)
- (done) Literature review (have made progress in this year) have picked up main parts of the infoviz community - changed topic once got new scholarship (check off to say completed)
- can pencil in energy aspect now (met with them earlier in the year) - have a need for this work 
- next meeting in June (and have regular meetings monthly over the next year and hopefully build up applications)
- Have idea of when I think I will have a finished piece of work (literature review)
- Content for each of the pieces should be mapped out
- ASC later in year (submitted an abstract for a poster)
- getting design of experiment clear and question we want to answer clear can go round and round (good idea to have something simple to test) - dont have time for something too complicated in phd need to have it in small managable pieces
- ASC december 2023 - apsect of literature review + results from preliminary experiemnt
- IEEE VIS (June 22 2023 poster abstract) - Conference October 2023 (present aspect of literature review)
- The Rstudio conference (posit conference) - hard to get a spot (worth applying for)
- Aim for something international in a year (or in last year)
- Also could do UseR! 2024 (or 2025) & talk about some of the software developed (technical output) 
- Could have an energy specific one (various applied energy)
- IEEE VIS (march 31st paper due in 2025) - Conference October 2025 (Vienna)
- Thesis due December 2025

{{< pagebreak >}}

# Bibliography

```{r, include=FALSE, eval=FALSE}
library(spelling)
qmd <- "index.qmd"
ignore <- readLines("WORDLIST")
check_spelling <- spell_check_files(
  qmd,
  ignore = ignore,
  lang = "en_GB"
)
if (nrow(check_spelling) > 0) {
  print(check_spelling)
  stop("Check spelling in Qmd files!")
}
```