---
title: "Plotting Apples, Oranges, and Distributions"
subtitle: "Current issues surrounding the research of uncertainty visualisations"
author: Harriet Mason
bibliography: references.bib
date: last-modified
format: pdf
---

- Entire report needs to be 10-20 pages

# Motivation/Literature Review
- Explanation of several reasons uncertainty visualisation needs to be investigated:

## Importance of visualising uncertainty
A survey conducted by Dr Jessica Hullman found that majority of visualisation authors surveyed agree that expressing uncertainty is important and should be done more often than it currently is, some even agreed that failing to do so is tantamount to fraud [@Hullman2020a]. Despite this, only a quarter of respondents included uncertainty in 50% or more of their visualisations [@Hullman2020a]. This means people are convinced that visualising uncertainty is important from a moral standpoint, but they have still been able to provide self sufficient reasoning that allows them to avoid doing it. 

While some people may have a lower baseline than the general public, most people get better at understanding uncertainty plots the more they are exposed to them [@Kay2016].
Refusing to express uncertainty because people don't understand them, prevents people from improving in their ability to understand the plots, causing those that may not be able to understand uncertainty to continue to be bad at it.

## Why people dont visualise uncertainty
There is a large amount of literature providing new ways to visualise uncertainty and showing its effectiveness, but much less on why people don't do it. Hullmans study found that the most common reasons authors don't visualise uncertainty despite knowing the importance of it are: not wanting to overwhelm the audience; an inability to calculate the uncertainty; a lack of access to the uncertainty information; and not wanting to make their data seem questionable [@Hullman2020a]. It is hard to read that data visulisation authors have a general inability to calculate or access uncertainty information and remain unconcerned about general incompetence in the field, however this issue does not seem to be completely groundless. A strong definition of uncertainty, a consensus on which uncertainty should be incorporated and which should be ignored, as well as simple methods to visualise it seem to be largely absent in the literature. These gaps are worth discussing at length and form a large part of the motivation behind *Project 1*. Authors failling to visualise uncertainty due to a fear of overwhelming their audience, however, are motivations that can easily be discredited with the current literature.

Not wanting to overwhelm the audience typically links back to a general belief that audiences struggle to understand uncertainty. Rather than asking for a blanket yes/no conclusion on the general publics ability to understand uncertainty, it would be more helpful to focus on the intricacies behind understanding uncertainty. Some research suggests that laypeople cannot understand complicated statistical technicalities (such as the difference between Frequentist and Baysian thinking) [@Hoekstra2014], nor can they reliably translate an error bar plot to an equivalent hypothesis test [@Bella2005], but they can make accurate and efficient decisions factoring the uncertainty into their choices [@Kay2016] [@Fernandes2018]. A study that had account for uncertainty in bus arrival times showed laypeople have ability to make fast and accurate decisions that accounted for uncertainty that was displayed on a small screen [@Kay2016].

This might suggest that visualisation authors are unaware of the research indicating that laypeople are able incorperate uncertainty into their decisisons, or they are using that reasoning because they themselves don't uncerstand their true motivations to avoid visualising uncertainty. For example, at least one interviewee from Hullman's survey claimed that expertise implies that the signal being conveyed is significant, but also said they would omit uncertainty if it obfuscated the message they were trying to convey; while other authors who were capable of calculating and and representing uncertainty well did not do it, and were unable to provide a self-satisfying reason why [@Hullman2020a]. These conflicting motivations devloid of sound logic are acknowledged by Hullman herself in the paper.

> "It is worth noting that many authors seemed confident in stating rationales, as though they perceived them to be truths that do not require examples to demonstrate. It is possible that rationales for omission represent ingrained beliefs more than conclusions authors have drawn from concrete experiences attempting to convey uncertainty" [@Hullman2020a]. 

The gap between what authors desire to visualise uncertainty and the decision makers desire to have uncertainty communicated to them also comes through in the methods we use to express uncertainty. While authors typically prefer to express uncertainty in vague terms and will use reasons such as the one above to justify it, decision makers prefer uncertainty in precise quantitative terms [@Erev_1990], [@Olson_1997]. There does not only seem to be a disconnect between the authors work and the desires of decision makers, but also a second disconnect that exists between the researchers who develop uncertainty visualisations and those that use them.

Many of the reasons people don't visualise uncertainty have a common belief that uncertainty is secondary to estimations. The argument speaks to an unspoken belief of those that work with data, that the uncertainty associated with an estimate (the noise) only exists to hide the estimate itself (the signal). From this view, uncertainty is only seen as additional or hindering information, therefore despite its alleged importance, when simplifying a plot uncertainty the first thing to go. This belief is also reflected in the development of new uncertainty visualisations. 

- Cases where people want/need to visualise uncertainty but methods are lacking (such as spatial/temporal)

## Energy forecasts and importance of unertainty vis for clean energy (motivation)

# Thesis Overview 
- Replace projects with (phase/stage/chapter/problems/questions)
- brief detail of the proposed content of all chapters. 
- description of the theoretical and conceptual framework that underlies the thesis, and procedures to be used in addressing the research questions
- Project 1: the stuff detailled in PhD writings on blog. Basically a literature review of the current state of uncertainty visualisation
- Project 2: Maybe an experiment/ application of ideas tests (AEMO application maybe)
- Project 3: Technical outputs (translation of research) - possibly a package

# Project 1 Details
1) Believe we should be able to get information that is not mapped to any aesthetic of the plot (One plot should provide all information about a distribution of collection of distributions)
2) Believe we should be able to get information about distributions not depicted in the plot (Unique questions do not unique distributions and I should be able to use one distribution to answer questions about another distribution)

## The current state perspective on visualising uncertainty
Current research in visualising uncertainty seems to have a focus on designing new options for visualising uncertainty for specific types of data. Typical papers comparing visualisation methods seem to fit into one or both of the follow categories:

1) a paper that suggests a new visualisation method for a specific type of data  
2) a paper that compares two existing methods to idenfiy which is better.  

Generally the goal seems to be to increase the number of options we have in our "visualisation bag" so that we always have a plot at the ready if we want to visualise uncertainty. These new plots also tend to focus on a catch all plot that is "best for making decision" or "best for visualising spatial uncertainty". Data visualisation is commonly utilised as a tool in data exploration, so it is not uncommon for a data analyst to make a plot with only a vague goal and use it to pull out a large number of adjacent observations. For example, a data analyst might make a scatter plot to find out the relationship between two variables, but in doing do also finds out that one variable has discrete outcomes over a range of values. It is common knowledge that any single plot cannot reveal all information about a data set, all plot types obfuscate and uncover different types of information, however this aspect of plotting becomes more apparent in uncertainty visualisation. Often when discussing "uncertainty" information, we expect readers to be able to draw information from a plot that was not estimated, prioritised, or visualised. These three issues run deep in the literature, but it is easiest to explain with an example that I will frequently return back to as I go into detail on these issues.

## Example: The HOPS, Error Bar, and Violin Plot Comparison
The issues that are rampent in the uncertainty visualisation literature are easily seen if we zoom in on one example. The study done by [@Hullman2015] is a great illustration in the importance of understanding your goal when visualising uncertainty. It is important to keep in mind that while I am discussing only one paper (for the time being) to illustrate a point, the issues I am discussing are the standard in uncertainty visualisation literature, this paper is not an outlier.

The study by [@Hullman2015] asked participants to provide some numerical properties of a distribution using a hypothetical outcome plot (HOPs), an error bar plot or violin plot. Participants were given questions relating to individual and multiple distributions. When shown a single distribution participants were asked about the mean of the distribution, the probability of an outcome being above some threshold (indicated on the plot with a red dot), or the probability of an outcome being between two given values. When shown two distributions the participants were asked "How often is measurement of solute B larger than the measurement of solute A?", and when shown three distributions "How often is measurement of solute B larger than the measurement of solute A and solute C?". @fig-examples shows an example error bar plot and violin plot for the two distribution case. There was also a high and low variance case for every plot and question combination. The study decided which technique was better using absolute error between the subjects response and the true value. They specified that the error bars are not confidence intervals as they represent the true underlying distribution, not the sampling distribution of the mean. They found that the HOPs reliably outperformed the violin and error bar plots for all the two and three distribution questions, but performed worse when estimating the mean when the variance was high, and when estimating threshold probability when the variance was low. The HOPs were no better than the violin or error bar plot in the other univariate cases. The authors suggest that the issue with estimating the mean comes from participants trying to integrate over large distances and a higher frame rate in the HOPs plot could fix it. 

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: fig-examples
#| fig-subcap: 
#|   - "Error bar plot"
#|   - "Violin plot"
#| fig-cap: "An example of the plots shown in the two variable tasks given in [@Hullman2015]. The example question provided with this plot was 'In what percentage of vials is there more of solute B than A (Probability(B > A)?'"
#| layout-ncol: 2

# Load Libries
library(tidyverse)
# Make error bar data
ymin <- c(30, 40)
ymax <- c(70, 80)
smean <- c(50, 60)
solute <- c("Solute A", "Solute B")
error_data <- tibble(ymin, ymax, smean, solute)

# Make error bar plot
ticks <- seq(0,100, 10)
error_data %>%
  ggplot(aes(x=solute)) + 
  geom_errorbar(aes(ymin=ymin, ymax=ymax), width=0.1, 
                linetype="dashed", size=1.4) +
  geom_boxplot(aes(y=smean), colour="dodgerblue") +
  labs(y="Parts Per Million (PPM)")+
  scale_y_continuous(breaks=ticks, limits=c(0,100)) +
  theme_classic() +
  theme(axis.title.x=element_blank(), 
        axis.text.x=element_text(size=15),
        aspect.ratio=1)

# Make violin plot data
n <- 100
soluteA <- rnorm(n, 50, 10)
soluteB <- rnorm(n, 60, 10)
solute <- c("Solute A", "Solute B")
violin_data <- tibble(ppm = c(soluteA, soluteB), 
                      solute =c(rep("Solute A", n), rep("Solute B", n)))

# Make violin plot
violin_data %>%
  ggplot(aes(x=solute, y=ppm)) + 
  geom_violin(fill= "dodgerblue", colour="dodgerblue") +
  labs(y="Parts Per Million (PPM)")+
  scale_y_continuous(breaks=ticks, limits=c(0,100)) +
  theme_classic() +
  theme(axis.title.x=element_blank(), 
        axis.text.x=element_text(size=15),
        aspect.ratio=1)  
```

## Issue 1: If you don't estimate it, you arent going to plot it
My first issue with the plots in Hullman's study is that they are visualsing different distributions. The error bar plot and the violin plot in @fig-examples provides information about the independent the marginal distributions of solutions A and B and provides no information on the joint distribution. The HOPs plot is similar to a slope graph except instead of connecting the outcomes from A and B with a line, they are connected through frames of an animation. The HOPs, just like a slope graph, depict a relationship between two varibles and this a *joint* distribution instead of two marginal distributions. This key feature can be used to improve upon the HOPs and direct our attention to the distribution that is even more useful for answering this question.

Two alternative plots are provided in @fig-alternatives. The scatter plot depicts the joint distribution of the two solutions and uses colour to highlight the details of a related Bernoulli distribution. The stacked bar chart is the final evolution of this process as it sets the distribution of the binary outcome $P(B>A)$ as the primary distribution of interest and entirely drops both the marginal and joint distribution. Through this process we moved the information needed to answer the question "What is $P(B>A)$?" from something you needed to calculate in your head (when looking at the error bar plots) to something you can *see* in the bar chart. 

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: fig-alternatives
#| fig-subcap: 
#|   - "Scatter Plot"
#|   - "Stacked Bar Chart"
#| fig-cap: "Two plots that could be used as alternatives to answer to question 'In what percentage of vials is there more of solute B than A (Probability(B > A)?'. The scatter plot in (a) focuses on the relationship between the concentration of solute A and solute B, while the stacked bar chart in (b) highlights the frequency with which each solute is greater than the other."
#| layout-ncol: 2

# Scatter plot data
scatter_data <- tibble(soluteA = soluteA,
                       soluteB = soluteB) %>%
  mutate(biggerb = ifelse(soluteB>soluteA, "Yes", "No"))

# Scatter plot
scatter_data %>%
  ggplot(aes(x=soluteA, y=soluteB)) +
  geom_point(aes(colour=biggerb)) +
  labs(x="Solute A Concentration (PPM)", 
       y="Solute B Concentration (PPM)") +
  guides(colour=guide_legend(title="Is P(B>A)?")) +
  scale_y_continuous(breaks=ticks, limits=c(0,100)) +
  scale_x_continuous(breaks=ticks, limits=c(0,100)) +
  theme_classic() +
  theme(aspect.ratio=1)  

# Make bar plot data
bar_data <- scatter_data %>%
  mutate(biggersolute = ifelse(biggerb=="Yes", "Solute B", "Solute A")) %>%
  select(-biggerb)

# Stacked bar chart
bar_data %>%
  mutate(dummy = "d") %>%
  ggplot(aes(x=dummy, fill=biggersolute)) +
  geom_bar() +
  labs(y = "Proportion") +
  guides(fill=guide_legend(title="% of draws that will have more of...")) + 
  scale_y_continuous(breaks=ticks, limits=c(0,100)) +
  theme_classic() +
  theme(axis.title.x=element_blank(), 
        axis.text.x=element_blank(),
        axis.ticks.x = element_blank(),
        aspect.ratio=4/2)  
  
```

The distinction between the distribution required to answer our question and the marginal distribution of our variables is often ignored in our discussions good or bad uncertainty visualisatios. @fig-distdraw illustrates how we typically go about thinking of distributions in order to answer a statistics question. When we compute statistics or perform a hypothesis test, we typically put a lot of thought into what the correct null distribution is for our question, when we perform visualisation we typically plot a collection of normal marginal distributions, ignoring the actual distribution required for the information we need. For example let's say we want to identify all the estimates in our linear regression are significantly different from 0. If we wanted to answer this question with statistics, we would typically perform an F-test using an F-distribution. If we wanted


For example, while it may seem intuitive to plot two time series using a line plot, a scatter plot is actually the better choice if you want to depict the correlation between the two time series [@Nathonours]. 

The discussions of visualising uncertainty seems to be completely devoid of any discussion of different types and uses of distribution visualisations. There seems to be an implicit understanding that the only distributions we care about are marginal distributions of single variables or sampling distributions of an estimate. Even the concept of some plots being used for "distributions" and plots for "relationships" is a little silly. In this example the violin plot and the scatter plot both showed that each solution had a marginal normal distribution, but while the error bar (although technically a plot for visualising distributions) gives no concept of mass leaves you unable to identify the existence of even a common distribution. 

![Illustration depicting the difference between the distributions we use for testing vs the visualisation we visualise](incorrectdistributions.jpeg){#fig-distdraw}



*PART 2 AND UNDERLYING STATISTICS ISSUES*
- High dimensional visualisation techniques also can be viewed through this frameoworok and is the reason I find the tour to be such a valuable tool in visualisation. The tour using because it tries to give a view of the overarching joint distribution. A parallel co-ordinate plot 
- Information can be hidden in both the visualisation you chose to use and the distribution you choose to estimate
- Additionally, you might argue that the scatter plots didn't visualise the "distribution" but rather, hypothetical outcomes of it.
- You need to identify if the distribution is relevant to your question, and if so, which distribution is relevant. 
- I argue that the problem is not that people are incorrectly reading error bars, but that error bars are being incorrectly used.
- Opens up a new gap in the literature (no good plot for doing what we currently use error bars for - checking the significance of a large number of variables)
There is also a large amount of confusion of what we should depict. 
- Frequency based questions are easily answered with a HOPs plot OR ANY PLOT OF HYPOTHETICAL OUTCOMES. 
- While you may be sitting here, thinking "this is obvious and pointless to point out" I suspect you are not as aware of how much you interchangably use different uncertainty methods. As a matter of fact, I can't think of a single unit in my undergraduate degree that didn't make this mistake at least once. Therefore it is unsurprising that every paper on uncertainty visualisation research seems to ignore this issue entirely. Not only do they ignore this, but many are written with the perspective of trying to find a "best" or "universal" uncertainty method. Ignoring the hypothesis that generated the uncertainty then leads to the uncertainty questions asked in every paper being different.
- With this example in mind, we now understand both the distribution we visualise and the method we use should be directly related to the uncertainty we want to ask questions about. These are the key points we need to keep in mind every time we visualise uncertainty, however even the things that this paper got right (i.e. in order to answer questions about an uncertainty distribution we need to show the uncertainty distribution) get lost when researchers try to reintroduce the context of the data. Papers discussing new ways to visualise "temporal uncertainty" or "spatial uncertainty" seem to forget the uncertainty is related to a distribution, not a type of data. If I ask you to explain to me what "temporal uncertainty" is, you may try and describe uncertainty related to a forecast, or variance of a variable, or even model residuals. These are not "temporal uncertainty", they are the distribution of a prediction, a variable, or of a model. None of these things are specific to temporal data. This may just sound like me being pedantic about poor naming, but this incorrect framing of uncertainty leads to very inconsistent research. Because we view uncertainty as a secondary feature of the data, and not as something we have constructed, we don't think about uncertainty visualisations with the "goal of the uncertainty" in mind.
- The problem is, we had a goal in mind when we estimated the uncertainty. The uncertainty related to whether or not two variables are significantly different from each other is different to the uncertainty related to whether or not each variable is significantly different from 0. If you make a forecast and want to know the confidence intervals associated with each point prediction, that would have a different confidence interval to the path of a 5 step ahead forecast. The concept of even visualising the uncertainty of some random variable without knowing what you want to know, is futile.If you ignore this clear distinction, you are going to misuse uncertainty techniques and beleive they are interchangable for every hypothesis, and thus draw incorrect conclusions. If you have ever used an error bar for a hypothesis that is not idenfiying a statistically significant range of values (using it to identify if two variables are statistically different from each other is also incorrect) then you are guilty of the exact issue I am talking about.
- If I asked you to perform an F test and gave you the information in an error bar plot, you would think I was an insane, and yet that is something we expect from error bar plots. 

### (Possible between section on priorities)

### Issue 2: If you dont plot it, I won't see it

There are only so many aesthetics we can map information to, and those aesthetics have a hierarchy of how easy they are the process. Additional secondary information can simultaneously give important context and clutter the visualisation, so it is important to understand our motivations when creating a visualisation. 

The error bar plot and the violin plot in @fig-examples puts implies we should be unconcerned with the relationship between the solutions and especially unconcerned with specific questions about it (such as "What is $P(B>A)$?"). The scatter plot in @fig-alternatives highlights that the relationship between the solutions is of importance and allows us to see think about the two variables as they occur together. This plot still maintain some information about the marginal distributions illustrated in the violin and error bar plots (you can tell that we seem to have to have two independent normal distributions) however it is no longer of primary importance, the plot is now telling us that the general relationship between the solutions is the most important feature.

What should become apparent comparing these plots is that there is a constant trade off between information relevant to the our question and secondary information that provides context. 


The primary issue I have with this study (and by extension most visualisation studies) is the philosophy driving the motivation. An error bar plot is good at displaying a range of statistically significant values. A violin plot is good at depicting the frequency of outcomes of a distribution. Both can be used to get a general idea of the variance or central value of a distribution, but the significant values, and the frequency of individual values is not all the information we might need from a distribution. While an animated HOPs plot was better at depicting probability, or frequency of an outcome, than an error bar plot, that does not mean it is the "best" at answering the question "In what percentage of vials is there more of solute B than A (Probability(B > A)?". @fig-alternatives depicts several plots that I actually think would do a better job than the HOPs plot used in the experiment.


*HIERARCHY DISCUSSION HERE* Lets take a look at the accuracy rank of an error bar.
- placebo rule, studys compare to placebo is bad, should compare to next best alternative (should compare plots that have the feature of interest at a similar importance ranking)
- paper Wanted to get a better understanding of peoples ability to assess if A>B, rather than asking a binary question about significance.

![Explicit highlighting of what information is contained in the error bar plot and high important the plot communicates that information to be](feature.jpeg)

*ERROR BAR HIERARCHY EXAMPLE HERE* This is why the conversation and research around error bars is very frustrating to read. 
- I think this issue is largely why error bar plots get a lot of flack as being a bad plot, I think they are just being misused. 
 - In the paper, Hullman does acknowledge that error bars are typically used to identify significance, but
- people couldn't answer that question with ANY DISTRIBUTION PLOT
- Ask yourself what questions can be answered with each aesthetic of the plot. Is this the hierarchy of information you want to express.
- mapping the most important feature to the highest aspect
- So after zooming in on the uses (and misuses) of error bar plots, it should be clear to see why error bar plots *are* valid visualisations of significance
- First of all, the study ignores the questions that would be ideal for an error bar or violin plot.
- they assume that people first make some probability estimate (they find P(A>B)) and then use that to assess significance. 
- Statistical significance is related to sampling distributions which they didn't care about
- "If there is special interest in knowing whether people make qualitative errors , we can set various qualitative errors such as 0.95 during the analysis phase and count the frequency of errors rather than their average magnitude" or just make an error bar plot.
- They argue that a normal distribution was the most favorable condition for the error bar and violin because it leads to a symmetric plot where the widest point is also the mean (so you acknowledge that violin plots are better at expressing the mode of a distribution???)
- There is also some evidence in their results that a question so removed from the purpose of the plot just confuses people. "In light of the common usage of error bars for presenting multiple independent distributions, it is noteworthy how poorly subjects using these representations performed on tasks asking them to assess how reliably a draw from one variable was larger than the other", Many subjects reported values less than 50%, which are not plausible given that the mean of B was larger than the mean of A." Did not find any obvious patterns of errors. "We speculate that many subjects simply had no idea how to make a good guess"


*INCORPERATING CONTEXT HERE*
- The people who don't plot uncertainty because they think its unimportant and the people who relegate uncertainty to the lowest ranking aesthetic on the list are both saying the same thing, "I think the uncertainty is unimportant". Including uncertainty is worth very little if its impossible to interpret and the last thing I notice in the plot. 
- You can chose to include the temporal aspect of the plot in some capacity, however it is not by accident that there is a general struggle to visualise uncertainty when there are so many competing ideas. This changes the way we view problems such as visualising spatial uncertainty. The question of how to constuct uncertainty plots for complicated data types (such as spatial data) is no longer driven by desiging an abstract uncertainty visualisation method for spatial data. The goal would be to find the best way to combine the most appropriate uncertainty visualisation for the relevant question and the best plot for the type of data. These methods may be able to be combined onto a single plot, however there may be cases where two different plots (one for the data and one for the uncertainty) is more appropriate and effective. 
- There is an unspoken assumption prevailing in uncertainty visualisation that the uncertainty distribution associated with one hypothesis can just be swapped out for another. An assumption that the literature itself seems to disprove since there are many papers that are "contradictory" if you view them through the lense of finding a best plot, but strangely consistent when you reframe the conclusions through the lense of the "best plot for this hypothesis". Unfortunately this evidence is almost always used to highlight a failing in the plot, rather than a failing in the question.

*SPATIAL UNCERTAINTY EXAMPLE*

### Section on distribution aspects
(Plots can give impressions of different things)
- specific values (mean, 95% CI intervals)
- Order
- Mass
- Connectedness

### Sorting the questions and experiments
- Which of these questions should the "best" uncertainty visualisation allow me to answer?
- My solution to this problem is to design uncertainty visualisation rules that are irrelevant to the data, but instead focus on the question being asked. I would review the research in uncertainty visualisation and reframe the conclusions with the goal of finding the best uncertainty method for each question, rather than for the type of data. I would then define rules for uncertainty visualisation so that authors know the correct way to express uncertainty for their purpose. Using this framework, the concept of designing a method for visualising temporal uncertainty becomes pointless.
- There is a large amount of acknowledgement that different plots hide and uncover different sources of information, so structuring experiments that allow us to have a better understanding of how these work would be more fruitful.
- Assumption that answering small questions are intermediate steps in answering big questions.
- Identifying which plots can answer a large number of questions and which questions they can answer is the experiement I suggest in project 2.
- there is a habit of comparing plots to NOTHING (error bar significance test) or to an alternative that is NOT "next best" (same level on the hierarchy)
- the way we perform studies on data visualisation is antithetical to the way we use data visualisations.
- I fundamentally disagree that we should be asking what plot is the best at depicting a distribution, and instead ask which questions different plots are better at identifying. These papers often ask one question (or a handful of similar questions) and compare the response on two or three plots. What would be more effective would be asking a large range of questions, with a large range of underlying distributions, and trying to match up the plots to their ideal question or case scenario. Finding a "best" plot for all case scenarios is a fruitless activity. 

- Example questions
Significance questions
- statistical significance of one value
- significant difference between variables
- Joint significance 
Probability Questions
- Which outcome is more probable 
Estimate questions
- What is the mean, mode and median of this distribution
Ordinal questions
- What is the maximum/minimum outcome you would expect from this distribution
- Between which values would 50% of the data appear
Vibe Questions
- Describe the distribution of the distribution 
- Do the variables have identifiable distributions
- Does the variable have a discrete or continuous distribution
- Are there any impossible values in the distribution
- Does this distribution have more than one mode?

*Final Paragraph*
Why do we need a plot that can "see" everything? Plots are similar to people in the sense that expecting one person to have a complete and flawless view of something as complicated as society ignores that one of the things that has allowed humanity to flourish is that there are so many of us. It allows each person to specialise and be good at something important to them rather than trying to be mediocre at everything. Similarly, no plot can see every possible thing you might want to know about your data. Leaning into and uncovering the strengths of each plot is the best way to research them because it reflects the ways in which we actually incorperate plotting into our diagram. While general principals are helpful, fundamentally each plot for each question should be as unique as the person who made it.

## Section notes
- (not sure if the taxonomy should go here)
- What would be ideal is a general application of methods to visualise uncertainty that does not require complete knowledge of all plots that exist. This framework would guide authors towards effective visualisations and allow them to build up knowledge of underlying statistical ideas.
- rethinking uncertainty visualisation in terms of the motivation of the plot (the research question) instead of "best" uncertainty visualisation.
- Show that we can find a best distribution visualisation and reintroduce context to get a good uncertainty plot for that question and context.
- Classification of current plots and research questions that have been shown to be effective for those types of plots
- Reintroduce context

# Timeline
- A timetable for completing the thesis and a statement of progress to date
- ASC later in year (submitted an abstract for a poster)
- IEEE VIS (June 22 2023 poster abstract) - Conference October 2023 
- Aim for something international in a year (or in last year)
- Also could do UseR! & talk about some of the software developed (technical output)
- Could have an energy specific one (various applied energy)
- IEEE VIS (march 31st paper due in 2025) - Conference October 2025 (Vienna)
- Thesis due December 2025

# Bibliography

```{r, include=FALSE, eval=FALSE}
library(spelling)
qmd <- "index.qmd"
ignore <- readLines("WORDLIST")
check_spelling <- spell_check_files(
  qmd,
  ignore = ignore,
  lang = "en_GB"
)
if (nrow(check_spelling) > 0) {
  print(check_spelling)
  stop("Check spelling in Qmd files!")
}
```